{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process IAP dissolved oxygen concentration data for spatiotemporal discretization - Argo\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IAP Argo (Oxygen) netCDF -> one CSV per standard depth level (no seasonal split)\n",
    "\n",
    "Final output fields (and ONLY these, fixed order):\n",
    "  Date, Time, Pressure, Latitude, Longitude, Temperature, Salinity, Oxygen, Source\n",
    "\n",
    "Formatting requirements (implemented in this script):\n",
    "- Latitude / Longitude: keep 4 decimals\n",
    "- Pressure: integer (rounded; you still treat m ≈ dbar)\n",
    "- Temperature / Salinity: keep 2 decimals (but set to empty in this script)\n",
    "- Oxygen: keep 2 decimals\n",
    "- Date: YYYY-MM-DD\n",
    "- Time: always empty (<NA>)\n",
    "- Temperature: always empty (<NA>)\n",
    "- Salinity: always empty (<NA>)\n",
    "- Oxygen: DOXY_QCed_interpolated_Adjusted_IAP (no QC flags are used)\n",
    "\n",
    "Output structure:\n",
    "  ROOT_DIR/{depth1}dbar/depth{depth1}.csv\n",
    "\n",
    "Depth matching (first-hit):\n",
    "- For each observation with pressure=p, tolerance r(p):\n",
    "    <=10:1, <=200:1.5, <=1000:2, <=2000:5, >2000:10\n",
    "- Search DEPTH1_LIST (ascending) for the first depth1 satisfying:\n",
    "    depth1 - r <= p <= depth1 + r\n",
    "  If no hit, discard the observation.\n",
    "\n",
    "Performance:\n",
    "- Expand by profile blocks (CHUNK_PROFILES) to control memory usage.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "INPUT_DIR = r\"/data/wang/IAP/IAP_Oxygen_Argo_netCDF_202404\"\n",
    "ROOT_DIR  = r\"/data/wang/Result_Data/alldoxy\"\n",
    "\n",
    "ENCODING = \"utf-8-sig\"\n",
    "CHUNK_PROFILES = 2000  # number of profiles per block (tune for your machine / memory)\n",
    "\n",
    "# Numeric formatting controls\n",
    "LATLON_DECIMALS = 4\n",
    "OXY_DECIMALS = 2\n",
    "\n",
    "# Pressure: keep integer (round to int)\n",
    "PRESSURE_ROUND_TO_INT = True\n",
    "\n",
    "# Standard depth levels (treated as dbar here)\n",
    "DEPTH1_LIST = [\n",
    "    1,10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "DEPTH1_ARR = np.asarray(DEPTH1_LIST, dtype=np.float64)\n",
    "\n",
    "# Output fields (and ONLY these)\n",
    "OUTPUT_COLUMNS = [\n",
    "    \"Date\", \"Time\", \"Pressure\", \"Latitude\", \"Longitude\",\n",
    "    \"Temperature\", \"Salinity\", \"Oxygen\", \"Source\", \"sigma_interp\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Depth tolerance & matching (vectorized)\n",
    "# =========================\n",
    "def depth_range_vec(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    p (here equivalent to depth in meters, but treated as dbar) -> tolerance:\n",
    "      <=10   -> 1\n",
    "      <=200  -> 1.5\n",
    "      <=1000 -> 2\n",
    "      <=2000 -> 5\n",
    "      >2000  -> 10\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    return np.select(\n",
    "        [p <= 10, p <= 200, p <= 1000, p <= 2000],\n",
    "        [1.0, 1.5, 2.0, 5.0],\n",
    "        default=10.0\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "def match_depth1_firsthit(p: np.ndarray, r: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the FIRST depth1 in DEPTH1_LIST (ascending) that hits the window:\n",
    "      depth1 - r <= p <= depth1 + r\n",
    "    Returns matched_depth1 (float; NaN if no hit).\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    r = r.astype(np.float64, copy=False)\n",
    "\n",
    "    lower = p - r\n",
    "    upper = p + r\n",
    "\n",
    "    idx = np.searchsorted(DEPTH1_ARR, lower, side=\"left\")\n",
    "    ok = idx < DEPTH1_ARR.size\n",
    "\n",
    "    matched = np.full(p.shape, np.nan, dtype=np.float64)\n",
    "    cand = np.empty_like(p, dtype=np.float64)\n",
    "    cand[ok] = DEPTH1_ARR[idx[ok]]\n",
    "\n",
    "    hit = ok & (cand <= upper)\n",
    "    matched[hit] = cand[hit]\n",
    "    return matched\n",
    "\n",
    "# =========================\n",
    "# Utility functions\n",
    "# =========================\n",
    "def ensure_depth_folders():\n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    for d1 in DEPTH1_LIST:\n",
    "        os.makedirs(os.path.join(ROOT_DIR, f\"{d1}dbar\"), exist_ok=True)\n",
    "\n",
    "def yyyymmdd_to_yyyy_mm_dd_obj(date_int: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input: int YYYYMMDD (1D, may contain -999)\n",
    "    Output: object array (\"YYYY-MM-DD\" or None)\n",
    "    \"\"\"\n",
    "    date_int = np.asarray(date_int)\n",
    "    out = np.empty(date_int.shape, dtype=object)\n",
    "    out[:] = None\n",
    "\n",
    "    valid = np.isfinite(date_int) & (date_int.astype(np.int64) != -999) & (date_int.astype(np.int64) > 0)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    s = date_int[valid].astype(np.int64).astype(str)\n",
    "    m8 = np.array([len(x) == 8 for x in s], dtype=bool)\n",
    "\n",
    "    vidx = np.where(valid)[0]\n",
    "    keep_idx = vidx[m8]\n",
    "    s_keep = s[m8]\n",
    "    if len(s_keep) > 0:\n",
    "        out[keep_idx] = [f\"{x[0:4]}-{x[4:6]}-{x[6:8]}\" for x in s_keep]\n",
    "    return out\n",
    "\n",
    "def append_df_to_depth_csv(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df must contain matched_depth1(int) + OUTPUT_COLUMNS\n",
    "    Group and append to depth{d}.csv\n",
    "    \"\"\"\n",
    "    for depth1, g in df.groupby(\"matched_depth1\", sort=False):\n",
    "        depth1 = int(depth1)\n",
    "        out_dir = os.path.join(ROOT_DIR, f\"{depth1}dbar\")\n",
    "        out_csv = os.path.join(out_dir, f\"depth{depth1}.csv\")\n",
    "\n",
    "        g_out = g[OUTPUT_COLUMNS]\n",
    "        write_header = not os.path.isfile(out_csv)\n",
    "\n",
    "        g_out.to_csv(\n",
    "            out_csv,\n",
    "            index=False,\n",
    "            mode=\"w\" if write_header else \"a\",\n",
    "            header=write_header,\n",
    "            encoding=ENCODING\n",
    "        )\n",
    "\n",
    "# =========================\n",
    "# Process a single nc file\n",
    "# =========================\n",
    "def process_one_nc(nc_path: str) -> dict:\n",
    "    print(f\"\\n[NC] {nc_path}\")\n",
    "\n",
    "    with Dataset(nc_path, \"r\") as nc:\n",
    "        oxy = nc.variables[\"DOXY_QCed_interpolated_Adjusted_IAP\"][:]       # (N_PROF, N_LEVELS)\n",
    "        dep = nc.variables[\"Depth_QCed_interpolated_Adjusted_IAP\"][:]      # (N_PROF, N_LEVELS) units=meters\n",
    "        date_prof = nc.variables[\"Date\"][:]                                # (N_PROF,)\n",
    "        lat_prof  = nc.variables[\"Latitude\"][:]                            # (N_PROF,)\n",
    "        lon_prof  = nc.variables[\"Longitude\"][:]                           # (N_PROF,)\n",
    "\n",
    "        m, n = oxy.shape\n",
    "\n",
    "    # Profile-level validity (Date/Lat/Lon)\n",
    "    latv = np.asarray(lat_prof, dtype=np.float64)\n",
    "    lonv = np.asarray(lon_prof, dtype=np.float64)\n",
    "    datei = np.asarray(date_prof)\n",
    "\n",
    "    prof_ok = (\n",
    "        np.isfinite(latv) & np.isfinite(lonv) &\n",
    "        (latv != -999) & (lonv != -999) &\n",
    "        np.isfinite(datei) & (datei != -999)\n",
    "    )\n",
    "    prof_idx = np.where(prof_ok)[0]\n",
    "\n",
    "    total_points_written = 0\n",
    "\n",
    "    if prof_idx.size == 0:\n",
    "        return {\n",
    "            \"nc_file\": nc_path,\n",
    "            \"profiles_in\": int(m),\n",
    "            \"profiles_valid\": 0,\n",
    "            \"points_in\": int(m * n),\n",
    "            \"points_written\": 0\n",
    "        }\n",
    "\n",
    "    # Process by profile blocks\n",
    "    for start in range(0, prof_idx.size, CHUNK_PROFILES):\n",
    "        block = prof_idx[start:start + CHUNK_PROFILES]\n",
    "        nb = block.size\n",
    "\n",
    "        # Convert block dates to YYYY-MM-DD strings; failures become None\n",
    "        date_str_prof = yyyymmdd_to_yyyy_mm_dd_obj(datei[block])\n",
    "        prof_has_date = np.array([x is not None for x in date_str_prof], dtype=bool)\n",
    "        if not np.any(prof_has_date):\n",
    "            continue\n",
    "\n",
    "        # Block data (dep used as Pressure)\n",
    "        oxy_b = np.asarray(oxy[block, :], dtype=np.float64, order=\"C\")\n",
    "        p_b   = np.asarray(dep[block, :], dtype=np.float64, order=\"C\")\n",
    "\n",
    "        # Point validity: pressure & oxygen finite and not -999; profile date must be valid\n",
    "        valid = (\n",
    "            np.isfinite(p_b) & np.isfinite(oxy_b) &\n",
    "            (p_b != -999) & (oxy_b != -999) &\n",
    "            prof_has_date[:, None]\n",
    "        )\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "\n",
    "        valid_flat = valid.ravel(order=\"C\")\n",
    "        idx_flat = np.nonzero(valid_flat)[0]\n",
    "        prof_local = (idx_flat // n).astype(np.int64)  # 0..nb-1\n",
    "\n",
    "        # Flattened variables\n",
    "        p_flat   = p_b.ravel(order=\"C\")[valid_flat]\n",
    "        oxy_flat = oxy_b.ravel(order=\"C\")[valid_flat]\n",
    "\n",
    "        # Expand profile-level values to point-level\n",
    "        lat_flat = latv[block][prof_local]\n",
    "        lon_flat = lonv[block][prof_local]\n",
    "        date_flat = np.array([date_str_prof[k] for k in prof_local], dtype=object)\n",
    "\n",
    "        # =========================\n",
    "        # Formatting: round/cast\n",
    "        # =========================\n",
    "        # Lat/Lon: 4 decimals\n",
    "        lat_flat = np.round(lat_flat, LATLON_DECIMALS)\n",
    "        lon_flat = np.round(lon_flat, LATLON_DECIMALS)\n",
    "\n",
    "        # Pressure: integer\n",
    "        if PRESSURE_ROUND_TO_INT:\n",
    "            p_flat = np.rint(p_flat).astype(np.int32, copy=False)\n",
    "\n",
    "        # Oxygen: 2 decimals\n",
    "        oxy_flat = np.round(oxy_flat, OXY_DECIMALS)\n",
    "\n",
    "        # Time / Temperature / Salinity are always empty\n",
    "        npts = date_flat.shape[0]\n",
    "        time_flat = np.full(npts, pd.NA, dtype=object)\n",
    "        tmp_flat  = np.full(npts, pd.NA, dtype=object)\n",
    "        sal_flat  = np.full(npts, pd.NA, dtype=object)\n",
    "        interp_flat = np.full(npts, pd.NA, dtype=object)\n",
    "\n",
    "        # Depth matching (first-hit)\n",
    "        # Note: do we match using the raw float p, or the integer-rounded p?\n",
    "        # Here we match using the integer-rounded p_flat for consistency with the output.\n",
    "        # If you prefer matching using raw float depths, compute r/match on p_raw instead.\n",
    "        r = depth_range_vec(p_flat.astype(np.float64, copy=False))\n",
    "        matched = match_depth1_firsthit(p_flat.astype(np.float64, copy=False), r)\n",
    "\n",
    "        keep = np.isfinite(matched)\n",
    "        if not np.any(keep):\n",
    "            continue\n",
    "\n",
    "        df_out = pd.DataFrame({\n",
    "            \"Date\": date_flat[keep],\n",
    "            \"Time\": time_flat[keep],\n",
    "            \"Pressure\": p_flat[keep],\n",
    "            \"Latitude\": lat_flat[keep],\n",
    "            \"Longitude\": lon_flat[keep],\n",
    "            \"Temperature\": tmp_flat[keep],\n",
    "            \"Salinity\": sal_flat[keep],\n",
    "            \"Oxygen\": oxy_flat[keep],\n",
    "            \"Source\": \"Argo\",\n",
    "            \"sigma_interp\": interp_flat[keep],\n",
    "            \"matched_depth1\": matched[keep].astype(np.int32)\n",
    "        })\n",
    "\n",
    "        append_df_to_depth_csv(df_out)\n",
    "        total_points_written += len(df_out)\n",
    "\n",
    "        # Cleanup\n",
    "        del df_out, oxy_b, p_b\n",
    "        gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"nc_file\": nc_path,\n",
    "        \"profiles_in\": int(m),\n",
    "        \"profiles_valid\": int(prof_idx.size),\n",
    "        \"points_in\": int(m * n),\n",
    "        \"points_written\": int(total_points_written)\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_depth_folders()\n",
    "\n",
    "    nc_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.nc\")))\n",
    "    if not nc_files:\n",
    "        print(f\"[ERROR] No nc files found in: {INPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    logs = []\n",
    "    for f in nc_files:\n",
    "        res = process_one_nc(f)\n",
    "        logs.append(res)\n",
    "        print(f\"[OK] profiles_valid={res['profiles_valid']} | points_written={res['points_written']}\")\n",
    "\n",
    "    log_path = os.path.join(ROOT_DIR, \"argo_iap_do_only_processing_summary.csv\")\n",
    "    pd.DataFrame(logs).to_csv(log_path, index=False, encoding=ENCODING)\n",
    "\n",
    "    print(\"\\n[DONE] IAP-Argo DO-only layered CSV writing completed.\")\n",
    "    print(f\"[LOG] {log_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13622119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "IAP OSD/CTD (WOD18 source) netCDF -> one CSV per standard depth level (no seasonal split)\n",
    "\n",
    "Final output fields (and ONLY these, fixed order):\n",
    "  Date,Time,Pressure,Latitude,Longitude,Temperature,Salinity,Oxygen,Source\n",
    "\n",
    "Formatting requirements (implemented here):\n",
    "- Latitude / Longitude: keep 4 decimals\n",
    "- Pressure: integer (rounded); still treated as m ≈ dbar\n",
    "- Temperature / Salinity: keep 2 decimals (but set to empty in this script)\n",
    "- Oxygen: keep 2 decimals\n",
    "- Date: YYYY-MM-DD (parsed from netCDF yyyymmdd; drop profiles if parsing fails)\n",
    "- Time / Temperature / Salinity are always empty (<NA>)\n",
    "- Oxygen uses DOXY_QCed_interpolated_Adjusted_IAP; no QC is applied\n",
    "- Source is fixed as \"OSDCTD\"\n",
    "\n",
    "Output layout:\n",
    "  ROOT_DIR/{depth1}dbar/depth{depth1}.csv\n",
    "\n",
    "Depth matching (first-hit):\n",
    "- For each observation with pressure p, tolerance r(p):\n",
    "    <=10:1, <=200:1.5, <=1000:2, <=2000:5, >2000:10\n",
    "- Scan DEPTH1_LIST in ascending order and take the first depth1 satisfying:\n",
    "    depth1 - r <= p <= depth1 + r\n",
    "  If no match, discard that observation.\n",
    "\n",
    "Performance:\n",
    "- Expand by profile blocks (CHUNK_PROFILES) to control memory\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "INPUT_DIR = r\"/data/wang/IAP/IAP_Oxygen_OSDCTD_netCDF_202404\"\n",
    "ROOT_DIR  = r\"/data/wang/Result_Data/alldoxy\"\n",
    "\n",
    "ENCODING = \"utf-8-sig\"\n",
    "CHUNK_PROFILES = 2000  # number of profiles processed per block (tune for memory)\n",
    "\n",
    "# Numeric formatting\n",
    "LATLON_DECIMALS = 4\n",
    "OXY_DECIMALS = 2\n",
    "\n",
    "# Pressure: keep integer (round to int)\n",
    "PRESSURE_ROUND_TO_INT = True\n",
    "\n",
    "# Standard depth levels (treated as dbar here)\n",
    "DEPTH1_LIST = [\n",
    "    1,10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "DEPTH1_ARR = np.asarray(DEPTH1_LIST, dtype=np.float64)\n",
    "\n",
    "# Output fields (and ONLY these)\n",
    "OUTPUT_COLUMNS = [\n",
    "    \"Date\", \"Time\", \"Pressure\", \"Latitude\", \"Longitude\",\n",
    "    \"Temperature\", \"Salinity\", \"Oxygen\", \"Source\", \"sigma_interp\"\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Depth tolerance & matching (vectorized)\n",
    "# =========================\n",
    "def depth_range_vec(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    p (treated as depth/m but interpreted as dbar) -> tolerance:\n",
    "      <=10   -> 1\n",
    "      <=200  -> 1.5\n",
    "      <=1000 -> 2\n",
    "      <=2000 -> 5\n",
    "      >2000  -> 10\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    return np.select(\n",
    "        [p <= 10, p <= 200, p <= 1000, p <= 2000],\n",
    "        [1.0, 1.5, 2.0, 5.0],\n",
    "        default=10.0\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "def match_depth1_firsthit(p: np.ndarray, r: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the FIRST depth1 (ascending in DEPTH1_LIST) that hits:\n",
    "      depth1 - r <= p <= depth1 + r\n",
    "    Return matched_depth1 (float; NaN if no hit).\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    r = r.astype(np.float64, copy=False)\n",
    "\n",
    "    lower = p - r\n",
    "    upper = p + r\n",
    "\n",
    "    idx = np.searchsorted(DEPTH1_ARR, lower, side=\"left\")\n",
    "    ok = idx < DEPTH1_ARR.size\n",
    "\n",
    "    matched = np.full(p.shape, np.nan, dtype=np.float64)\n",
    "    cand = np.empty_like(p, dtype=np.float64)\n",
    "    cand[ok] = DEPTH1_ARR[idx[ok]]\n",
    "\n",
    "    hit = ok & (cand <= upper)\n",
    "    matched[hit] = cand[hit]\n",
    "    return matched\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "def ensure_depth_folders():\n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    for d1 in DEPTH1_LIST:\n",
    "        os.makedirs(os.path.join(ROOT_DIR, f\"{d1}dbar\"), exist_ok=True)\n",
    "\n",
    "def yyyymmdd_to_yyyy_mm_dd_obj(date_int: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Input: int YYYYMMDD (1D, may contain -999)\n",
    "    Output: object array (\"YYYY-MM-DD\" or None)\n",
    "    \"\"\"\n",
    "    date_int = np.asarray(date_int)\n",
    "    out = np.empty(date_int.shape, dtype=object)\n",
    "    out[:] = None\n",
    "\n",
    "    valid = np.isfinite(date_int) & (date_int.astype(np.int64) != -999) & (date_int.astype(np.int64) > 0)\n",
    "    if not np.any(valid):\n",
    "        return out\n",
    "\n",
    "    s = date_int[valid].astype(np.int64).astype(str)\n",
    "    m8 = np.array([len(x) == 8 for x in s], dtype=bool)\n",
    "\n",
    "    vidx = np.where(valid)[0]\n",
    "    keep_idx = vidx[m8]\n",
    "    s_keep = s[m8]\n",
    "    if len(s_keep) > 0:\n",
    "        out[keep_idx] = [f\"{x[0:4]}-{x[4:6]}-{x[6:8]}\" for x in s_keep]\n",
    "    return out\n",
    "\n",
    "def append_df_to_depth_csv(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    df must contain matched_depth1(int) + OUTPUT_COLUMNS.\n",
    "    Grouped append into depth{d}.csv\n",
    "    \"\"\"\n",
    "    for depth1, g in df.groupby(\"matched_depth1\", sort=False):\n",
    "        depth1 = int(depth1)\n",
    "        out_dir = os.path.join(ROOT_DIR, f\"{depth1}dbar\")\n",
    "        out_csv = os.path.join(out_dir, f\"depth{depth1}.csv\")\n",
    "\n",
    "        g_out = g[OUTPUT_COLUMNS]\n",
    "        write_header = not os.path.isfile(out_csv)\n",
    "\n",
    "        g_out.to_csv(\n",
    "            out_csv,\n",
    "            index=False,\n",
    "            mode=\"w\" if write_header else \"a\",\n",
    "            header=write_header,\n",
    "            encoding=ENCODING\n",
    "        )\n",
    "\n",
    "# =========================\n",
    "# Single netCDF file processing\n",
    "# =========================\n",
    "def process_one_nc(nc_path: str) -> dict:\n",
    "    print(f\"\\n[NC] {nc_path}\")\n",
    "\n",
    "    with Dataset(nc_path, \"r\") as nc:\n",
    "        oxy = nc.variables[\"DOXY_QCed_interpolated_Adjusted_IAP\"][:]       # (N_PROF, N_LEVELS)\n",
    "        dep = nc.variables[\"Depth_QCed_interpolated_Adjusted_IAP\"][:]      # (N_PROF, N_LEVELS) units=meters\n",
    "        date_prof = nc.variables[\"Date\"][:]                                # (N_PROF,)\n",
    "        lat_prof  = nc.variables[\"Latitude\"][:]                            # (N_PROF,)\n",
    "        lon_prof  = nc.variables[\"Longitude\"][:]                           # (N_PROF,)\n",
    "\n",
    "        m, n = oxy.shape\n",
    "\n",
    "    # Profile-level validity (Date/Lat/Lon)\n",
    "    latv = np.asarray(lat_prof, dtype=np.float64)\n",
    "    lonv = np.asarray(lon_prof, dtype=np.float64)\n",
    "    datei = np.asarray(date_prof)\n",
    "\n",
    "    prof_ok = (\n",
    "        np.isfinite(latv) & np.isfinite(lonv) &\n",
    "        (latv != -999) & (lonv != -999) &\n",
    "        np.isfinite(datei) & (datei != -999)\n",
    "    )\n",
    "    prof_idx = np.where(prof_ok)[0]\n",
    "\n",
    "    total_points_written = 0\n",
    "\n",
    "    if prof_idx.size == 0:\n",
    "        return {\n",
    "            \"nc_file\": nc_path,\n",
    "            \"profiles_in\": int(m),\n",
    "            \"profiles_valid\": 0,\n",
    "            \"points_in\": int(m * n),\n",
    "            \"points_written\": 0\n",
    "        }\n",
    "\n",
    "    # Process profiles in blocks\n",
    "    for start in range(0, prof_idx.size, CHUNK_PROFILES):\n",
    "        block = prof_idx[start:start + CHUNK_PROFILES]\n",
    "        nb = block.size\n",
    "\n",
    "        # Convert Date to YYYY-MM-DD strings; parsing failure => None\n",
    "        date_str_prof = yyyymmdd_to_yyyy_mm_dd_obj(datei[block])\n",
    "        prof_has_date = np.array([x is not None for x in date_str_prof], dtype=bool)\n",
    "        if not np.any(prof_has_date):\n",
    "            continue\n",
    "\n",
    "        # Block arrays (dep is used as Pressure)\n",
    "        oxy_b = np.asarray(oxy[block, :], dtype=np.float64, order=\"C\")\n",
    "        p_b   = np.asarray(dep[block, :], dtype=np.float64, order=\"C\")\n",
    "\n",
    "        # Point validity: pressure & oxygen finite and not -999; also profile date must be valid\n",
    "        valid = (\n",
    "            np.isfinite(p_b) & np.isfinite(oxy_b) &\n",
    "            (p_b != -999) & (oxy_b != -999) &\n",
    "            prof_has_date[:, None]\n",
    "        )\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "\n",
    "        valid_flat = valid.ravel(order=\"C\")\n",
    "        idx_flat = np.nonzero(valid_flat)[0]\n",
    "        prof_local = (idx_flat // n).astype(np.int64)  # 0..nb-1\n",
    "\n",
    "        # Flattened variables\n",
    "        p_flat   = p_b.ravel(order=\"C\")[valid_flat]\n",
    "        oxy_flat = oxy_b.ravel(order=\"C\")[valid_flat]\n",
    "\n",
    "        # Expand profile-level lat/lon/date to point-level\n",
    "        lat_flat = latv[block][prof_local]\n",
    "        lon_flat = lonv[block][prof_local]\n",
    "        date_flat = np.array([date_str_prof[k] for k in prof_local], dtype=object)\n",
    "\n",
    "        # =========================\n",
    "        # Formatting: round/cast\n",
    "        # =========================\n",
    "        # Lat/Lon: 4 decimals\n",
    "        lat_flat = np.round(lat_flat, LATLON_DECIMALS)\n",
    "        lon_flat = np.round(lon_flat, LATLON_DECIMALS)\n",
    "\n",
    "        # Pressure: integer\n",
    "        if PRESSURE_ROUND_TO_INT:\n",
    "            p_flat = np.rint(p_flat).astype(np.int32, copy=False)\n",
    "\n",
    "        # Oxygen: 2 decimals\n",
    "        oxy_flat = np.round(oxy_flat, OXY_DECIMALS)\n",
    "\n",
    "        # Time/Temperature/Salinity are always empty\n",
    "        npts = date_flat.shape[0]\n",
    "        na_col = np.full(npts, pd.NA, dtype=object)\n",
    "\n",
    "        # Depth matching (first-hit): match using integer Pressure to stay consistent with output\n",
    "        r = depth_range_vec(p_flat.astype(np.float64, copy=False))\n",
    "        matched = match_depth1_firsthit(p_flat.astype(np.float64, copy=False), r)\n",
    "\n",
    "        keep = np.isfinite(matched)\n",
    "        if not np.any(keep):\n",
    "            continue\n",
    "\n",
    "        df_out = pd.DataFrame({\n",
    "            \"Date\": date_flat[keep],\n",
    "            \"Time\": na_col[keep],\n",
    "            \"Pressure\": p_flat[keep],          # Depth(m) is treated as dbar (integer)\n",
    "            \"Latitude\": lat_flat[keep],\n",
    "            \"Longitude\": lon_flat[keep],\n",
    "            \"Temperature\": na_col[keep],\n",
    "            \"Salinity\": na_col[keep],\n",
    "            \"Oxygen\": oxy_flat[keep],\n",
    "            \"Source\": \"OSDCTD\",\n",
    "            \"sigma_interp\": na_col[keep],\n",
    "            \"matched_depth1\": matched[keep].astype(np.int32)\n",
    "        })\n",
    "\n",
    "        append_df_to_depth_csv(df_out)\n",
    "        total_points_written += len(df_out)\n",
    "\n",
    "        del df_out, oxy_b, p_b\n",
    "        gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"nc_file\": nc_path,\n",
    "        \"profiles_in\": int(m),\n",
    "        \"profiles_valid\": int(prof_idx.size),\n",
    "        \"points_in\": int(m * n),\n",
    "        \"points_written\": int(total_points_written)\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Main entry\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_depth_folders()\n",
    "\n",
    "    nc_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*.nc\")))\n",
    "    if not nc_files:\n",
    "        print(f\"[ERROR] No nc files found in: {INPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    logs = []\n",
    "    for f in nc_files:\n",
    "        res = process_one_nc(f)\n",
    "        logs.append(res)\n",
    "        print(f\"[OK] profiles_valid={res['profiles_valid']} | points_written={res['points_written']}\")\n",
    "\n",
    "    log_path = os.path.join(ROOT_DIR, \"iap_osdctd_do_only_processing_summary.csv\")\n",
    "    pd.DataFrame(logs).to_csv(log_path, index=False, encoding=ENCODING)\n",
    "\n",
    "    print(\"\\n[DONE] IAP-OSDCTD DO-only depth-layer CSV export finished\")\n",
    "    print(f\"[LOG] {log_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Profile-wise PCHIP interpolation onto standard depth levels + post-filtering\n",
    "+ an interpolation-uncertainty proxy field.\n",
    "\n",
    "Proxy idea (1st-order):\n",
    "  interp_unc(p0) ≈ |dO/dp|(p0) * d_min(p0)\n",
    "where\n",
    "  - d_min(p0) is distance to the nearest *oxygen* observation (after Pressure de-dup)\n",
    "  - |dO/dp|(p0) is estimated from neighboring *oxygen* observations around p0\n",
    "    (if bracketing exists use below+above; otherwise use nearest two on one side)\n",
    "\n",
    "Profile definition (no rounding, Date kept as-is):\n",
    "  profile_key = (Date, Latitude, Longitude)\n",
    "\n",
    "Assumption:\n",
    "  input file is ordered by profiles (all rows of a profile are contiguous).\n",
    "\n",
    "Steps per profile:\n",
    "  1) Deduplicate by Pressure: average Temperature/Salinity/Oxygen for identical Pressure\n",
    "  2) Sort by Pressure (ascending)\n",
    "  3) PCHIP interpolate whole profile onto standard depths within [minP, maxP], no extrapolation\n",
    "  4) Keep standard levels by rules (using oxygen-observation pressures):\n",
    "       - Rule1: within ±x(z) has >=1 oxygen obs\n",
    "       - Rule2: for remaining, within ±y(z) has >=2 oxygen obs\n",
    "  5) Output only kept standard levels AND only those with finite interpolated Oxygen\n",
    "     plus a new field: InterpUnc (μmol kg^-1)\n",
    "\n",
    "Overwrite:\n",
    "  --overwrite => atomic replace original file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Config\n",
    "# ============================================================\n",
    "\n",
    "INPUT_FILES = [\n",
    "    \"/data/wang/CCHDO/cchdo_bottle_filtered.csv\",\n",
    "    \"/data/wang/CCHDO/cchdo_ctd_filtered.csv\",\n",
    "    \"/data/wang/Geotraces IDP2021/GEOTRACES_IDP2021_filtered.csv\",\n",
    "    \"/data/wang/GLODAP/GLODAP2023_filtered.csv\",\n",
    "    \"/data/wang/OceanSItes/OceanSITES_filtered.csv\",\n",
    "]\n",
    "\n",
    "STD_DEPTHS = np.array([\n",
    "    1,10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "], dtype=np.float64)\n",
    "\n",
    "# --- Adaptive windows (x stricter than y) ---\n",
    "def x_window(dbar: float) -> float:\n",
    "    if dbar <= 50:\n",
    "        return 2.0\n",
    "    elif dbar <= 800:\n",
    "        return 5.0\n",
    "    elif dbar <= 2000:\n",
    "        return 10\n",
    "    else:\n",
    "        return 20.0\n",
    "\n",
    "def y_window(dbar: float) -> float:\n",
    "    if dbar <= 50:\n",
    "        return 5.0\n",
    "    elif dbar <= 800:\n",
    "        return 15.0\n",
    "    elif dbar <= 2000:\n",
    "        return 30.0\n",
    "    else:\n",
    "        return 120.0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "\n",
    "def to_float(s: str) -> float:\n",
    "    if s is None:\n",
    "        return np.nan\n",
    "    ss = str(s).strip()\n",
    "    if ss == \"\" or ss.lower() in {\"nan\", \"na\", \"none\"}:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(ss)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def atomic_replace(src_tmp: str, dst: str) -> None:\n",
    "    os.replace(src_tmp, dst)\n",
    "\n",
    "\n",
    "def profile_key_from_row(row: Dict[str, str]) -> Tuple[str, str, str]:\n",
    "    # Date kept as-is; lat/lon kept as-is (string)\n",
    "    return (row.get(\"Date\", \"\"), row.get(\"Latitude\", \"\"), row.get(\"Longitude\", \"\"))\n",
    "\n",
    "\n",
    "def estimate_local_grad_and_dmin(\n",
    "    p_obs: np.ndarray,\n",
    "    o_obs: np.ndarray,\n",
    "    z: np.ndarray,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each standard depth z[i]:\n",
    "      - dmin[i] = min_j |z[i] - p_obs[j]|\n",
    "      - grad[i] estimated from neighboring obs around z[i]\n",
    "      - unc[i] = |grad[i]| * dmin[i]\n",
    "    p_obs must be strictly increasing; o_obs finite.\n",
    "    \"\"\"\n",
    "    nz = z.size\n",
    "    dmin = np.full(nz, np.nan, dtype=np.float64)\n",
    "    grad = np.full(nz, np.nan, dtype=np.float64)\n",
    "    unc  = np.full(nz, np.nan, dtype=np.float64)\n",
    "\n",
    "    n = p_obs.size\n",
    "    if n < 2:\n",
    "        return dmin, grad, unc\n",
    "\n",
    "    # nearest distance via searchsorted (O(log n) each)\n",
    "    idx_right = np.searchsorted(p_obs, z, side=\"left\")  # first index >= z\n",
    "    for i in range(nz):\n",
    "        zr = float(z[i])\n",
    "        ir = int(idx_right[i])\n",
    "\n",
    "        # dmin\n",
    "        candidates = []\n",
    "        if 0 <= ir < n:\n",
    "            candidates.append(abs(p_obs[ir] - zr))\n",
    "        if 0 <= ir - 1 < n:\n",
    "            candidates.append(abs(p_obs[ir - 1] - zr))\n",
    "        if candidates:\n",
    "            dmin[i] = float(min(candidates))\n",
    "\n",
    "        # gradient estimation\n",
    "        # Prefer bracketing: (ir-1, ir)\n",
    "        if 1 <= ir <= n - 1:\n",
    "            p1, p2 = float(p_obs[ir - 1]), float(p_obs[ir])\n",
    "            o1, o2 = float(o_obs[ir - 1]), float(o_obs[ir])\n",
    "            dp = p2 - p1\n",
    "            if dp > 0:\n",
    "                grad[i] = abs((o2 - o1) / dp)\n",
    "        else:\n",
    "            # no bracketing: use nearest two points on the available side\n",
    "            if ir <= 0 and n >= 2:\n",
    "                # all obs are deeper than z: use first two\n",
    "                p1, p2 = float(p_obs[0]), float(p_obs[1])\n",
    "                o1, o2 = float(o_obs[0]), float(o_obs[1])\n",
    "                dp = p2 - p1\n",
    "                if dp > 0:\n",
    "                    grad[i] = abs((o2 - o1) / dp)\n",
    "            elif ir >= n and n >= 2:\n",
    "                # all obs are shallower than z: use last two\n",
    "                p1, p2 = float(p_obs[-2]), float(p_obs[-1])\n",
    "                o1, o2 = float(o_obs[-2]), float(o_obs[-1])\n",
    "                dp = p2 - p1\n",
    "                if dp > 0:\n",
    "                    grad[i] = abs((o2 - o1) / dp)\n",
    "\n",
    "        if np.isfinite(dmin[i]) and np.isfinite(grad[i]):\n",
    "            unc[i] = grad[i] * dmin[i]\n",
    "\n",
    "    return dmin, grad, unc\n",
    "\n",
    "\n",
    "def finalize_profile_and_write(\n",
    "    prof_rows: List[Dict[str, str]],\n",
    "    writer: csv.DictWriter,\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Return (n_in_rows, n_out_rows_written) for this profile.\n",
    "    \"\"\"\n",
    "    if not prof_rows:\n",
    "        return 0, 0\n",
    "\n",
    "    n_in = len(prof_rows)\n",
    "\n",
    "    # Keep metadata from first row\n",
    "    date_s = prof_rows[0].get(\"Date\", \"\")\n",
    "    lat_s  = prof_rows[0].get(\"Latitude\", \"\")\n",
    "    lon_s  = prof_rows[0].get(\"Longitude\", \"\")\n",
    "    src_s  = prof_rows[0].get(\"Source\", prof_rows[0].get(\"Sourcez\", \"\"))  # tolerate typo\n",
    "\n",
    "    # Collect by Pressure (dedup)\n",
    "    bucket: Dict[float, List[Tuple[float, float, float]]] = {}\n",
    "    for r in prof_rows:\n",
    "        p = to_float(r.get(\"Pressure\", \"\"))\n",
    "        if not np.isfinite(p):\n",
    "            continue\n",
    "        t = to_float(r.get(\"Temperature\", \"\"))\n",
    "        s = to_float(r.get(\"Salinity\", \"\"))\n",
    "        o = to_float(r.get(\"Oxygen\", \"\"))\n",
    "        bucket.setdefault(p, []).append((t, s, o))\n",
    "\n",
    "    if len(bucket) < 2:\n",
    "        return n_in, 0\n",
    "\n",
    "    p_uniq = np.array(sorted(bucket.keys()), dtype=np.float64)\n",
    "\n",
    "    # Average duplicates per pressure (ignore NaNs)\n",
    "    t_uniq = np.full_like(p_uniq, np.nan, dtype=np.float64)\n",
    "    s_uniq = np.full_like(p_uniq, np.nan, dtype=np.float64)\n",
    "    o_uniq = np.full_like(p_uniq, np.nan, dtype=np.float64)\n",
    "\n",
    "    for i, p in enumerate(p_uniq):\n",
    "        vals = bucket[p]\n",
    "        tt = np.array([v[0] for v in vals], dtype=np.float64)\n",
    "        ss = np.array([v[1] for v in vals], dtype=np.float64)\n",
    "        oo = np.array([v[2] for v in vals], dtype=np.float64)\n",
    "\n",
    "        if np.any(np.isfinite(tt)):\n",
    "            t_uniq[i] = np.nanmean(tt)\n",
    "        if np.any(np.isfinite(ss)):\n",
    "            s_uniq[i] = np.nanmean(ss)\n",
    "        if np.any(np.isfinite(oo)):\n",
    "            o_uniq[i] = np.nanmean(oo)\n",
    "\n",
    "    # Only oxygen-finite points participate in oxygen interpolation + support tests + uncertainty proxy\n",
    "    ok_o = np.isfinite(o_uniq)\n",
    "    if np.count_nonzero(ok_o) < 2:\n",
    "        return n_in, 0\n",
    "\n",
    "    p_obs = p_uniq[ok_o]\n",
    "    o_obs = o_uniq[ok_o]\n",
    "\n",
    "    # Define standard depths within observed *pressure* range (overall)\n",
    "    pmin = float(np.nanmin(p_uniq))\n",
    "    pmax = float(np.nanmax(p_uniq))\n",
    "    z = STD_DEPTHS[(STD_DEPTHS >= pmin) & (STD_DEPTHS <= pmax)]\n",
    "    if z.size == 0:\n",
    "        return n_in, 0\n",
    "\n",
    "    # PCHIP on whole profile (no extrapolation)\n",
    "    o_pchip = PchipInterpolator(p_obs, o_obs, extrapolate=False)\n",
    "    o_z = o_pchip(z)\n",
    "\n",
    "    # Temperature/Salinity optional interpolation (only if >=2 finite points)\n",
    "    t_z = np.full_like(o_z, np.nan, dtype=np.float64)\n",
    "    s_z = np.full_like(o_z, np.nan, dtype=np.float64)\n",
    "\n",
    "    ok_t = np.isfinite(t_uniq)\n",
    "    if np.count_nonzero(ok_t) >= 2:\n",
    "        t_pchip = PchipInterpolator(p_uniq[ok_t], t_uniq[ok_t], extrapolate=False)\n",
    "        t_z = t_pchip(z)\n",
    "\n",
    "    ok_s = np.isfinite(s_uniq)\n",
    "    if np.count_nonzero(ok_s) >= 2:\n",
    "        s_pchip = PchipInterpolator(p_uniq[ok_s], s_uniq[ok_s], extrapolate=False)\n",
    "        s_z = s_pchip(z)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Post-filter rules (based on oxygen-observation pressures p_obs)\n",
    "    # Rule1: within ±x(z) has >=1 oxygen obs\n",
    "    # Rule2: remaining: within ±y(z) has >=2 oxygen obs\n",
    "    # ----------------------------\n",
    "    keep = np.zeros(z.shape, dtype=bool)\n",
    "    dist = np.abs(z[:, None] - p_obs[None, :])  # (nz, nobs_oxygen)\n",
    "\n",
    "    x_vec = np.array([x_window(float(zz)) for zz in z], dtype=np.float64)\n",
    "    cnt_x = np.sum(dist <= x_vec[:, None], axis=1)\n",
    "    keep |= (cnt_x >= 1)\n",
    "\n",
    "    y_vec = np.array([y_window(float(zz)) for zz in z], dtype=np.float64)\n",
    "    cnt_y = np.sum(dist <= y_vec[:, None], axis=1)\n",
    "    keep |= ((~keep) & (cnt_y >= 2))\n",
    "\n",
    "    # Also require interpolated oxygen finite; and we do NOT output NaN rows\n",
    "    keep &= np.isfinite(o_z)\n",
    "    if not np.any(keep):\n",
    "        return n_in, 0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Interpolation uncertainty proxy: |dO/dp| * dmin\n",
    "    # computed for all z, then only written for kept levels\n",
    "    # ----------------------------\n",
    "    _dmin, _grad, unc = estimate_local_grad_and_dmin(p_obs, o_obs, z)\n",
    "\n",
    "    # Write kept rows\n",
    "    n_out = 0\n",
    "    for zz, tt, ss, oo, uu, kk in zip(z, t_z, s_z, o_z, unc, keep):\n",
    "        if not kk:\n",
    "            continue\n",
    "\n",
    "        out_row = {\n",
    "            \"Date\": date_s,\n",
    "            \"Pressure\": f\"{float(zz):.0f}\",\n",
    "            \"Latitude\": lat_s,\n",
    "            \"Longitude\": lon_s,\n",
    "            \"Temperature\": \"\" if not np.isfinite(tt) else f\"{float(tt):.6f}\",\n",
    "            \"Salinity\": \"\" if not np.isfinite(ss) else f\"{float(ss):.6f}\",\n",
    "            \"Oxygen\": f\"{float(oo):.6f}\",\n",
    "            \"sigma_interp\": \"\" if not np.isfinite(uu) else f\"{float(uu):.6f}\",\n",
    "            \"Source\": src_s,\n",
    "        }\n",
    "        writer.writerow(out_row)\n",
    "        n_out += 1\n",
    "\n",
    "    return n_in, n_out\n",
    "\n",
    "\n",
    "def process_one_file(path: str, overwrite: bool = False) -> None:\n",
    "    in_path = path\n",
    "    out_path = path if overwrite else (path.replace(\".csv\", \"_stddepth.csv\"))\n",
    "    tmp_path = out_path + \".tmp\"\n",
    "\n",
    "    fieldnames = [\n",
    "        \"Date\", \"Pressure\", \"Latitude\", \"Longitude\",\n",
    "        \"Temperature\", \"Salinity\", \"Oxygen\",\n",
    "        \"sigma_interp\",\n",
    "        \"Source\"\n",
    "    ]\n",
    "\n",
    "    total_profiles = 0\n",
    "    total_in_rows = 0\n",
    "    total_out_rows = 0\n",
    "\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\", newline=\"\") as fin, \\\n",
    "         open(tmp_path, \"w\", encoding=\"utf-8\", newline=\"\") as fout:\n",
    "\n",
    "        reader = csv.DictReader(fin)\n",
    "        writer = csv.DictWriter(fout, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        writer.writeheader()\n",
    "\n",
    "        current_key: Optional[Tuple[str, str, str]] = None\n",
    "        prof_rows: List[Dict[str, str]] = []\n",
    "\n",
    "        for row in reader:\n",
    "            key = profile_key_from_row(row)\n",
    "            if current_key is None:\n",
    "                current_key = key\n",
    "\n",
    "            if key != current_key:\n",
    "                # finalize previous profile\n",
    "                total_profiles += 1\n",
    "                n_in, n_out = finalize_profile_and_write(prof_rows, writer)\n",
    "                total_in_rows += n_in\n",
    "                total_out_rows += n_out\n",
    "\n",
    "                # reset\n",
    "                prof_rows = [row]\n",
    "                current_key = key\n",
    "            else:\n",
    "                prof_rows.append(row)\n",
    "\n",
    "        # last profile\n",
    "        if prof_rows:\n",
    "            total_profiles += 1\n",
    "            n_in, n_out = finalize_profile_and_write(prof_rows, writer)\n",
    "            total_in_rows += n_in\n",
    "            total_out_rows += n_out\n",
    "\n",
    "    if overwrite:\n",
    "        atomic_replace(tmp_path, out_path)\n",
    "        print(f\"[OK] {in_path} -> {out_path} (overwritten)\")\n",
    "    else:\n",
    "        atomic_replace(tmp_path, out_path)\n",
    "        print(f\"[OK] {in_path} -> {out_path}\")\n",
    "\n",
    "    print(f\"  profiles={total_profiles} | in_rows={total_in_rows} | out_rows={total_out_rows}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--overwrite\", action=\"store_true\", help=\"Atomic replace original file(s).\")\n",
    "    # Jupyter/ipykernel injects extra args like --f=xxx.json; ignore them safely:\n",
    "    args, _unknown = ap.parse_known_args()\n",
    "\n",
    "    for f in INPUT_FILES:\n",
    "        if not os.path.exists(f):\n",
    "            print(f\"[SKIP] missing: {f}\")\n",
    "            continue\n",
    "        process_one_file(f, overwrite=bool(args.overwrite))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove duplicate sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Layered export of \"metadata CSV\" (CCHDO / GEOTRACES-IDP2021 / GLODAP / OceanSITES, etc.):\n",
    "\n",
    "- One output file per standard depth level (depth1):\n",
    "    ROOT_DIR/{depth1}dbar/depth{depth1}.csv\n",
    "\n",
    "- Append mode:\n",
    "    * If the file does not exist -> write header + data\n",
    "    * If the file exists -> append rows (no header)\n",
    "    * If an existing file has a different header:\n",
    "        - back up the old file as *.bak\n",
    "        - remove/replace and re-create with the correct header\n",
    "\n",
    "- The final output must contain ONLY the following columns (fixed order):\n",
    "    Date,Time,Pressure,Latitude,Longitude,Temperature,Salinity,Oxygen,Source,sigma_interp\n",
    "\n",
    "- Date is normalized to YYYY-MM-DD\n",
    "- Time is derived from Date:\n",
    "    * If the original has no time component OR time is 00:00 -> NA\n",
    "- Source filtering:\n",
    "    * EXCLUDE_SOURCES (case-insensitive; internally compared in uppercase)\n",
    "\n",
    "Numeric formatting enforced by this script:\n",
    "- Latitude / Longitude: 4 decimals\n",
    "- Pressure: integer (rounded to int)\n",
    "- Temperature / Salinity: 2 decimals\n",
    "- Oxygen: 2 decimals\n",
    "- sigma_interp: if numeric -> 6 decimals; otherwise empty\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "\n",
    "INPUT_CSVS = [\n",
    "    \"/data/wang/CCHDO/cchdo_bottle_filtered_stddepth.csv\",\n",
    "    \"/data/wang/CCHDO/cchdo_ctd_filtered_stddepth.csv\",\n",
    "    \"/data/wang/Geotraces IDP2021/GEOTRACES_IDP2021_filtered_stddepth.csv\",\n",
    "    \"/data/wang/GLODAP/GLODAP2023_filtered_stddepth.csv\",\n",
    "    \"/data/wang/OceanSItes/OceanSITES_filtered_stddepth.csv\",\n",
    "]\n",
    "\n",
    "ROOT_DIR = \"/data/wang/Result_Data/alldoxy\"\n",
    "CHUNKSIZE = 1_000_000\n",
    "ENCODING = \"utf-8-sig\"\n",
    "\n",
    "# Input columns (explicitly includes sigma_interp)\n",
    "INPUT_COLUMNS = [\n",
    "    \"Date\", \"Latitude\", \"Longitude\", \"Pressure\",\n",
    "    \"Temperature\", \"Salinity\", \"Oxygen\", \"Source\",\n",
    "    \"sigma_interp\"\n",
    "]\n",
    "\n",
    "# Final output columns (ONLY these; fixed order; sigma_interp after Source)\n",
    "OUTPUT_COLUMNS = [\n",
    "    \"Date\", \"Time\", \"Pressure\", \"Latitude\", \"Longitude\",\n",
    "    \"Temperature\", \"Salinity\", \"Oxygen\", \"Source\", \"sigma_interp\"\n",
    "]\n",
    "\n",
    "# Source filtering: excluded sources are not exported (and not used for profile counts)\n",
    "EXCLUDE_SOURCES = {\"ARGO\", \"PFL\", \"GLD\", \"DRB\", \"UOR\"}\n",
    "\n",
    "# \"Profile\" definition: count unique (Date, Lat, Lon) per (input file × Source)\n",
    "PROFILE_COORD_ROUND = 4   # None = no rounding; 4 = round to 4 decimals (recommended)\n",
    "\n",
    "# Numeric formatting controls\n",
    "LATLON_DECIMALS = 4\n",
    "TS_DECIMALS = 2\n",
    "OXY_DECIMALS = 2\n",
    "UNC_DECIMALS = 6\n",
    "PRESSURE_ROUND_TO_INT = True\n",
    "\n",
    "# Standard depth levels (dbar)\n",
    "DEPTH1_LIST = [\n",
    "    1,10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "DEPTH1_ARR = np.asarray(DEPTH1_LIST, dtype=np.float64)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Depth matching (vectorized)\n",
    "# =========================\n",
    "def depth_range_vec(p: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pressure (dbar) -> tolerance window used for first-hit binning:\n",
    "      <=10    -> 1\n",
    "      <=200   -> 1.5\n",
    "      <=1000  -> 2\n",
    "      <=2000  -> 5\n",
    "      >2000   -> 10\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    return np.select(\n",
    "        [p <= 10, p <= 200, p <= 1000, p <= 2000],\n",
    "        [1.0, 1.5, 2.0, 5.0],\n",
    "        default=10.0\n",
    "    ).astype(np.float64, copy=False)\n",
    "\n",
    "\n",
    "def match_depth1_firsthit(p: np.ndarray, r: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Find the FIRST depth1 in DEPTH1_LIST (ascending) that hits the window:\n",
    "      depth1 - r <= p <= depth1 + r\n",
    "    Returns matched_depth1 (float; NaN if no hit).\n",
    "    \"\"\"\n",
    "    p = p.astype(np.float64, copy=False)\n",
    "    r = r.astype(np.float64, copy=False)\n",
    "\n",
    "    lower = p - r\n",
    "    upper = p + r\n",
    "\n",
    "    idx = np.searchsorted(DEPTH1_ARR, lower, side=\"left\")\n",
    "    ok = idx < DEPTH1_ARR.size\n",
    "\n",
    "    matched = np.full(p.shape, np.nan, dtype=np.float64)\n",
    "    cand = np.empty_like(p, dtype=np.float64)\n",
    "    cand[ok] = DEPTH1_ARR[idx[ok]]\n",
    "\n",
    "    hit = ok & (cand <= upper)\n",
    "    matched[hit] = cand[hit]\n",
    "    return matched\n",
    "\n",
    "\n",
    "def ensure_depth_folders():\n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    for d1 in DEPTH1_LIST:\n",
    "        os.makedirs(os.path.join(ROOT_DIR, f\"{d1}dbar\"), exist_ok=True)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Date -> Date(YYYY-MM-DD) + Time(HH:MM or NA)\n",
    "# =========================\n",
    "def derive_date_time(date_series: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    ss = date_series.astype(\"string\")\n",
    "    has_time = ss.str.contains(\":\", regex=False)\n",
    "\n",
    "    dt = pd.to_datetime(ss, errors=\"coerce\")\n",
    "\n",
    "    date_out = dt.dt.strftime(\"%Y-%m-%d\").astype(\"string\")\n",
    "    time_out = dt.dt.strftime(\"%H:%M\").astype(\"string\")\n",
    "\n",
    "    time_out = time_out.mask(~has_time, pd.NA)\n",
    "    time_out = time_out.mask(time_out == \"00:00\", pd.NA)\n",
    "\n",
    "    date_out = date_out.mask(dt.isna(), pd.NA)\n",
    "    time_out = time_out.mask(dt.isna(), pd.NA)\n",
    "\n",
    "    return date_out, time_out\n",
    "\n",
    "\n",
    "def normalize_coord_series(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if PROFILE_COORD_ROUND is None:\n",
    "        return x\n",
    "    return x.round(int(PROFILE_COORD_ROUND))\n",
    "\n",
    "\n",
    "def ensure_out_header_compatible(out_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    If out_csv already exists but its header is not identical to OUTPUT_COLUMNS:\n",
    "    - back up as *.bak\n",
    "    - remove/replace the original file\n",
    "    so that subsequent appends do not misalign columns.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(out_csv):\n",
    "        return\n",
    "    try:\n",
    "        with open(out_csv, \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
    "            first = f.readline().strip(\"\\n\\r\")\n",
    "        existing = [x.strip() for x in first.split(\",\")]\n",
    "        if existing == OUTPUT_COLUMNS:\n",
    "            return\n",
    "        bak = out_csv + \".bak\"\n",
    "        os.replace(out_csv, bak)\n",
    "        print(f\"  [WARN] header mismatch: backed up existing file -> {bak}\")\n",
    "    except Exception:\n",
    "        bak = out_csv + \".bak\"\n",
    "        os.replace(out_csv, bak)\n",
    "        print(f\"  [WARN] failed to validate header, backed up -> {bak}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Per-file processing (per-depth outputs + profile stats)\n",
    "# =========================\n",
    "def process_one_file(csv_path: str):\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[SKIP] not found: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n[FILE] {csv_path}\")\n",
    "\n",
    "    # For each file: source -> set(uint64 hashes)\n",
    "    profile_hash_sets: dict[str, set[int]] = {}\n",
    "\n",
    "    total_rows_in = 0\n",
    "    total_rows_after_source_filter = 0\n",
    "    total_rows_after_date_valid = 0\n",
    "    total_rows_after_pressure_valid = 0\n",
    "    total_rows_written = 0\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        csv_path,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        usecols=INPUT_COLUMNS,\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "    for chunk_idx, df in enumerate(reader, start=1):\n",
    "        total_rows_in += len(df)\n",
    "        print(f\"  Processing chunk {chunk_idx} ... rows={len(df)}\")\n",
    "\n",
    "        # ---------- Normalize Source + filter ----------\n",
    "        src_norm = df[\"Source\"].astype(\"string\").str.strip().str.upper()\n",
    "        df = df.loc[~src_norm.isin(EXCLUDE_SOURCES)].copy()\n",
    "        src_norm = src_norm.loc[df.index]\n",
    "        total_rows_after_source_filter += len(df)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # ---------- Date -> Date + Time (drop rows with invalid Date) ----------\n",
    "        date_norm, time_norm = derive_date_time(df[\"Date\"])\n",
    "        df[\"Date\"] = date_norm\n",
    "        df[\"Time\"] = time_norm\n",
    "\n",
    "        df = df.dropna(subset=[\"Date\"]).copy()\n",
    "        src_norm = src_norm.loc[df.index]\n",
    "        total_rows_after_date_valid += len(df)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # ---------- Convert numeric columns ----------\n",
    "        lat = pd.to_numeric(df[\"Latitude\"], errors=\"coerce\")\n",
    "        lon = pd.to_numeric(df[\"Longitude\"], errors=\"coerce\")\n",
    "        p   = pd.to_numeric(df[\"Pressure\"], errors=\"coerce\")\n",
    "        tmp = pd.to_numeric(df[\"Temperature\"], errors=\"coerce\")\n",
    "        sal = pd.to_numeric(df[\"Salinity\"], errors=\"coerce\")\n",
    "        oxy = pd.to_numeric(df[\"Oxygen\"], errors=\"coerce\")\n",
    "        sig = pd.to_numeric(df[\"sigma_interp\"], errors=\"coerce\")\n",
    "\n",
    "        # ---------- Profile key: (Date, Lat, Lon) + Source ----------\n",
    "        lat_key = normalize_coord_series(lat)\n",
    "        lon_key = normalize_coord_series(lon)\n",
    "\n",
    "        key_valid = df[\"Date\"].notna() & lat_key.notna() & lon_key.notna() & src_norm.notna()\n",
    "        if key_valid.any():\n",
    "            key_df = pd.DataFrame({\n",
    "                \"Date\": df.loc[key_valid, \"Date\"].astype(\"string\"),\n",
    "                \"Lat\":  lat_key.loc[key_valid].astype(np.float64),\n",
    "                \"Lon\":  lon_key.loc[key_valid].astype(np.float64),\n",
    "            })\n",
    "            key_hash = pd.util.hash_pandas_object(key_df, index=False).to_numpy(dtype=np.uint64, copy=False)\n",
    "            src_sub = src_norm.loc[key_valid].to_numpy()\n",
    "\n",
    "            for s in np.unique(src_sub):\n",
    "                m = (src_sub == s)\n",
    "                if not m.any():\n",
    "                    continue\n",
    "                u = np.unique(key_hash[m])\n",
    "                st = profile_hash_sets.get(s)\n",
    "                if st is None:\n",
    "                    st = set()\n",
    "                    profile_hash_sets[s] = st\n",
    "                st.update(u.tolist())\n",
    "\n",
    "        # ---------- Pressure validity (required for depth binning) ----------\n",
    "        valid_p = np.isfinite(p.to_numpy(dtype=np.float64, copy=False))\n",
    "        df = df.loc[valid_p].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Re-align series to the filtered index\n",
    "        lat = lat.loc[df.index]\n",
    "        lon = lon.loc[df.index]\n",
    "        p   = p.loc[df.index]\n",
    "        tmp = tmp.loc[df.index]\n",
    "        sal = sal.loc[df.index]\n",
    "        oxy = oxy.loc[df.index]\n",
    "        sig = sig.loc[df.index]\n",
    "        src_norm = src_norm.loc[df.index]\n",
    "\n",
    "        total_rows_after_pressure_valid += len(df)\n",
    "\n",
    "        # ---------- Numeric formatting ----------\n",
    "        lat = lat.round(LATLON_DECIMALS)\n",
    "        lon = lon.round(LATLON_DECIMALS)\n",
    "\n",
    "        if PRESSURE_ROUND_TO_INT:\n",
    "            p_int = np.rint(p.to_numpy(dtype=np.float64, copy=False)).astype(np.int32, copy=False)\n",
    "            p = pd.Series(p_int, index=df.index)\n",
    "        else:\n",
    "            p = p.astype(np.float64).copy()\n",
    "\n",
    "        tmp = tmp.round(TS_DECIMALS)\n",
    "        sal = sal.round(TS_DECIMALS)\n",
    "        oxy = oxy.round(OXY_DECIMALS)\n",
    "        sig = sig.round(UNC_DECIMALS)\n",
    "\n",
    "        # ---------- Assign matched_depth1 (use integer Pressure for consistent binning/output) ----------\n",
    "        p_arr = p.to_numpy(dtype=np.float64, copy=False)\n",
    "        r = depth_range_vec(p_arr)\n",
    "        matched = match_depth1_firsthit(p_arr, r)\n",
    "\n",
    "        df[\"matched_depth1\"] = matched\n",
    "        df = df.dropna(subset=[\"matched_depth1\"]).copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df[\"matched_depth1\"] = df[\"matched_depth1\"].astype(np.int32)\n",
    "\n",
    "        # ---------- Build final output (strict columns + formatted values) ----------\n",
    "        df[\"Latitude\"] = lat\n",
    "        df[\"Longitude\"] = lon\n",
    "        df[\"Pressure\"] = p\n",
    "        df[\"Temperature\"] = tmp\n",
    "        df[\"Salinity\"] = sal\n",
    "        df[\"Oxygen\"] = oxy\n",
    "        df[\"Source\"] = src_norm  # write Source in normalized uppercase\n",
    "        df[\"sigma_interp\"] = sig\n",
    "\n",
    "        # ---------- Grouped append-write by depth ----------\n",
    "        for depth1, g in df.groupby(\"matched_depth1\", sort=False):\n",
    "            depth1 = int(depth1)\n",
    "            out_dir = os.path.join(ROOT_DIR, f\"{depth1}dbar\")\n",
    "            out_csv = os.path.join(out_dir, f\"depth{depth1}.csv\")\n",
    "\n",
    "            ensure_out_header_compatible(out_csv)\n",
    "\n",
    "            g_out = g[OUTPUT_COLUMNS]\n",
    "            write_header = not os.path.isfile(out_csv)\n",
    "\n",
    "            g_out.to_csv(\n",
    "                out_csv,\n",
    "                index=False,\n",
    "                mode=\"w\" if write_header else \"a\",\n",
    "                header=write_header,\n",
    "                encoding=ENCODING\n",
    "            )\n",
    "            total_rows_written += len(g_out)\n",
    "\n",
    "    per_source_profiles = {s: len(st) for s, st in profile_hash_sets.items()}\n",
    "\n",
    "    return {\n",
    "        \"input_file\": csv_path,\n",
    "        \"rows_in\": int(total_rows_in),\n",
    "        \"rows_after_source_filter\": int(total_rows_after_source_filter),\n",
    "        \"rows_after_date_valid\": int(total_rows_after_date_valid),\n",
    "        \"rows_after_pressure_valid\": int(total_rows_after_pressure_valid),\n",
    "        \"rows_written\": int(total_rows_written),\n",
    "        \"profiles_by_source\": per_source_profiles\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main entry\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_depth_folders()\n",
    "\n",
    "    log_rows = []\n",
    "    file_summaries = []\n",
    "\n",
    "    for f in INPUT_CSVS:\n",
    "        res = process_one_file(f)\n",
    "        if res is None:\n",
    "            continue\n",
    "\n",
    "        file_summaries.append({\n",
    "            \"input_file\": res[\"input_file\"],\n",
    "            \"rows_in\": res[\"rows_in\"],\n",
    "            \"rows_after_source_filter\": res[\"rows_after_source_filter\"],\n",
    "            \"rows_after_date_valid\": res[\"rows_after_date_valid\"],\n",
    "            \"rows_after_pressure_valid\": res[\"rows_after_pressure_valid\"],\n",
    "            \"rows_written\": res[\"rows_written\"],\n",
    "        })\n",
    "\n",
    "        prof = res[\"profiles_by_source\"]\n",
    "        if len(prof) == 0:\n",
    "            log_rows.append({\"input_file\": res[\"input_file\"], \"Source\": \"\", \"n_profiles_DateLatLon\": 0})\n",
    "        else:\n",
    "            for s, n in sorted(prof.items(), key=lambda x: x[0]):\n",
    "                log_rows.append({\"input_file\": res[\"input_file\"], \"Source\": s, \"n_profiles_DateLatLon\": int(n)})\n",
    "\n",
    "    log_path = os.path.join(ROOT_DIR, \"profile_counts_by_source.csv\")\n",
    "    pd.DataFrame(log_rows).to_csv(log_path, index=False, encoding=ENCODING)\n",
    "\n",
    "    summary_path = os.path.join(ROOT_DIR, \"file_processing_summary.csv\")\n",
    "    pd.DataFrame(file_summaries).to_csv(summary_path, index=False, encoding=ENCODING)\n",
    "\n",
    "    print(\"\\n[DONE] Layered export of metadata CSVs completed.\")\n",
    "    print(f\"[LOG] Profile counts: {log_path}\")\n",
    "    print(f\"[LOG] File processing summary: {summary_path}\")\n",
    "    print(\"[INFO] Per-depth outputs include sigma_interp after Source.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
