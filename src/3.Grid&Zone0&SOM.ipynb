{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Monthly grid aggregation for Oxygen + representativeness error proxy (SIGMA_rep)\n",
    "Python 3.9 + Jupyter compatible.\n",
    "\n",
    "Implemented requirements:\n",
    "- Process TARGET_DEPTHS under: /data/wang/Result_Data/alldoxy/{dep}dbar/*TRAIN.csv\n",
    "  (also tolerates \"{dep}dabr\")\n",
    "- Output ONLY ONE file per depth:\n",
    "    {dep}dbar/depth{dep}_TRAIN.csv\n",
    "- Original *TRAIN.csv are NOT deleted; they are renamed to:\n",
    "    <stem>__ORIG.csv  (collision-safe)\n",
    "- If output file already exists, rename it aside first:\n",
    "    depth{dep}_TRAIN__ORIG_<timestamp>.csv\n",
    "\n",
    "Binning:\n",
    "- Longitude: wrap to [-180,180), then floor-bin to 0.5° anchored at -180\n",
    "  output with ONE decimal (string): -180.0, -179.5, ..., 179.5\n",
    "- Latitude: nearest center mapping to lat_centers.txt\n",
    "  output EXACT strings from lat_centers.txt\n",
    "\n",
    "Aggregation per (Date=YYYY-MM-15, Year, Month, Latitude, Longitude, Pressure):\n",
    "- Oxygen: median(Oxygen)\n",
    "- Oxygen_MAD: median(|Oxygen - median(Oxygen)|)\n",
    "- SIGMA_rep: 1.4826 * Oxygen_MAD (string trimmed zeros)\n",
    "- sigma_interp: median(sigma_interp)\n",
    "- n_obs: count\n",
    "- Source: Source of observation closest to median Oxygen (tie -> first)\n",
    "- Source_fraction:\n",
    "    - if n_obs == 1 => empty string \"\"\n",
    "    - else JSON string of per-source COUNTS sorted by count desc then name asc\n",
    "\n",
    "Notes:\n",
    "- Source assumed always present.\n",
    "- Oxygen assumed already valid (0,600).\n",
    "\"\"\"\n",
    "\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User configuration\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "LAT_CENTERS_PATH = ROOT_DIR / \"lat_centers.txt\"\n",
    "\n",
    "TARGET_DEPTHS = [\n",
    "    1\n",
    "]\n",
    "\n",
    "# Lon bins definition\n",
    "LON_START = -180.0\n",
    "LON_END_EXCL = 180.0\n",
    "LON_RES = 0.5\n",
    "LON_BINS = np.arange(LON_START, LON_END_EXCL, LON_RES, dtype=np.float64)  # -180 ... 179.5\n",
    "\n",
    "# Minimal columns required from TRAIN.csv\n",
    "USECOLS = [\"Date\", \"Latitude\", \"Longitude\", \"Oxygen\", \"Source\", \"sigma_interp\"]\n",
    "ENCODINGS_TO_TRY = (\"utf-8\", \"utf-8-sig\", \"latin1\")\n",
    "LOW_MEMORY = False\n",
    "\n",
    "# If RAM is tight, set chunksize (e.g., 1_000_000). None = read whole file.\n",
    "CHUNKSIZE = None  # e.g., 1_000_000\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_lat_centers(path: Path) -> Dict[str, np.ndarray]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"lat_centers.txt not found: {path}\")\n",
    "\n",
    "    centers_str: List[str] = []\n",
    "    centers_float: List[float] = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            centers_str.append(s)\n",
    "            centers_float.append(float(s))\n",
    "\n",
    "    if not centers_str:\n",
    "        raise ValueError(f\"lat_centers.txt is empty: {path}\")\n",
    "\n",
    "    cf = np.array(centers_float, dtype=np.float64)\n",
    "    cs = np.array(centers_str, dtype=object)\n",
    "\n",
    "    if not np.all(cf[1:] >= cf[:-1]):\n",
    "        idx = np.argsort(cf)\n",
    "        cf = cf[idx]\n",
    "        cs = cs[idx]\n",
    "\n",
    "    return {\"centers_float\": cf, \"centers_str\": cs}\n",
    "\n",
    "\n",
    "def safe_read_csv(path: Path, usecols: List[str]):\n",
    "    last_err = None\n",
    "    for enc in ENCODINGS_TO_TRY:\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                path,\n",
    "                usecols=usecols,\n",
    "                encoding=enc,\n",
    "                low_memory=LOW_MEMORY,\n",
    "                chunksize=CHUNKSIZE,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to read {path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "\n",
    "def wrap_lon_to_180(lon: np.ndarray) -> np.ndarray:\n",
    "    x = lon.astype(np.float64, copy=False)\n",
    "    out = ((x + 180.0) % 360.0) - 180.0\n",
    "    out[out == 180.0] = -180.0\n",
    "    return out\n",
    "\n",
    "\n",
    "def lon_to_bin_floor(lon_wrapped: np.ndarray) -> np.ndarray:\n",
    "    x = lon_wrapped.astype(np.float64, copy=False)\n",
    "    x = np.clip(x, -180.0, np.nextafter(180.0, -np.inf))\n",
    "    idx = np.floor((x - LON_START) / LON_RES).astype(np.int64)\n",
    "    idx = np.clip(idx, 0, len(LON_BINS) - 1)\n",
    "    return (LON_START + idx * LON_RES).astype(np.float64)\n",
    "\n",
    "\n",
    "def lat_to_nearest_center_index(lat: np.ndarray, centers_float: np.ndarray) -> np.ndarray:\n",
    "    x = lat.astype(np.float64, copy=False)\n",
    "    x = np.clip(x, centers_float[0], centers_float[-1])\n",
    "\n",
    "    idx = np.searchsorted(centers_float, x, side=\"left\")\n",
    "    idx = np.clip(idx, 0, len(centers_float) - 1)\n",
    "\n",
    "    idx0 = np.maximum(idx - 1, 0)\n",
    "    idx1 = idx\n",
    "\n",
    "    c0 = centers_float[idx0]\n",
    "    c1 = centers_float[idx1]\n",
    "\n",
    "    choose_left = np.abs(x - c0) <= np.abs(x - c1)\n",
    "    return np.where(choose_left, idx0, idx1).astype(np.int64)\n",
    "\n",
    "\n",
    "def find_depth_folders(root: Path, target_depths: List[int]) -> Dict[int, Path]:\n",
    "    out: Dict[int, Path] = {}\n",
    "    pat = re.compile(r\"^(\\d+)dba[rr]$\")  # dbar or dabr\n",
    "    for p in root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = pat.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        dep = int(m.group(1))\n",
    "        if dep not in target_depths:\n",
    "            continue\n",
    "        if dep in out:\n",
    "            if p.name.endswith(\"dbar\"):\n",
    "                out[dep] = p\n",
    "        else:\n",
    "            out[dep] = p\n",
    "    return out\n",
    "\n",
    "\n",
    "def list_train_files(depth_dir: Path) -> List[Path]:\n",
    "    return sorted([p for p in depth_dir.glob(\"*TRAIN.csv\") if p.is_file()])\n",
    "\n",
    "\n",
    "def make_date_ym15(year: pd.Series, month: pd.Series) -> pd.Series:\n",
    "    y = year.astype(np.int32)\n",
    "    m = month.astype(np.int32)\n",
    "    return y.astype(str).str.zfill(4) + \"-\" + m.astype(str).str.zfill(2) + \"-15\"\n",
    "\n",
    "\n",
    "def fmt_sigma_rep(x: float) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    s = f\"{float(x):.12f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return s if s else \"0\"\n",
    "\n",
    "\n",
    "def fmt_lon_one_decimal(x: float) -> str:\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return f\"{float(x):.1f}\"\n",
    "\n",
    "\n",
    "def source_fraction_json_count(src_series: pd.Series) -> str:\n",
    "    vc = src_series.astype(\"string\").str.strip().value_counts(dropna=True)\n",
    "    if vc.sum() == 0:\n",
    "        return \"{}\"\n",
    "    items = [(str(k), int(v)) for k, v in vc.items()]\n",
    "    items.sort(key=lambda kv: (-kv[1], kv[0]))\n",
    "    obj = {k: v for k, v in items}\n",
    "    return json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\"))\n",
    "\n",
    "\n",
    "def normalize_chunk(df: pd.DataFrame, lat_centers: Dict[str, np.ndarray], depth_dbar: int) -> pd.DataFrame:\n",
    "    dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    year = dt.dt.year\n",
    "    month = dt.dt.month\n",
    "\n",
    "    lat = pd.to_numeric(df[\"Latitude\"], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(df[\"Longitude\"], errors=\"coerce\")\n",
    "    oxy = pd.to_numeric(df[\"Oxygen\"], errors=\"coerce\")\n",
    "    sigi = pd.to_numeric(df[\"sigma_interp\"], errors=\"coerce\")\n",
    "    src = df[\"Source\"]\n",
    "\n",
    "    m = year.notna() & month.notna() & lat.notna() & lon.notna() & oxy.notna() & src.notna()\n",
    "    if int(m.sum()) == 0:\n",
    "        return pd.DataFrame(columns=[\"Date\",\"Year\",\"Month\",\"Latitude\",\"Longitude\",\"Pressure\",\"Oxygen\",\"sigma_interp\",\"Source\"])\n",
    "\n",
    "    year = year[m].astype(np.int16)\n",
    "    month = month[m].astype(np.int8)\n",
    "\n",
    "    latv = lat[m].astype(np.float64)\n",
    "    lonv = lon[m].astype(np.float64)\n",
    "    oxyv = oxy[m].astype(np.float64)\n",
    "    sigv = sigi[m].astype(np.float64)\n",
    "    srcv = src[m].astype(\"string\")\n",
    "\n",
    "    lon_wrapped = wrap_lon_to_180(lonv.to_numpy(dtype=np.float64, copy=False))\n",
    "    lon_bin = lon_to_bin_floor(lon_wrapped)\n",
    "\n",
    "    centers_float = lat_centers[\"centers_float\"]\n",
    "    centers_str = lat_centers[\"centers_str\"]\n",
    "    lat_idx = lat_to_nearest_center_index(latv.to_numpy(dtype=np.float64, copy=False), centers_float)\n",
    "    lat_bin_str = centers_str[lat_idx]\n",
    "\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"Year\": year,\n",
    "            \"Month\": month,\n",
    "            \"Latitude\": pd.Series(lat_bin_str, dtype=\"string\"),\n",
    "            \"Longitude\": pd.Series([fmt_lon_one_decimal(v) for v in lon_bin], dtype=\"string\"),\n",
    "            \"Pressure\": int(depth_dbar),\n",
    "            \"Oxygen\": oxyv,\n",
    "            \"sigma_interp\": sigv,\n",
    "            \"Source\": srcv,\n",
    "        }\n",
    "    )\n",
    "    out[\"Date\"] = make_date_ym15(out[\"Year\"], out[\"Month\"])\n",
    "    return out[[\"Date\",\"Year\",\"Month\",\"Latitude\",\"Longitude\",\"Pressure\",\"Oxygen\",\"sigma_interp\",\"Source\"]]\n",
    "\n",
    "\n",
    "def load_and_normalize_all(train_files: List[Path], lat_centers: Dict[str, np.ndarray], depth_dbar: int) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for fp in train_files:\n",
    "        reader_or_df = safe_read_csv(fp, USECOLS)\n",
    "\n",
    "        if CHUNKSIZE is None:\n",
    "            df0 = reader_or_df  # type: ignore\n",
    "            parts.append(normalize_chunk(df0, lat_centers, depth_dbar))\n",
    "            del df0\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for chunk in reader_or_df:  # type: ignore\n",
    "                parts.append(normalize_chunk(chunk, lat_centers, depth_dbar))\n",
    "                del chunk\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"    [READ] {fp.name} -> parts={len(parts)}\")\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=[\"Date\",\"Year\",\"Month\",\"Latitude\",\"Longitude\",\"Pressure\",\"Oxygen\",\"sigma_interp\",\"Source\"])\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    parts.clear()\n",
    "    gc.collect()\n",
    "    return out\n",
    "\n",
    "\n",
    "def backup_original_train_files(train_files: List[Path]) -> None:\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    for fp in train_files:\n",
    "        bak = fp.with_name(f\"{fp.stem}__ORIG.csv\")\n",
    "        if bak.exists():\n",
    "            bak_old = fp.with_name(f\"{fp.stem}__ORIG__OLD_{ts}.csv\")\n",
    "            bak.rename(bak_old)\n",
    "            print(f\"    [MOVE] {bak.name} -> {bak_old.name}\")\n",
    "        fp.rename(bak)\n",
    "        print(f\"    [BACKUP] {fp.name} -> {bak.name}\")\n",
    "\n",
    "\n",
    "def safe_backup_if_exists(path: Path) -> None:\n",
    "    if not path.exists():\n",
    "        return\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    bak = path.with_name(f\"{path.stem}__ORIG_{ts}{path.suffix}\")\n",
    "    path.rename(bak)\n",
    "    print(f\"  [MOVE] Existing output {path.name} -> {bak.name}\")\n",
    "\n",
    "\n",
    "def aggregate(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out_cols = [\n",
    "        \"Date\",\"Year\",\"Month\",\"Latitude\",\"Longitude\",\"Pressure\",\n",
    "        \"Oxygen\",\"Oxygen_MAD\",\"sigma_rep\",\"sigma_interp\",\"n_obs\",\"Source\",\"Source_fraction\"\n",
    "    ]\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=out_cols)\n",
    "\n",
    "    keys = [\"Date\",\"Year\",\"Month\",\"Latitude\",\"Longitude\",\"Pressure\"]\n",
    "    g = df.groupby(keys, sort=False, observed=True)\n",
    "\n",
    "    # median oxygen per row\n",
    "    med_per_row = g[\"Oxygen\"].transform(\"median\")\n",
    "    abs_dev = (df[\"Oxygen\"] - med_per_row).abs()\n",
    "\n",
    "    # aggregated median oxygen\n",
    "    oxy_med = g[\"Oxygen\"].median().rename(\"Oxygen\").reset_index()\n",
    "\n",
    "    # MAD\n",
    "    mad = abs_dev.groupby([df[k] for k in keys], sort=False).median().rename(\"Oxygen_MAD\").reset_index()\n",
    "    mad.columns = keys + [\"Oxygen_MAD\"]\n",
    "\n",
    "    # sigma_interp median\n",
    "    sig_med = g[\"sigma_interp\"].median().rename(\"sigma_interp\").reset_index()\n",
    "\n",
    "    # n_obs\n",
    "    n_obs = g.size().rename(\"n_obs\").reset_index()\n",
    "\n",
    "    # Source closest to median\n",
    "    idx = abs_dev.groupby([df[k] for k in keys], sort=False).idxmin()\n",
    "    src_pick = df.loc[idx, keys + [\"Source\"]].drop_duplicates(subset=keys, keep=\"first\")\n",
    "\n",
    "    # Source_fraction JSON (counts)\n",
    "    src_frac = g[\"Source\"].apply(source_fraction_json_count).rename(\"Source_fraction\").reset_index()\n",
    "\n",
    "    # Merge\n",
    "    agg = (\n",
    "        oxy_med.merge(mad, on=keys, how=\"left\")\n",
    "               .merge(sig_med, on=keys, how=\"left\")\n",
    "               .merge(n_obs, on=keys, how=\"left\")\n",
    "               .merge(src_pick, on=keys, how=\"left\")\n",
    "               .merge(src_frac, on=keys, how=\"left\")\n",
    "    )\n",
    "\n",
    "    # If n_obs == 1 => Source_fraction = \"\"\n",
    "    agg[\"Source_fraction\"] = agg[\"Source_fraction\"].where(agg[\"n_obs\"].astype(\"int64\") > 1, \"\")\n",
    "\n",
    "    # SIGMA_rep formatted string\n",
    "    sigma_rep_num = (1.4826 * agg[\"Oxygen_MAD\"].astype(np.float64)).to_numpy()\n",
    "    agg[\"sigma_rep\"] = [fmt_sigma_rep(v) for v in sigma_rep_num]\n",
    "\n",
    "    # Sort (lat/lon are strings)\n",
    "    agg[\"_Lat_sort\"] = pd.to_numeric(agg[\"Latitude\"], errors=\"coerce\")\n",
    "    agg[\"_Lon_sort\"] = pd.to_numeric(agg[\"Longitude\"], errors=\"coerce\")\n",
    "    agg = (\n",
    "        agg[out_cols + [\"_Lat_sort\",\"_Lon_sort\"]]\n",
    "        .sort_values([\"Year\",\"Month\",\"_Lat_sort\",\"_Lon_sort\"], kind=\"mergesort\")\n",
    "        .drop(columns=[\"_Lat_sort\",\"_Lon_sort\"])\n",
    "    )\n",
    "\n",
    "    gc.collect()\n",
    "    return agg\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def run():\n",
    "    print(\"[INFO] Loading lat centers...\")\n",
    "    lat_centers = read_lat_centers(LAT_CENTERS_PATH)\n",
    "    print(f\"[INFO] lat_centers: n={lat_centers['centers_float'].size} | \"\n",
    "          f\"range=({lat_centers['centers_float'].min():.4f}, {lat_centers['centers_float'].max():.4f})\")\n",
    "    print(f\"[INFO] lon_bins: n={LON_BINS.size} | range=({LON_BINS.min():.1f}, {LON_BINS.max():.1f}) step={LON_RES}\")\n",
    "\n",
    "    depth_dirs = find_depth_folders(ROOT_DIR, TARGET_DEPTHS)\n",
    "    if not depth_dirs:\n",
    "        raise RuntimeError(f\"No target depth folders found under {ROOT_DIR} for given TARGET_DEPTHS (n={len(TARGET_DEPTHS)})\")\n",
    "\n",
    "    for dep in TARGET_DEPTHS:\n",
    "        ddir = depth_dirs.get(dep)\n",
    "        if ddir is None:\n",
    "            print(f\"[SKIP] Missing folder for depth {dep} (searched dbar/dabr).\")\n",
    "            continue\n",
    "\n",
    "        train_files = list_train_files(ddir)\n",
    "        if not train_files:\n",
    "            print(f\"[SKIP] No *TRAIN.csv in {ddir}.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[DEPTH {dep} dbar] folder={ddir} | TRAIN files={len(train_files)}\")\n",
    "\n",
    "        df_all = load_and_normalize_all(train_files, lat_centers, dep)\n",
    "        print(f\"  [INFO] Rows after minimal validity filter: {len(df_all):,}\")\n",
    "\n",
    "        agg_df = aggregate(df_all)\n",
    "        print(f\"  [INFO] Aggregated cells (with obs): {len(agg_df):,}\")\n",
    "\n",
    "        # backup originals\n",
    "        backup_original_train_files(train_files)\n",
    "\n",
    "        # write output \n",
    "        out_path = ddir / f\"depth{dep}_TRAIN.csv\"\n",
    "        safe_backup_if_exists(out_path)\n",
    "\n",
    "        agg_df.to_csv(out_path, index=False)\n",
    "        print(f\"  [OK] Wrote aggregated file: {out_path}\")\n",
    "\n",
    "        del df_all, agg_df\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\n[DONE]\")\n",
    "\n",
    "\n",
    "# ---- Execute in Jupyter ----\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Add per-record observation uncertainty SIGMA_obs based on Source\n",
    "Python 3.9 + Jupyter compatible.\n",
    "\n",
    "Targets:\n",
    "  /data/wang/Result_Data/alldoxy/{1dbar,10dbar}/*TRAIN.csv\n",
    "  (also tolerates folder typo like \"{depth}dabr\")\n",
    "\n",
    "Rule (Source -> SIGMA_obs):\n",
    "  Argo            -> 1.5\n",
    "  OSDCTD          -> 1.5\n",
    "  CCHDO_Bottle    -> 1.0\n",
    "  CCHDO_CTD       -> 1.5\n",
    "  GLODAPV2 2022   -> 1.0\n",
    "  OceanSITES      -> 2.0\n",
    "  Geotraces IDP   -> 1.0\n",
    "\n",
    "Behavior (NO backup):\n",
    "- For each matched *TRAIN.csv:\n",
    "    1) Read original (chunked)\n",
    "    2) Add/overwrite column \"SIGMA_obs\"\n",
    "    3) Write to a temp file in the SAME directory\n",
    "    4) Atomically replace original file via os.replace(temp, original)\n",
    "\n",
    "Notes:\n",
    "- Preserves ALL original columns; only adds \"SIGMA_obs\".\n",
    "- \"SIGMA_obs\" is inserted immediately after \"Source\" column if present; otherwise appended.\n",
    "- Unknown Source -> SIGMA_obs = NaN; prints top unknowns.\n",
    "- Output encoding is UTF-8 (pandas default).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Iterable\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User configuration\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "TARGET_DEPTHS = [\t\t1\n",
    "]   # change if needed\n",
    "\n",
    "CHUNKSIZE = 1_000_000     # set None to disable chunking\n",
    "ENCODINGS_TO_TRY = (\"utf-8\", \"utf-8-sig\", \"latin1\")\n",
    "\n",
    "# Source -> SIGMA_obs mapping\n",
    "SIGMA_OBS_MAP_RAW: Dict[str, float] = {\n",
    "    \"Argo\": 1.5,\n",
    "    \"OSDCTD\": 1.5,\n",
    "    \"CCHDO_Bottle\": 1.0,\n",
    "    \"CCHDO_CTD\": 1.5,\n",
    "    \"GLODAPV2 2022\": 1.0,\n",
    "    \"OceanSITES\": 2.0,\n",
    "    \"Geotraces IDP\": 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _norm_source(s: str) -> str:\n",
    "    \"\"\"Normalize Source for mapping (strip, collapse spaces, case-insensitive).\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    x = str(s).strip()\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    return x.upper()\n",
    "\n",
    "\n",
    "SIGMA_OBS_MAP: Dict[str, float] = {_norm_source(k): float(v) for k, v in SIGMA_OBS_MAP_RAW.items()}\n",
    "\n",
    "\n",
    "def find_depth_folders(root: Path, target_depths: List[int]) -> Dict[int, Path]:\n",
    "    \"\"\"Accept both '{dep}dbar' and typo '{dep}dabr'. Prefer 'dbar' if both exist.\"\"\"\n",
    "    out: Dict[int, Path] = {}\n",
    "    pat = re.compile(r\"^(\\d+)dba[rr]$\")  # dbar or dabr\n",
    "    for p in root.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        m = pat.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        dep = int(m.group(1))\n",
    "        if dep not in target_depths:\n",
    "            continue\n",
    "        if dep in out:\n",
    "            if p.name.endswith(\"dbar\"):\n",
    "                out[dep] = p\n",
    "        else:\n",
    "            out[dep] = p\n",
    "    return out\n",
    "\n",
    "\n",
    "def list_train_files(depth_dir: Path) -> List[Path]:\n",
    "    return sorted([p for p in depth_dir.glob(\"*TRAIN.csv\") if p.is_file()])\n",
    "\n",
    "\n",
    "def safe_read_csv_iter(path: Path) -> Iterable[pd.DataFrame]:\n",
    "    \"\"\"Yield df chunks; reads ALL columns (preserve original attrs).\"\"\"\n",
    "    last_err = None\n",
    "    for enc in ENCODINGS_TO_TRY:\n",
    "        try:\n",
    "            if CHUNKSIZE is None:\n",
    "                yield pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "            else:\n",
    "                for chunk in pd.read_csv(path, encoding=enc, low_memory=False, chunksize=CHUNKSIZE):\n",
    "                    yield chunk\n",
    "            return\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to read {path}. Last error: {repr(last_err)}\")\n",
    "\n",
    "\n",
    "def insert_after_source(cols: List[str], new_col: str) -> List[str]:\n",
    "    \"\"\"Insert new_col immediately after 'Source' if present, else append.\"\"\"\n",
    "    if new_col in cols:\n",
    "        return cols\n",
    "    if \"Source\" in cols:\n",
    "        i = cols.index(\"Source\")\n",
    "        return cols[: i + 1] + [new_col] + cols[i + 1 :]\n",
    "    return cols + [new_col]\n",
    "\n",
    "\n",
    "def add_sigma_obs_inplace(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Add/overwrite SIGMA_obs based on Source mapping; unknown -> NaN.\"\"\"\n",
    "    if \"Source\" not in df.columns:\n",
    "        raise ValueError(\"Missing required column 'Source'\")\n",
    "\n",
    "    src_norm = df[\"Source\"].astype(\"string\").map(lambda x: _norm_source(x))\n",
    "    df[\"sigma_obs\"] = src_norm.map(SIGMA_OBS_MAP).astype(\"float64\")\n",
    "\n",
    "\n",
    "def atomic_replace_csv(original: Path, write_chunks_fn) -> None:\n",
    "    \"\"\"\n",
    "    Write to temp file in same directory, then os.replace(temp, original) for atomic swap.\n",
    "    write_chunks_fn(temp_path) should write full CSV to temp_path.\n",
    "    \"\"\"\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    tmp = original.with_name(f\".{original.name}.tmp_{os.getpid()}_{ts}\")\n",
    "    try:\n",
    "        write_chunks_fn(tmp)\n",
    "\n",
    "        # best-effort fsync temp file for durability\n",
    "        try:\n",
    "            with open(tmp, \"rb\") as f:\n",
    "                os.fsync(f.fileno())\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        os.replace(tmp, original)  # atomic on same filesystem\n",
    "    finally:\n",
    "        if tmp.exists():\n",
    "            # if anything failed before replace\n",
    "            try:\n",
    "                tmp.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main processing\n",
    "# =========================\n",
    "def process_one_file(fp: Path) -> None:\n",
    "    print(f\"  -> Processing: {fp}\")\n",
    "\n",
    "    unknown_sources: Dict[str, int] = {}\n",
    "    total_rows = 0\n",
    "\n",
    "    def _write_to_temp(tmp_path: Path) -> None:\n",
    "        nonlocal total_rows, unknown_sources\n",
    "        first = True\n",
    "\n",
    "        for chunk in safe_read_csv_iter(fp):\n",
    "            total_rows += int(len(chunk))\n",
    "\n",
    "            add_sigma_obs_inplace(chunk)\n",
    "\n",
    "            # track unknown sources\n",
    "            src = chunk[\"Source\"].astype(\"string\").map(lambda x: _norm_source(x))\n",
    "            unk = src[chunk[\"sigma_obs\"].isna()].value_counts(dropna=True)\n",
    "            if len(unk) > 0:\n",
    "                for k, v in unk.items():\n",
    "                    key = str(k)\n",
    "                    unknown_sources[key] = unknown_sources.get(key, 0) + int(v)\n",
    "\n",
    "            # ensure column order\n",
    "            cols = list(chunk.columns)\n",
    "            # remove then re-insert to ensure correct position\n",
    "            cols = [c for c in cols if c != \"sigma_obs\"]\n",
    "            cols = insert_after_source(cols, \"sigma_obs\")\n",
    "            chunk = chunk[cols]\n",
    "\n",
    "            chunk.to_csv(tmp_path, index=False, mode=\"w\" if first else \"a\", header=first)\n",
    "            first = False\n",
    "\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "\n",
    "    atomic_replace_csv(fp, _write_to_temp)\n",
    "\n",
    "    print(f\"     [OK] Replaced: {fp.name} | rows={total_rows:,}\")\n",
    "    if unknown_sources:\n",
    "        top = sorted(unknown_sources.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        msg = \", \".join([f\"{k}:{v}\" for k, v in top])\n",
    "        print(f\"     [WARN] Unknown Source -> sigma_obs=NaN (top): {msg}\")\n",
    "\n",
    "\n",
    "def run():\n",
    "    depth_dirs = find_depth_folders(ROOT_DIR, TARGET_DEPTHS)\n",
    "    if not depth_dirs:\n",
    "        raise RuntimeError(f\"No target depth folders found under {ROOT_DIR} for {TARGET_DEPTHS}\")\n",
    "\n",
    "    for dep in TARGET_DEPTHS:\n",
    "        ddir = depth_dirs.get(dep)\n",
    "        if ddir is None:\n",
    "            print(f\"[WARN] Missing folder for depth {dep} (searched dbar/dabr). Skip.\")\n",
    "            continue\n",
    "\n",
    "        train_files = list_train_files(ddir)\n",
    "        if not train_files:\n",
    "            print(f\"[WARN] No *TRAIN.csv in {ddir}. Skip.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[DEPTH {dep}] folder={ddir} | TRAIN files={len(train_files)}\")\n",
    "        for fp in train_files:\n",
    "            process_one_file(fp)\n",
    "\n",
    "    print(\"\\n[DONE]\")\n",
    "\n",
    "\n",
    "# ---- Execute in Jupyter ----\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Map SIGMA_rep > 3 for Month==1 from:\n",
    "  /data/wang/Result_Data/alldoxy/1dbar/depth1_TRAIN.csv\n",
    "\n",
    "Default: grid-binned mean for values > 3 only.\n",
    "Optional: scatter (subsample) for quick look.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User config\n",
    "# =========================\n",
    "CSV_PATH = Path(\"/data/wang/Result_Data/alldoxy/1dbar/depth1_TRAIN.csv\")\n",
    "OUT_PNG  = CSV_PATH.with_name(\"depth1_TRAIN_Month01_SIGMA_rep_gt3_map.png\")\n",
    "\n",
    "COL_LAT  = \"Latitude\"\n",
    "COL_LON  = \"Longitude\"\n",
    "COL_MON  = \"Month\"\n",
    "COL_VAL  = \"sigma_rep\"\n",
    "\n",
    "THRESH = 3.0  # only plot values > THRESH\n",
    "\n",
    "# ---- Gridding (recommended) ----\n",
    "DX_DEG = 1.0\n",
    "DY_DEG = 1.0\n",
    "\n",
    "# ---- Chunked read (for big files) ----\n",
    "CHUNKSIZE = 2_000_000\n",
    "\n",
    "# ---- Plot control ----\n",
    "USE_SCATTER = False\n",
    "SCATTER_MAX_N = 300_000\n",
    "\n",
    "\n",
    "def wrap_lon(lon):\n",
    "    lon = np.asarray(lon, dtype=\"float64\")\n",
    "    return ((lon + 180.0) % 360.0) - 180.0\n",
    "\n",
    "\n",
    "def grid_mean_map_gt(csv_path: Path, thresh: float):\n",
    "    lon_edges = np.arange(-180.0, 180.0 + DX_DEG, DX_DEG)\n",
    "    lat_edges = np.arange(-90.0,   90.0 + DY_DEG, DY_DEG)\n",
    "\n",
    "    sum_grid = np.zeros((lat_edges.size - 1, lon_edges.size - 1), dtype=\"float64\")\n",
    "    cnt_grid = np.zeros_like(sum_grid, dtype=\"int64\")\n",
    "\n",
    "    usecols = [COL_MON, COL_LAT, COL_LON, COL_VAL]\n",
    "\n",
    "    for chunk in pd.read_csv(csv_path, usecols=usecols, chunksize=CHUNKSIZE, low_memory=False):\n",
    "        # Month==1\n",
    "        mon = pd.to_numeric(chunk[COL_MON], errors=\"coerce\")\n",
    "        msk = (mon == 1)\n",
    "        if not msk.any():\n",
    "            continue\n",
    "\n",
    "        sub = chunk.loc[msk, [COL_LAT, COL_LON, COL_VAL]].copy()\n",
    "        sub[COL_LAT] = pd.to_numeric(sub[COL_LAT], errors=\"coerce\")\n",
    "        sub[COL_LON] = pd.to_numeric(sub[COL_LON], errors=\"coerce\")\n",
    "        sub[COL_VAL] = pd.to_numeric(sub[COL_VAL], errors=\"coerce\")\n",
    "        sub = sub.dropna(subset=[COL_LAT, COL_LON, COL_VAL])\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        lat = sub[COL_LAT].to_numpy(dtype=\"float64\", copy=False)\n",
    "        lon = wrap_lon(sub[COL_LON].to_numpy(dtype=\"float64\", copy=False))\n",
    "        val = sub[COL_VAL].to_numpy(dtype=\"float64\", copy=False)\n",
    "\n",
    "        # keep only val > thresh\n",
    "        ok = (\n",
    "            (lat >= -90) & (lat <= 90) &\n",
    "            (lon >= -180) & (lon < 180) &\n",
    "            np.isfinite(val) & (val > thresh)\n",
    "        )\n",
    "        if not np.any(ok):\n",
    "            continue\n",
    "\n",
    "        lat = lat[ok]; lon = lon[ok]; val = val[ok]\n",
    "\n",
    "        sum2d, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges], weights=val)\n",
    "        cnt2d, _, _ = np.histogram2d(lat, lon, bins=[lat_edges, lon_edges])\n",
    "\n",
    "        sum_grid += sum2d\n",
    "        cnt_grid += cnt2d.astype(\"int64\")\n",
    "\n",
    "    mean_grid = np.full_like(sum_grid, np.nan, dtype=\"float64\")\n",
    "    valid = cnt_grid > 0\n",
    "    mean_grid[valid] = sum_grid[valid] / cnt_grid[valid]\n",
    "\n",
    "    return lon_edges, lat_edges, mean_grid, cnt_grid\n",
    "\n",
    "\n",
    "def scatter_map_gt(csv_path: Path, thresh: float):\n",
    "    usecols = [COL_MON, COL_LAT, COL_LON, COL_VAL]\n",
    "    df = pd.read_csv(csv_path, usecols=usecols, low_memory=False)\n",
    "\n",
    "    df[COL_MON] = pd.to_numeric(df[COL_MON], errors=\"coerce\")\n",
    "    df = df.loc[df[COL_MON] == 1, [COL_LAT, COL_LON, COL_VAL]].copy()\n",
    "\n",
    "    df[COL_LAT] = pd.to_numeric(df[COL_LAT], errors=\"coerce\")\n",
    "    df[COL_LON] = pd.to_numeric(df[COL_LON], errors=\"coerce\")\n",
    "    df[COL_VAL] = pd.to_numeric(df[COL_VAL], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[COL_LAT, COL_LON, COL_VAL])\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No valid rows for Month==1 after cleaning.\")\n",
    "\n",
    "    lon = wrap_lon(df[COL_LON].to_numpy(dtype=\"float64\", copy=False))\n",
    "    lat = df[COL_LAT].to_numpy(dtype=\"float64\", copy=False)\n",
    "    val = df[COL_VAL].to_numpy(dtype=\"float64\", copy=False)\n",
    "\n",
    "    ok = (\n",
    "        (lat >= -90) & (lat <= 90) &\n",
    "        (lon >= -180) & (lon < 180) &\n",
    "        np.isfinite(val) & (val > thresh)\n",
    "    )\n",
    "    lon, lat, val = lon[ok], lat[ok], val[ok]\n",
    "\n",
    "    if lon.size > SCATTER_MAX_N:\n",
    "        idx = np.random.default_rng(42).choice(lon.size, size=SCATTER_MAX_N, replace=False)\n",
    "        lon, lat, val = lon[idx], lat[idx], val[idx]\n",
    "\n",
    "    return lon, lat, val\n",
    "\n",
    "\n",
    "def main():\n",
    "    fig = plt.figure(figsize=(12, 5.5))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_global()\n",
    "\n",
    "    ax.add_feature(cfeature.LAND, zorder=1, facecolor=\"0.9\", edgecolor=\"0.5\", linewidth=0.3)\n",
    "    ax.add_feature(cfeature.COASTLINE, zorder=2, linewidth=0.4)\n",
    "\n",
    "    if USE_SCATTER:\n",
    "        lon, lat, val = scatter_map_gt(CSV_PATH, THRESH)\n",
    "        sc = ax.scatter(\n",
    "            lon, lat, c=val, s=3, transform=ccrs.PlateCarree(),\n",
    "            linewidths=0, alpha=0.6, zorder=3\n",
    "        )\n",
    "        cb = plt.colorbar(sc, ax=ax, orientation=\"vertical\", pad=0.02, shrink=0.92)\n",
    "        cb.set_label(f\"{COL_VAL} (> {THRESH:g})\")\n",
    "        ax.set_title(f\"{COL_VAL} (Month=1, >{THRESH:g}) | scatter (n={lon.size})\")\n",
    "\n",
    "    else:\n",
    "        lon_edges, lat_edges, mean_grid, cnt_grid = grid_mean_map_gt(CSV_PATH, THRESH)\n",
    "\n",
    "        pm = ax.pcolormesh(\n",
    "            lon_edges, lat_edges, mean_grid,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            shading=\"auto\", zorder=3\n",
    "        )\n",
    "        cb = plt.colorbar(pm, ax=ax, orientation=\"vertical\", pad=0.02, shrink=0.92)\n",
    "        cb.set_label(f\"{COL_VAL} (> {THRESH:g}) | binned mean\")\n",
    "        ax.set_title(f\"{COL_VAL} (Month=1, >{THRESH:g}) | gridded mean ({DX_DEG:.2f}°×{DY_DEG:.2f}°)\")\n",
    "\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.3, alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(OUT_PNG, dpi=300)\n",
    "    print(f\"[OK] Saved figure: {OUT_PNG}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# User configuration\n",
    "# -------------------------\n",
    "ROOT_DIR = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "NC_PATH  = Path(\"/data/wang/Merage_Biomes_0p5deg.nc\")\n",
    "\n",
    "# Depths to process\n",
    "depths = [\n",
    "        1,10,20,30,40,50,60,70,80,90,100,\n",
    "        110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "        270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "        430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "        590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "        750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "        1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "        1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "        1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "        1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "        2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "        3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "        5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "\n",
    "# Input CSV filename pattern inside each \"{dep}dbar\" directory\n",
    "PATTERN = \"*TRAIN.csv\"   # e.g., \"*TRAIN.csv\" or \"*NoAgg.csv\", etc.\n",
    "\n",
    "LAT_COL = \"Latitude\"\n",
    "LON_COL = \"Longitude\"\n",
    "MONTH_COL = \"Month\"  # required for SOM_Zone (1..12)\n",
    "\n",
    "# SOM labels\n",
    "SOM_LABEL_DIR = Path(\"/data/wang/Result_Data/Province_SOM/labels\")\n",
    "SOM_FILE_FMT  = \"SOM_province_labels_depth{dep}m.nc\"\n",
    "ALLOW_NEAREST_DEPTH_FILE = True\n",
    "SOM_INVALID_VALUE = -1\n",
    "\n",
    "# Output controls\n",
    "ATOMIC_OVERWRITE = True         # True: atomic overwrite original file; False: write to a new file\n",
    "OUT_SUFFIX = \"_withSOM\"         # output filename suffix\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper functions\n",
    "# -------------------------\n",
    "def normalize_lon(lon: np.ndarray, lon_nc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize input longitudes to match the NC convention (0..360 or -180..180).\"\"\"\n",
    "    lon = lon.astype(np.float64)\n",
    "    lon_nc = np.asarray(lon_nc, dtype=np.float64)\n",
    "    lon_nc_min = float(np.nanmin(lon_nc))\n",
    "    lon_nc_max = float(np.nanmax(lon_nc))\n",
    "\n",
    "    nc_is_0_360 = (lon_nc_min >= -1e-6) and (lon_nc_max > 180.0)\n",
    "    nc_is_m180_180 = (lon_nc_min < 0.0) and (lon_nc_max <= 180.0 + 1e-6)\n",
    "\n",
    "    if nc_is_0_360:\n",
    "        lon2 = np.mod(lon, 360.0)\n",
    "        lon2[lon2 < 0] += 360.0\n",
    "        return lon2\n",
    "    if nc_is_m180_180:\n",
    "        lon2 = np.mod(lon + 180.0, 360.0) - 180.0\n",
    "        return lon2\n",
    "    return lon\n",
    "\n",
    "\n",
    "def build_grid_mapper(lon_nc: np.ndarray, lat_nc: np.ndarray):\n",
    "    \"\"\"Build a mapper from (lon, lat) to the nearest regular-grid indices.\"\"\"\n",
    "    lon_nc = np.asarray(lon_nc, dtype=np.float64)\n",
    "    lat_nc = np.asarray(lat_nc, dtype=np.float64)\n",
    "\n",
    "    if not (np.all(np.diff(lon_nc) > 0) and np.all(np.diff(lat_nc) > 0)):\n",
    "        raise ValueError(\"NC lon/lat must be strictly increasing. Please check the file.\")\n",
    "\n",
    "    lon_step = float(np.nanmedian(np.diff(lon_nc)))\n",
    "    lat_step = float(np.nanmedian(np.diff(lat_nc)))\n",
    "    lon0 = float(lon_nc[0])\n",
    "    lat0 = float(lat_nc[0])\n",
    "    nlon = lon_nc.size\n",
    "    nlat = lat_nc.size\n",
    "\n",
    "    lon_tol = 0.5 * lon_step + 1e-6\n",
    "    lat_tol = 0.5 * lat_step + 1e-6\n",
    "\n",
    "    def to_index(lon_in: np.ndarray, lat_in: np.ndarray):\n",
    "        lon_in = np.asarray(lon_in, dtype=np.float64)\n",
    "        lat_in = np.asarray(lat_in, dtype=np.float64)\n",
    "\n",
    "        lon_pos = (lon_in - lon0) / lon_step\n",
    "        lat_pos = (lat_in - lat0) / lat_step\n",
    "\n",
    "        lon_idx = np.rint(lon_pos).astype(np.int64)\n",
    "        lat_idx = np.rint(lat_pos).astype(np.int64)\n",
    "\n",
    "        in_bounds = (lon_idx >= 0) & (lon_idx < nlon) & (lat_idx >= 0) & (lat_idx < nlat)\n",
    "\n",
    "        lon_center = lon0 + lon_idx.astype(np.float64) * lon_step\n",
    "        lat_center = lat0 + lat_idx.astype(np.float64) * lat_step\n",
    "\n",
    "        close_enough = (np.abs(lon_in - lon_center) <= lon_tol) & (np.abs(lat_in - lat_center) <= lat_tol)\n",
    "        valid = in_bounds & close_enough\n",
    "        return lon_idx, lat_idx, valid\n",
    "\n",
    "    meta = dict(\n",
    "        lon0=lon0, lat0=lat0,\n",
    "        lon_step=lon_step, lat_step=lat_step,\n",
    "        nlon=nlon, nlat=nlat,\n",
    "        lon_tol=lon_tol, lat_tol=lat_tol\n",
    "    )\n",
    "    return to_index, meta\n",
    "\n",
    "\n",
    "def extract_by_indices_2d(var_lon_lat: np.ndarray,\n",
    "                          lon_idx: np.ndarray,\n",
    "                          lat_idx: np.ndarray,\n",
    "                          valid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"2D variable with shape (nlon, nlat). Return extracted values; otherwise NaN.\"\"\"\n",
    "    nlon, nlat = var_lon_lat.shape\n",
    "    out = np.full(lon_idx.shape, np.nan, dtype=np.float64)\n",
    "    if valid.any():\n",
    "        li = lon_idx[valid]\n",
    "        la = lat_idx[valid]\n",
    "        flat = li * nlat + la\n",
    "        out[valid] = var_lon_lat.reshape(-1)[flat]\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_by_indices_3d_month_lat_lon(var_mll: np.ndarray,\n",
    "                                       mon_idx: np.ndarray,\n",
    "                                       lat_idx: np.ndarray,\n",
    "                                       lon_idx: np.ndarray,\n",
    "                                       valid_xy: np.ndarray,\n",
    "                                       valid_mon: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"3D variable with shape (12, nlat, nlon). Return extracted values; otherwise NaN.\"\"\"\n",
    "    out = np.full(mon_idx.shape, np.nan, dtype=np.float64)\n",
    "    good = valid_xy & valid_mon\n",
    "    if not good.any():\n",
    "        return out\n",
    "\n",
    "    mi = mon_idx[good].astype(np.int64, copy=False)\n",
    "    la = lat_idx[good].astype(np.int64, copy=False)\n",
    "    lo = lon_idx[good].astype(np.int64, copy=False)\n",
    "\n",
    "    nlat = var_mll.shape[1]\n",
    "    nlon = var_mll.shape[2]\n",
    "    flat = (mi * (nlat * nlon)) + (la * nlon) + lo\n",
    "\n",
    "    out[good] = var_mll.reshape(-1)[flat]\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_depth_from_filename(p: Path) -> Optional[int]:\n",
    "    \"\"\"Parse depth (meters) from filename like '..._depth{dep}m.nc'.\"\"\"\n",
    "    m = re.search(r\"depth(\\d+)m\\.nc$\", p.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "\n",
    "def find_som_file_for_depth(dep: int) -> Optional[Path]:\n",
    "    \"\"\"Find the SOM label file for a given depth (exact match or nearest, if enabled).\"\"\"\n",
    "    exact = SOM_LABEL_DIR / SOM_FILE_FMT.format(dep=dep)\n",
    "    if exact.exists():\n",
    "        return exact\n",
    "    if not ALLOW_NEAREST_DEPTH_FILE:\n",
    "        return None\n",
    "\n",
    "    cand = []\n",
    "    for f in SOM_LABEL_DIR.glob(\"SOM_province_labels_depth*m.nc\"):\n",
    "        d = parse_depth_from_filename(f)\n",
    "        if d is not None:\n",
    "            cand.append((abs(d - dep), d, f))\n",
    "    if not cand:\n",
    "        return None\n",
    "    cand.sort(key=lambda x: (x[0], x[1]))\n",
    "    return cand[0][2]\n",
    "\n",
    "\n",
    "def load_meanbiomes_nc(nc_path: Path):\n",
    "    \"\"\"Load MeanBiomes/ExcludeMask and lon/lat from the provided NetCDF.\"\"\"\n",
    "    if not nc_path.exists():\n",
    "        raise FileNotFoundError(f\"NC file not found: {nc_path}\")\n",
    "\n",
    "    ds = xr.open_dataset(nc_path)\n",
    "    for v in [\"MeanBiomes\", \"ExcludeMask\", \"lon\", \"lat\"]:\n",
    "        if v not in ds.variables:\n",
    "            ds.close()\n",
    "            raise KeyError(f\"Variable '{v}' not found in {nc_path}\")\n",
    "\n",
    "    mean_biomes = ds[\"MeanBiomes\"].values  # (nlon, nlat)\n",
    "    exclude_mask = ds[\"ExcludeMask\"].values\n",
    "    lon_nc = ds[\"lon\"].values\n",
    "    lat_nc = ds[\"lat\"].values\n",
    "    ds.close()\n",
    "\n",
    "    if mean_biomes.ndim != 2 or exclude_mask.ndim != 2:\n",
    "        raise ValueError(\"MeanBiomes / ExcludeMask must be 2D arrays.\")\n",
    "    if mean_biomes.shape != exclude_mask.shape:\n",
    "        raise ValueError(\"MeanBiomes and ExcludeMask shapes do not match.\")\n",
    "    if mean_biomes.shape != (lon_nc.size, lat_nc.size):\n",
    "        raise ValueError(\"2D variable shape must match (len(lon), len(lat)).\")\n",
    "\n",
    "    return mean_biomes, exclude_mask, lon_nc, lat_nc\n",
    "\n",
    "\n",
    "def load_som_nc_for_depth(dep: int):\n",
    "    \"\"\"Load SOM province labels and coordinate variables for the given depth.\"\"\"\n",
    "    if not SOM_LABEL_DIR.exists():\n",
    "        raise FileNotFoundError(f\"SOM label directory not found: {SOM_LABEL_DIR}\")\n",
    "\n",
    "    som_nc = find_som_file_for_depth(dep)\n",
    "    if som_nc is None:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    ds = xr.open_dataset(som_nc)\n",
    "    for v in [\"province_id\", \"lon\", \"lat\", \"month\"]:\n",
    "        if v not in ds.variables:\n",
    "            ds.close()\n",
    "            raise KeyError(f\"Variable '{v}' not found in {som_nc}\")\n",
    "\n",
    "    province_id = ds[\"province_id\"].values  # expected shape (12, nlat, nlon)\n",
    "    lon_som = ds[\"lon\"].values\n",
    "    lat_som = ds[\"lat\"].values\n",
    "    ts_depth = float(ds.attrs.get(\"ts_depth_m\", np.nan))\n",
    "    lon_conv = ds.attrs.get(\"lon_convention\", \"unknown\")\n",
    "    ds.close()\n",
    "\n",
    "    if province_id.ndim != 3 or province_id.shape[0] != 12:\n",
    "        raise ValueError(f\"province_id must be 3D with the first dimension = 12 months; got {province_id.shape}\")\n",
    "    if province_id.shape[1] != lat_som.size or province_id.shape[2] != lon_som.size:\n",
    "        raise ValueError(\"province_id shape does not match (lat, lon).\")\n",
    "\n",
    "    return som_nc, province_id, lon_som, lat_som, ts_depth, lon_conv\n",
    "\n",
    "\n",
    "def process_one_csv_three_fields(csv_path: Path,\n",
    "                                out_path: Path,\n",
    "                                mean_biomes: np.ndarray,\n",
    "                                exclude_mask: np.ndarray,\n",
    "                                lon_mb: np.ndarray,\n",
    "                                lat_mb: np.ndarray,\n",
    "                                province_id: Optional[np.ndarray],\n",
    "                                lon_som: Optional[np.ndarray],\n",
    "                                lat_som: Optional[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Process a whole CSV file:\n",
    "    - Remove existing Zone0 / ExcludeMask / SOM_Zone columns (if present)\n",
    "    - Recompute and write new values based on MeanBiomes/ExcludeMask and SOM labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Build mappers\n",
    "    to_idx_mb, meta_mb = build_grid_mapper(lon_mb, lat_mb)\n",
    "\n",
    "    if province_id is not None:\n",
    "        to_idx_som, meta_som = build_grid_mapper(lon_som, lat_som)\n",
    "    else:\n",
    "        to_idx_som, meta_som = None, None\n",
    "\n",
    "    print(f\"\\n[FILE] {csv_path}\")\n",
    "    print(f\"  MeanBiomes grid: lon0={meta_mb['lon0']}, lat0={meta_mb['lat0']}, \"\n",
    "          f\"dlon={meta_mb['lon_step']}, dlat={meta_mb['lat_step']}\")\n",
    "\n",
    "    if province_id is None:\n",
    "        print(\"  SOM grid: [skipped] no SOM file for this depth (SOM_Zone will be NA)\")\n",
    "    else:\n",
    "        print(f\"  SOM grid: lon0={meta_som['lon0']}, lat0={meta_som['lat0']}, \"\n",
    "              f\"dlon={meta_som['lon_step']}, dlat={meta_som['lat_step']}\")\n",
    "\n",
    "    # Read the entire file\n",
    "    chunk = pd.read_csv(csv_path)\n",
    "\n",
    "    # Drop existing columns if already present\n",
    "    for col in [\"Zone0\", \"ExcludeMask\", \"SOM_Zone\"]:\n",
    "        if col in chunk.columns:\n",
    "            del chunk[col]\n",
    "\n",
    "    # Parse lat/lon\n",
    "    lat = pd.to_numeric(chunk[LAT_COL], errors=\"coerce\").to_numpy(np.float64)\n",
    "    lon_raw = pd.to_numeric(chunk[LON_COL], errors=\"coerce\").to_numpy(np.float64)\n",
    "\n",
    "    # ---------- MeanBiomes mapping ----------\n",
    "    lon_mb_in = normalize_lon(lon_raw, lon_mb)\n",
    "    lon_idx_mb, lat_idx_mb, valid_mb = to_idx_mb(lon_mb_in, lat)\n",
    "\n",
    "    zone_vals = extract_by_indices_2d(mean_biomes, lon_idx_mb, lat_idx_mb, valid_mb)\n",
    "    excl_vals = extract_by_indices_2d(exclude_mask, lon_idx_mb, lat_idx_mb, valid_mb)\n",
    "\n",
    "    # ---------- Zone0 and ExcludeMask assignment ----------\n",
    "    zone_int = np.rint(zone_vals).astype(np.int64, casting=\"unsafe\", copy=False)\n",
    "    excl_int = np.rint(excl_vals).astype(np.int64, casting=\"unsafe\", copy=False)\n",
    "\n",
    "    zone_valid = np.isfinite(zone_vals)\n",
    "    excl_valid = np.isfinite(excl_vals)\n",
    "\n",
    "    chunk[\"Zone0\"] = pd.Series(pd.array(np.where(zone_valid, zone_int, pd.NA), dtype=\"Int64\"))\n",
    "    chunk[\"ExcludeMask\"] = pd.Series(pd.array(np.where(excl_valid, excl_int, pd.NA), dtype=\"Int64\"))\n",
    "\n",
    "    # ---------- SOM mapping (optional) ----------\n",
    "    if province_id is None:\n",
    "        chunk[\"SOM_Zone\"] = pd.Series(pd.array(np.full(len(chunk), pd.NA), dtype=\"Int64\"))\n",
    "    else:\n",
    "        if MONTH_COL not in chunk.columns:\n",
    "            raise KeyError(f\"Missing '{MONTH_COL}' in {csv_path.name} (SOM_Zone depends on month).\")\n",
    "\n",
    "        lon_som_in = normalize_lon(lon_raw, lon_som)\n",
    "        lon_idx_som, lat_idx_som, valid_som_xy = to_idx_som(lon_som_in, lat)\n",
    "\n",
    "        mon_raw = pd.to_numeric(chunk[MONTH_COL], errors=\"coerce\").to_numpy(np.float64)\n",
    "        mon_int = np.rint(mon_raw).astype(np.int64, casting=\"unsafe\", copy=False)\n",
    "        valid_mon = np.isfinite(mon_raw) & (mon_int >= 1) & (mon_int <= 12)\n",
    "        mon_idx = (mon_int - 1).astype(np.int64, copy=False)\n",
    "\n",
    "        som_vals = extract_by_indices_3d_month_lat_lon(\n",
    "            province_id, mon_idx, lat_idx_som, lon_idx_som, valid_som_xy, valid_mon\n",
    "        )\n",
    "\n",
    "        som_int = np.rint(som_vals).astype(np.int64, casting=\"unsafe\", copy=False)\n",
    "        # Rule: must be >= 0 and must not be the fill value (-1)\n",
    "        som_valid = np.isfinite(som_vals) & (som_int >= 0) & (som_int != SOM_INVALID_VALUE)\n",
    "\n",
    "        chunk[\"SOM_Zone\"] = pd.Series(pd.array(np.where(som_valid, som_int, pd.NA), dtype=\"Int64\"))\n",
    "\n",
    "    # Write processed data\n",
    "    chunk.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"[INFO] Completed: {csv_path.name} -> {out_path.name}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if not ROOT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"ROOT_DIR not found: {ROOT_DIR}\")\n",
    "if not SOM_LABEL_DIR.exists():\n",
    "    raise FileNotFoundError(f\"SOM label directory not found: {SOM_LABEL_DIR}\")\n",
    "\n",
    "print(\"[OK] Loading MeanBiomes/ExcludeMask NC...\")\n",
    "mean_biomes, exclude_mask, lon_mb, lat_mb = load_meanbiomes_nc(NC_PATH)\n",
    "print(\"[OK] MeanBiomes NC loaded.\")\n",
    "print(\"  MeanBiomes:\", mean_biomes.shape, \"ExcludeMask:\", exclude_mask.shape)\n",
    "\n",
    "for dep in depths:\n",
    "    dep_dir = ROOT_DIR / f\"{dep}dbar\"\n",
    "    if not dep_dir.exists():\n",
    "        print(f\"\\n[SKIP] Depth directory not found: {dep_dir}\")\n",
    "        continue\n",
    "\n",
    "    files = sorted(dep_dir.glob(PATTERN))\n",
    "    if not files:\n",
    "        print(f\"\\n[SKIP] No '{PATTERN}' files found in {dep_dir}\")\n",
    "        continue\n",
    "\n",
    "    som_nc, province_id, lon_som, lat_som, ts_depth, lon_conv = load_som_nc_for_depth(dep)\n",
    "    if som_nc is None:\n",
    "        print(f\"\\n[DEPTH] {dep} dbar | files={len(files)} | SOM: [missing] -> SOM_Zone will be NA\")\n",
    "    else:\n",
    "        print(f\"\\n[DEPTH] {dep} dbar | files={len(files)} | SOM={som_nc.name} | ts_depth_m={ts_depth} | lon_convention={lon_conv}\")\n",
    "\n",
    "    for csv_path in files:\n",
    "        out_path = csv_path.with_name(csv_path.stem + OUT_SUFFIX + csv_path.suffix)\n",
    "\n",
    "        process_one_csv_three_fields(\n",
    "            csv_path=csv_path,\n",
    "            out_path=out_path,\n",
    "            mean_biomes=mean_biomes,\n",
    "            exclude_mask=exclude_mask,\n",
    "            lon_mb=lon_mb,\n",
    "            lat_mb=lat_mb,\n",
    "            province_id=province_id,\n",
    "            lon_som=lon_som,\n",
    "            lat_som=lat_som\n",
    "        )\n",
    "\n",
    "print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_rep: if value equals 0, set it to empty (blank)\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch edit for *TRAIN.csv under:\n",
    "  /data/wang/Result_Data/alldoxy/{depth}dbar or {depth}dabr/\n",
    "\n",
    "Requirement:\n",
    "- For target depths in TARGET_DEPTHS, in each *TRAIN.csv:\n",
    "    if sigma_rep == 0  -> set to empty (blank)\n",
    "- All other fields remain unchanged (read/write as strings to minimize reformatting).\n",
    "- Edit \"in place\" via safe-write (temp file) then atomic replace.\n",
    "\n",
    "Python 3.9 + Jupyter compatible.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# User configuration\n",
    "# =========================\n",
    "ROOT_DIR = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "TARGET_DEPTHS = [\n",
    "    10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "FILE_GLOB = \"*TRAIN.csv\"\n",
    "\n",
    "# Large files: adjust based on memory / I/O\n",
    "CHUNK_SIZE = 600_000\n",
    "\n",
    "# Target column to edit\n",
    "COL = \"sigma_rep\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "ZERO_RE = re.compile(r\"^[\\+\\-]?0+(\\.0+)?([eE][\\+\\-]?0+)?$\")\n",
    "\n",
    "def is_zero_string(x: str) -> bool:\n",
    "    \"\"\"\n",
    "    Decide whether a string represents numeric zero (common CSV encodings).\n",
    "    Examples treated as zero:\n",
    "      '0', '0.0', '0.00', '+0', '-0', '0e0', '0E+00', '000.000'\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return False\n",
    "    s = str(x).strip()\n",
    "    if s == \"\":\n",
    "        return False\n",
    "    return bool(ZERO_RE.match(s))\n",
    "\n",
    "\n",
    "def process_one_csv_inplace(csv_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Stream-read as strings to preserve formatting as much as possible,\n",
    "    replace sigma_rep zeros with blank, then overwrite the file safely.\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[SKIP] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    tmp_path = csv_path.with_suffix(csv_path.suffix + \".tmp\")\n",
    "\n",
    "    # Read as strings and do NOT auto-convert empty strings to NaN\n",
    "    reader = pd.read_csv(\n",
    "        csv_path,\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        dtype=str,\n",
    "        keep_default_na=False,\n",
    "        na_values=[],\n",
    "        low_memory=False,\n",
    "    )\n",
    "\n",
    "    wrote_header = False\n",
    "    total_rows = 0\n",
    "    changed = 0\n",
    "    cols_ref = None\n",
    "\n",
    "    try:\n",
    "        for chunk in reader:\n",
    "            if cols_ref is None:\n",
    "                cols_ref = list(chunk.columns)\n",
    "\n",
    "            if COL in chunk.columns:\n",
    "                s = chunk[COL].astype(str)\n",
    "                m = s.map(is_zero_string)\n",
    "                if m.any():\n",
    "                    changed += int(m.sum())\n",
    "                    chunk.loc[m, COL] = \"\"  # blank\n",
    "\n",
    "            total_rows += len(chunk)\n",
    "\n",
    "            chunk.to_csv(\n",
    "                tmp_path,\n",
    "                mode=\"w\" if not wrote_header else \"a\",\n",
    "                index=False,\n",
    "                header=(not wrote_header),\n",
    "                columns=cols_ref,   # preserve original column order\n",
    "                encoding=\"utf-8\",\n",
    "            )\n",
    "            wrote_header = True\n",
    "\n",
    "        # Atomic replace\n",
    "        os.replace(tmp_path, csv_path)\n",
    "\n",
    "        print(f\"[OK] {csv_path} | rows={total_rows:,} | {COL} zero->blank: {changed:,}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Cleanup temp file on failure\n",
    "        try:\n",
    "            if tmp_path.exists():\n",
    "                tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"[ERROR] {csv_path} | {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def find_depth_dirs(root: Path, depth: int):\n",
    "    \"\"\"\n",
    "    Accept both 'dbar' and occasional 'dabr' naming.\n",
    "    Return a list of existing directories.\n",
    "    \"\"\"\n",
    "    candidates = [root / f\"{depth}dbar\", root / f\"{depth}dabr\"]\n",
    "    return [p for p in candidates if p.is_dir()]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def main():\n",
    "    if not ROOT_DIR.is_dir():\n",
    "        raise FileNotFoundError(f\"ROOT_DIR not found: {ROOT_DIR}\")\n",
    "\n",
    "    all_files = []\n",
    "    for dep in TARGET_DEPTHS:\n",
    "        dep_dirs = find_depth_dirs(ROOT_DIR, dep)\n",
    "        if not dep_dirs:\n",
    "            print(f\"[WARN] Depth folder not found for {dep}: tried {dep}dbar / {dep}dabr\")\n",
    "            continue\n",
    "\n",
    "        for d in dep_dirs:\n",
    "            files = sorted(d.glob(FILE_GLOB))\n",
    "            if not files:\n",
    "                print(f\"[WARN] No files matched '{FILE_GLOB}' in {d}\")\n",
    "            all_files.extend(files)\n",
    "\n",
    "    if not all_files:\n",
    "        print(\"[DONE] No target files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Found {len(all_files)} file(s) to process.\")\n",
    "    for fp in all_files:\n",
    "        process_one_csv_inplace(fp)\n",
    "\n",
    "    print(\"[DONE] All processing finished.\")\n",
    "\n",
    "\n",
    "# In Jupyter, just run this cell; as a script, it also works.\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
