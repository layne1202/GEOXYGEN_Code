{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Final training script (Python 3.9 + Jupyter compatible)\n",
    "\n",
    "Fixes:\n",
    "1) Density weighting bin-key now uses int64 encoding (no numpy string concat).\n",
    "2) RFECV.fit(sample_weight=...) is NOT supported in some sklearn versions.\n",
    "   -> fallback to RFECV without sample_weight, while keeping weights in:\n",
    "      - permutation importance (weighted)\n",
    "      - Optuna CV objective (weighted)\n",
    "      - final training (weighted)\n",
    "\n",
    "NEW (this revision):\n",
    "3) For each depth, after training all zone models, build \"overall test set\" predictions by\n",
    "   loading each zone model + metadata and predicting for that zone's test rows, then output:\n",
    "      - test_with_pred.csv   (ALL test rows, with Pred filled where zone model exists)\n",
    "      - overall_test_metrics.txt  (overall test RMSE/MAE/R2 + coverage)\n",
    "      - (optional) test_panel.png if ENABLE_PLOTTING\n",
    "\n",
    "User request (this turn):\n",
    "- Explicitly \"stitch into an overall/global test set and output overall skill\"\n",
    "- Keep everything else unchanged.\n",
    "\"\"\"\n",
    "\n",
    "import os, gc, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================ Config ============================\n",
    "\n",
    "ENABLE_PLOTTING = True\n",
    "COMPUTE_SHAP = False\n",
    "ENABLE_FEATURE_SELECTION = True\n",
    "\n",
    "# ---- Inverse-density weighting (ON) ----\n",
    "ENABLE_DENSITY_WEIGHTING = True\n",
    "WEIGHT_GRID_RESOLUTION_DEG = 5.0     # 5°\n",
    "WEIGHT_TIME_RESOLUTION_YR  = 10.0    # 10-year bins\n",
    "\n",
    "# Optional: extra penalty for transition period (kept but default OFF)\n",
    "ENABLE_TRANSITION_PERIOD_PENALTY = False\n",
    "PENALTY_YEAR_START = 1985\n",
    "PENALTY_YEAR_END   = 2000\n",
    "PENALTY_FACTOR     = 0.5\n",
    "\n",
    "# ---- Permutation importance (CV) ----\n",
    "PERM_N_SPLITS   = 5\n",
    "PERM_N_REPEATS  = 3\n",
    "PERM_TOPK       = None\n",
    "PERM_MAX_VAL_SAMPLES = 12000\n",
    "\n",
    "# ---- RFECV ----\n",
    "RFECV_STEP      = 1\n",
    "RFECV_SCORING   = \"neg_root_mean_squared_error\"\n",
    "USE_ADAPTIVE_RFECV_MIN = True\n",
    "RFECV_MIN_RATIO = 0.40\n",
    "RFECV_MIN_ABS   = 6\n",
    "RFECV_MIN_FEATS = 8\n",
    "RFECV_MIN_FEATS = 8\n",
    "ALWAYS_KEEP = []\n",
    "# ---- Optuna ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "N_TRIALS_OPTUNA = int(os.environ.get(\"N_TRIALS_OPTUNA\", \"32\"))\n",
    "RANDOM_SEED = 42\n",
    "N_ZONE_WORKERS = int(os.environ.get(\"ZONE_WORKERS\", \"4\"))\n",
    "\n",
    "MAX_CORES = 24\n",
    "total_cores = min(os.cpu_count() or 16, MAX_CORES)\n",
    "_default_cb_threads = max(4, total_cores // max(1, N_ZONE_WORKERS))\n",
    "N_THREADS = int(os.environ.get(\"CB_N_THREADS\", str(_default_cb_threads)))\n",
    "\n",
    "# Avoid BLAS/OMP oversubscription\n",
    "os.environ[\"OMP_NUM_THREADS\"]        = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]        = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"]   = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]    = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from math import sqrt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================ Run depth list ============================\n",
    "\n",
    "depthlist = [\"1900\",\"1800\"]\n",
    "\n",
    "SEASONS = [\"Spring\", \"Summer\", \"Autumn\", \"Winter\", \"NewYear\"]\n",
    "\n",
    "# ============================ Time setup ============================\n",
    "\n",
    "yearstart = int(os.environ.get(\"YEARSTART\", \"1960\"))\n",
    "\n",
    "_TEST_YEARS_MAP = {\n",
    "    1960: [1961,1970,1984,1993,2003,2012,2020],\n",
    "}\n",
    "TEST_YEARS = _TEST_YEARS_MAP.get(yearstart, _TEST_YEARS_MAP[yearstart])\n",
    "\n",
    "# ============================ Feature config ============================\n",
    "\n",
    "TempName  = \"Temp\"\n",
    "SalName   = \"Sal\"\n",
    "Satname   = \"O2_sat\"\n",
    "timename  = \"Year\"\n",
    "Zonename  = \"Zone0\"\n",
    "\n",
    "CORE_FEATURES = [\n",
    "    timename,\n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"Latitude\", #\"lon_cos20\", \"lon_sin110\",\n",
    "    TempName, SalName, \"SOM_Zone\",\n",
    "]\n",
    "\n",
    "\n",
    "ENFORCE_VALID_FEATURES = True\n",
    "\n",
    "# ============================ Paths ============================\n",
    "\n",
    "ROOT_DIR = \"/data/wang/Result_Data/alldoxy\"\n",
    "yearstart_tag = str(yearstart)\n",
    "OUTPUT_DIR = os.path.join(\"/data/wang/Result_Data\", f\"models_ML\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================ Sentinel / Range checks ============================\n",
    "\n",
    "SENTINEL_VALUES = [\n",
    "    -9999, -9999.0, -999, -32767, 32767,\n",
    "    1e20, -1e20, 9.96921e36, -9.96921e36\n",
    "]\n",
    "\n",
    "FEATURE_RANGES = {\n",
    "    \"Latitude\": (-90, 90),\n",
    "    \"Longitude\": (-180, 180),\n",
    "    TempName: (-2, 45),\n",
    "    SalName: (0, 45),\n",
    "    \"Chla\": (0, None),\n",
    "    \"pCO2\": (0, None),\n",
    "}\n",
    "\n",
    "DESIRED_SHAP_FEATURES = [TempName, SalName, \"U\", \"V\", \"SSH\", \"pCO2\", \"DIC\", \"Chla\"]\n",
    "\n",
    "TARGET = \"Oxygen\"\n",
    "ZONE   = Zonename\n",
    "TIME   = \"Year\"\n",
    "MIN_SAMPLES = 300\n",
    "\n",
    "# ============================ Plot style ============================\n",
    "\n",
    "if ENABLE_PLOTTING:\n",
    "    mpl.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",\n",
    "        \"font.size\": 10.5,\n",
    "        \"axes.linewidth\": 1.1,\n",
    "        \"xtick.direction\": \"in\",\n",
    "        \"ytick.direction\": \"in\",\n",
    "        \"xtick.major.size\": 4,\n",
    "        \"ytick.major.size\": 4,\n",
    "        \"savefig.dpi\": 340,\n",
    "        \"figure.figsize\": (6.4, 4.8)\n",
    "    })\n",
    "\n",
    "# ============================ Helpers ============================\n",
    "\n",
    "AUX_FEATURES = []\n",
    "MERGE_ZONES_1_TO_12 = False\n",
    "MERGE_MAP = {}\n",
    "\n",
    "def adjust_features_and_merge(depth):\n",
    "    global AUX_FEATURES, MERGE_ZONES_1_TO_12, MERGE_MAP\n",
    "    depth_int = int(depth)\n",
    "    AUX_FEATURES = [Satname, \"MLD\",\"U\",\"V\",\"SSH\",\"EKE\",\"PAR\",\"CO2_flux\",\"pH\",\"pCO2\",\"DIC\",\"Alkalinity\",\"Chla\"]\n",
    "    if 0 < depth_int <= 3000:\n",
    "        MERGE_ZONES_1_TO_12 = False\n",
    "    else:\n",
    "        MERGE_ZONES_1_TO_12 = True\n",
    "\n",
    "    MERGE_MAP = {}\n",
    "    if MERGE_ZONES_1_TO_12:\n",
    "        for o in [1, 2, 3, 4, 5]:\n",
    "            MERGE_MAP[o] = 1\n",
    "\n",
    "    print(f\"[Depth {depth_int}] AUX_FEATURES={AUX_FEATURES}\")\n",
    "    print(f\"[Depth {depth_int}] MERGE_ZONES_1_TO_12={MERGE_ZONES_1_TO_12}\")\n",
    "\n",
    "def compute_chla_offset(x):\n",
    "    x = np.asarray(x, float)\n",
    "    pos = x[np.isfinite(x) & (x > 0)]\n",
    "    if pos.size == 0:\n",
    "        return 1e-6\n",
    "    p5 = np.nanpercentile(pos, 5)\n",
    "    return max(1e-6, 0.01 * p5)\n",
    "\n",
    "def apply_chla_log_inplace(df, offset):\n",
    "    if \"Chla\" in df.columns:\n",
    "        z = np.asarray(df[\"Chla\"], float)\n",
    "        z = np.where(np.isfinite(z), z, np.nan)\n",
    "        df[\"Chla\"] = np.log10(np.maximum(z, 0) + float(offset))\n",
    "\n",
    "def transform_features(df_part, feature_cols, chla_offset):\n",
    "    X = df_part[feature_cols].copy()\n",
    "    if \"Chla\" in X.columns:\n",
    "        apply_chla_log_inplace(X, chla_offset if chla_offset is not None else 1e-6)\n",
    "    return X\n",
    "\n",
    "def calibration_stats(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    try:\n",
    "        lr = LinearRegression().fit(y_true.reshape(-1, 1), y_pred)\n",
    "        return dict(\n",
    "            slope=float(lr.coef_[0]),\n",
    "            intercept=float(lr.intercept_),\n",
    "            bias=float(np.nanmean(y_pred - y_true)),\n",
    "            r2=float(r2_score(y_true, y_pred))\n",
    "        )\n",
    "    except Exception:\n",
    "        return {\"slope\": np.nan, \"intercept\": np.nan, \"bias\": np.nan, \"r2\": np.nan}\n",
    "\n",
    "def enforce_valid_feature_rows(df, features, sentinel_values=None, feature_ranges=None):\n",
    "    if not features:\n",
    "        return df\n",
    "    cols = [c for c in features if c in df.columns]\n",
    "    if not cols:\n",
    "        return df\n",
    "\n",
    "    sentinel_values = list(sentinel_values or [])\n",
    "    feature_ranges  = dict(feature_ranges or {})\n",
    "\n",
    "    df2 = df.copy()\n",
    "    for c in cols:\n",
    "        df2[c] = pd.to_numeric(df2[c], errors=\"coerce\")\n",
    "\n",
    "    arr = df2[cols].to_numpy(dtype=float, copy=False)\n",
    "    keep_mask = np.isfinite(arr).all(axis=1)\n",
    "\n",
    "    if sentinel_values:\n",
    "        sent_bad = np.zeros(len(df2), dtype=bool)\n",
    "        for v in sentinel_values:\n",
    "            sent_bad |= (arr == float(v)).any(axis=1)\n",
    "        keep_mask &= ~sent_bad\n",
    "\n",
    "    if feature_ranges:\n",
    "        range_mask = np.ones(len(df2), dtype=bool)\n",
    "        for c in cols:\n",
    "            rng = feature_ranges.get(c, None)\n",
    "            if rng is None:\n",
    "                continue\n",
    "            lo, hi = rng if isinstance(rng, (tuple, list)) else (None, None)\n",
    "            col = df2[c].to_numpy(dtype=float, copy=False)\n",
    "            m = np.isfinite(col)\n",
    "            if lo is not None:\n",
    "                m &= (col >= float(lo))\n",
    "            if hi is not None:\n",
    "                m &= (col <= float(hi))\n",
    "            range_mask &= m\n",
    "        keep_mask &= range_mask\n",
    "\n",
    "    kept = int(keep_mask.sum())\n",
    "    dropped = int(len(df2) - kept)\n",
    "    if dropped > 0:\n",
    "        print(f\"[ValidFilter] kept {kept}/{len(df2)}; dropped {dropped} rows due to invalid feature(s).\")\n",
    "    return df2.loc[keep_mask].copy()\n",
    "\n",
    "def plot_test_panel(y_true, y_pred, out_path,\n",
    "                    xlab=r\"Predicted DO ($\\mu mol\\ kg^{-1}$)\",\n",
    "                    ylab=r\"Observed DO ($\\mu mol\\ kg^{-1}$)\"):\n",
    "    \"\"\"\n",
    "    Compact parity hexbin + residual histogram panel.\n",
    "    (Matches your previous idea; safe if scipy absent.)\n",
    "    \"\"\"\n",
    "    if not ENABLE_PLOTTING:\n",
    "        return\n",
    "\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    if y_true.size == 0:\n",
    "        return\n",
    "\n",
    "    resid = y_true - y_pred\n",
    "\n",
    "    try:\n",
    "        import scipy.stats as st\n",
    "        skew = float(st.skew(resid, nan_policy=\"omit\"))\n",
    "        kurt = float(st.kurtosis(resid, nan_policy=\"omit\", fisher=False))\n",
    "    except Exception:\n",
    "        skew, kurt = np.nan, np.nan\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    r2   = float(r2_score(y_true, y_pred))\n",
    "\n",
    "    fig = plt.figure(figsize=(12.8, 5.4))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    hb = ax1.hexbin(y_pred, y_true, gridsize=150, mincnt=1, linewidths=0)\n",
    "    arr = hb.get_array()\n",
    "    if arr.size:\n",
    "        vmax = float(np.nanpercentile(arr, 99.0))\n",
    "        hb.set_clim(0, vmax)\n",
    "\n",
    "    dmax = float(min(np.nanmax(y_pred), np.nanmax(y_true)))\n",
    "    dmin = 0.0\n",
    "    ax1.plot([dmin, dmax], [dmin, dmax], ls=\"--\", lw=1.2, color=\"0.55\")\n",
    "    ax1.set_xlabel(xlab)\n",
    "    ax1.set_ylabel(ylab)\n",
    "    ax1.grid(alpha=0.22, lw=0.5)\n",
    "    ax1.text(0.03, 0.95,\n",
    "             f\"R2={r2:.3f}\\nMAE={mae:.3f}\\nRMSE={rmse:.3f}\",\n",
    "             transform=ax1.transAxes, va=\"top\", ha=\"left\")\n",
    "\n",
    "    cb = fig.colorbar(hb, ax=ax1, pad=0.01)\n",
    "    cb.set_label(\"Counts\")\n",
    "\n",
    "    # residual hist\n",
    "    try:\n",
    "        lo = float(np.nanpercentile(resid, 0.1))\n",
    "        hi = float(np.nanpercentile(resid, 99.9))\n",
    "        if (not np.isfinite(lo)) or (not np.isfinite(hi)) or lo >= hi:\n",
    "            raise ValueError\n",
    "    except Exception:\n",
    "        lo = float(np.nanmin(resid)) if np.isfinite(resid).any() else -1.0\n",
    "        hi = float(np.nanmax(resid)) if np.isfinite(resid).any() else 1.0\n",
    "        if (not np.isfinite(lo)) or (not np.isfinite(hi)) or lo >= hi:\n",
    "            lo, hi = -1.0, 1.0\n",
    "\n",
    "    ax2.hist(resid, bins=80, alpha=0.9, edgecolor=\"none\")\n",
    "    ax2.set_xlabel(r\"Residual (Obs - Pred) ($\\mu mol\\ kg^{-1}$)\")\n",
    "    ax2.set_ylabel(\"Counts\")\n",
    "    ax2.grid(alpha=0.20, lw=0.5)\n",
    "    ax2.text(0.03, 0.95,\n",
    "             f\"skew={skew:+.3f}\\nkurt={kurt:.3f}\",\n",
    "             transform=ax2.transAxes, va=\"top\", ha=\"left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ============================ Inverse-density weights (FIXED) ============================\n",
    "\n",
    "def build_density_sample_weight(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    5°×5°×10-year bins:\n",
    "      w = 1/sqrt(count), normalize to mean=1\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return np.array([], dtype=float)\n",
    "\n",
    "    w = np.ones(n, dtype=float)\n",
    "    if not ENABLE_DENSITY_WEIGHTING:\n",
    "        return w\n",
    "\n",
    "    if not {\"Latitude\", \"Longitude\", \"Year\"}.issubset(df.columns):\n",
    "        print(\"[DensityWeight] Missing Latitude/Longitude/Year. Fallback to uniform weights.\")\n",
    "        return w\n",
    "\n",
    "    res_deg = float(WEIGHT_GRID_RESOLUTION_DEG)\n",
    "    res_yr  = float(WEIGHT_TIME_RESOLUTION_YR)\n",
    "\n",
    "    lat = pd.to_numeric(df[\"Latitude\"], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "    lon = pd.to_numeric(df[\"Longitude\"], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "    yr  = pd.to_numeric(df[\"Year\"], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "\n",
    "    key_ok = np.isfinite(lat) & np.isfinite(lon) & np.isfinite(yr)\n",
    "    if not np.any(key_ok):\n",
    "        print(\"[DensityWeight] All Latitude/Longitude/Year invalid. Fallback to uniform weights.\")\n",
    "        return w\n",
    "\n",
    "    idx_ok = np.where(key_ok)[0]\n",
    "\n",
    "    # ensure lon in [-180,180)\n",
    "    lon_ok = ((lon[idx_ok] + 180.0) % 360.0) - 180.0\n",
    "\n",
    "    lat_bin  = np.floor(lat[idx_ok] / res_deg).astype(np.int64)\n",
    "    lon_bin  = np.floor(lon_ok / res_deg).astype(np.int64)\n",
    "    time_bin = np.floor(yr[idx_ok]  / res_yr).astype(np.int64)\n",
    "\n",
    "    # int64 key\n",
    "    t0  = int(time_bin.min())\n",
    "    la0 = int(lat_bin.min())\n",
    "    lo0 = int(lon_bin.min())\n",
    "\n",
    "    tb  = (time_bin - t0).astype(np.int64)\n",
    "    lab = (lat_bin  - la0).astype(np.int64)\n",
    "    lob = (lon_bin  - lo0).astype(np.int64)\n",
    "\n",
    "    n_lat = int(lab.max()) + 1\n",
    "    n_lon = int(lob.max()) + 1\n",
    "\n",
    "    key = (tb * np.int64(n_lat) + lab) * np.int64(n_lon) + lob\n",
    "\n",
    "    _, inv = np.unique(key, return_inverse=True)\n",
    "    counts = np.bincount(inv).astype(float)\n",
    "    cnt_each = counts[inv]\n",
    "\n",
    "    ww = 1.0 / np.sqrt(np.maximum(cnt_each, 1.0))\n",
    "\n",
    "    if ENABLE_TRANSITION_PERIOD_PENALTY:\n",
    "        yy = yr[idx_ok]\n",
    "        m = (yy >= float(PENALTY_YEAR_START)) & (yy <= float(PENALTY_YEAR_END))\n",
    "        if np.any(m):\n",
    "            ww[m] *= float(PENALTY_FACTOR)\n",
    "\n",
    "    w[idx_ok] = ww\n",
    "\n",
    "    mean_w = float(np.nanmean(w)) if np.isfinite(w).any() else 1.0\n",
    "    if (not np.isfinite(mean_w)) or mean_w <= 0:\n",
    "        mean_w = 1.0\n",
    "    w = w / mean_w\n",
    "    return w\n",
    "\n",
    "# ============================ Decade-block KFold ============================\n",
    "\n",
    "class DecadeBlockKFold:\n",
    "    def __init__(self, n_splits=5, random_state=42):\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be >= 2\")\n",
    "        self.n_splits = int(n_splits)\n",
    "        self.random_state = int(random_state)\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n = len(X)\n",
    "        if groups is None or len(groups) != n:\n",
    "            rng = np.random.RandomState(self.random_state)\n",
    "            idx = np.arange(n)\n",
    "            rng.shuffle(idx)\n",
    "            folds = np.array_split(idx, min(self.n_splits, max(2, n)))\n",
    "            for f in folds:\n",
    "                va = np.sort(f)\n",
    "                tr = np.sort(np.setdiff1d(idx, va, assume_unique=False))\n",
    "                yield tr, va\n",
    "            return\n",
    "\n",
    "        years = np.asarray(groups, dtype=float)\n",
    "        decade = np.where(np.isfinite(years), (np.floor(years / 10.0) * 10).astype(int), -999999)\n",
    "\n",
    "        uniq_dec = np.unique(decade)\n",
    "        if len(uniq_dec) < 2:\n",
    "            rng = np.random.RandomState(self.random_state)\n",
    "            idx = np.arange(n)\n",
    "            rng.shuffle(idx)\n",
    "            folds = np.array_split(idx, min(self.n_splits, max(2, n)))\n",
    "            for f in folds:\n",
    "                va = np.sort(f)\n",
    "                tr = np.sort(np.setdiff1d(idx, va, assume_unique=False))\n",
    "                yield tr, va\n",
    "            return\n",
    "\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        rng.shuffle(uniq_dec)\n",
    "        k_eff = min(self.n_splits, len(uniq_dec))\n",
    "        dec_folds = np.array_split(uniq_dec, k_eff)\n",
    "\n",
    "        for dec_va in dec_folds:\n",
    "            dec_va_set = set(dec_va.tolist())\n",
    "            va_mask = np.isin(decade, list(dec_va_set))\n",
    "            va_idx = np.where(va_mask)[0]\n",
    "            tr_idx = np.where(~va_mask)[0]\n",
    "            yield tr_idx, va_idx\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        if groups is None:\n",
    "            return self.n_splits\n",
    "        years = np.asarray(groups, dtype=float)\n",
    "        decade = np.where(np.isfinite(years), (np.floor(years / 10.0) * 10).astype(int), -999999)\n",
    "        uniq_dec = np.unique(decade)\n",
    "        return min(self.n_splits, max(2, len(uniq_dec)))\n",
    "\n",
    "# ============================ Model factories ============================\n",
    "\n",
    "def make_cb_regressor_for_fs():\n",
    "    return CatBoostRegressor(\n",
    "        iterations=700,\n",
    "        depth=8,\n",
    "        learning_rate=0.06,\n",
    "        loss_function=\"RMSE\",\n",
    "        eval_metric=\"RMSE\",\n",
    "        random_seed=RANDOM_SEED,\n",
    "        thread_count=N_THREADS,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "# ============================ Weighted permutation importance (CV) ============================\n",
    "\n",
    "def _weighted_mse(y_true, y_pred, w):\n",
    "    y_true = np.asarray(y_true, float)\n",
    "    y_pred = np.asarray(y_pred, float)\n",
    "    w = np.asarray(w, float)\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred) & np.isfinite(w)\n",
    "    if not np.any(m):\n",
    "        return np.nan\n",
    "    y_true = y_true[m]; y_pred = y_pred[m]; w = w[m]\n",
    "    sw = np.sum(w)\n",
    "    if sw <= 0 or not np.isfinite(sw):\n",
    "        return np.nan\n",
    "    err = y_pred - y_true\n",
    "    return float(np.sum(w * err * err) / sw)\n",
    "\n",
    "def permutation_importance_cv_weighted(X_df, y, years, sample_weight,\n",
    "                                      n_splits=5, n_repeats=3, max_val_samples=12000):\n",
    "    cols = list(X_df.columns)\n",
    "    p = len(cols)\n",
    "    scores = np.zeros(p, dtype=float)\n",
    "\n",
    "    cv = DecadeBlockKFold(n_splits=n_splits, random_state=RANDOM_SEED)\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    sw_all = np.asarray(sample_weight, float)\n",
    "    rng_global = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "    for tr_idx, va_idx in cv.split(X_df, y, groups=years):\n",
    "        X_tr = X_df.iloc[tr_idx].values\n",
    "        y_tr = y[tr_idx]\n",
    "        sw_tr = sw_all[tr_idx]\n",
    "\n",
    "        if len(va_idx) > int(max_val_samples):\n",
    "            va_idx = rng_global.choice(va_idx, size=int(max_val_samples), replace=False)\n",
    "\n",
    "        X_va0 = X_df.iloc[va_idx].values\n",
    "        y_va = y[va_idx]\n",
    "        sw_va = sw_all[va_idx]\n",
    "\n",
    "        est = make_cb_regressor_for_fs()\n",
    "        est.fit(X_tr, y_tr, sample_weight=sw_tr)\n",
    "\n",
    "        base_pred = est.predict(X_va0)\n",
    "        base_mse = _weighted_mse(y_va, base_pred, sw_va)\n",
    "        if not np.isfinite(base_mse):\n",
    "            continue\n",
    "\n",
    "        for j in range(p):\n",
    "            inc = 0.0\n",
    "            for _ in range(int(n_repeats)):\n",
    "                X_perm = X_va0.copy()\n",
    "                perm_idx = rng_global.permutation(X_perm.shape[0])\n",
    "                X_perm[:, j] = X_perm[perm_idx, j]\n",
    "                pred = est.predict(X_perm)\n",
    "                mse_perm = _weighted_mse(y_va, pred, sw_va)\n",
    "                if np.isfinite(mse_perm):\n",
    "                    inc += max(0.0, mse_perm - base_mse)\n",
    "            scores[j] += inc / max(1, int(n_repeats))\n",
    "\n",
    "    denom = cv.get_n_splits(X_df, y, groups=years)\n",
    "    if denom <= 0:\n",
    "        denom = 1\n",
    "    scores /= float(denom)\n",
    "    return pd.Series(scores, index=cols).sort_values(ascending=False)\n",
    "\n",
    "# ============================ RFECV with decade-block CV (FIXED) ============================\n",
    "\n",
    "def rfecv_with_decade_blocks(X_df, y, years, sample_weight,\n",
    "                            always_keep=None,\n",
    "                            min_feats=8, step=1, scoring=\"neg_root_mean_squared_error\"):\n",
    "    \"\"\"\n",
    "    RFECV with decade-block CV.\n",
    "\n",
    "    IMPORTANT compatibility note:\n",
    "    - Some sklearn versions do NOT allow passing sample_weight into RFECV.fit().\n",
    "    - We therefore try weighted RFECV first; if TypeError, fall back to unweighted RFECV.\n",
    "      (Other stages still keep weights: permutation importance / Optuna CV / final training.)\n",
    "\n",
    "    Returns:\n",
    "      final_features, rfecv_object, info_dict\n",
    "    \"\"\"\n",
    "    always_keep = list(always_keep) if always_keep else []\n",
    "\n",
    "    cols_all = list(X_df.columns)\n",
    "    p_total = len(cols_all)\n",
    "\n",
    "    keep_in = [c for c in cols_all if c not in always_keep]\n",
    "    X_core = X_df[keep_in].copy()\n",
    "    p_core = len(keep_in)\n",
    "\n",
    "    if USE_ADAPTIVE_RFECV_MIN:\n",
    "        min_total = max(RFECV_MIN_ABS, int(np.ceil(p_total * RFECV_MIN_RATIO)))\n",
    "        min_total = min(min_total, max(1, p_total - 1))\n",
    "        min_total = min(min_total, max(1, min_feats, RFECV_MIN_FEATS))\n",
    "    else:\n",
    "        min_total = min(max(1, min_feats), max(1, p_total - 1))\n",
    "\n",
    "    min_core = max(1, min_total - len(always_keep))\n",
    "    min_core = min(min_core, max(1, p_core - 1))\n",
    "\n",
    "    info = {\n",
    "        \"p_total\": int(p_total),\n",
    "        \"p_core\": int(p_core),\n",
    "        \"min_total_used\": int(min_total),\n",
    "        \"min_core_used\": int(min_core),\n",
    "        \"policy\": (\"adaptive\" if USE_ADAPTIVE_RFECV_MIN else \"fixed\"),\n",
    "        \"ratio\": float(RFECV_MIN_RATIO),\n",
    "        \"abs_min\": int(RFECV_MIN_ABS),\n",
    "        \"fixed_min\": int(RFECV_MIN_FEATS),\n",
    "        \"rfecv_weighted_supported\": None\n",
    "    }\n",
    "\n",
    "    if p_core <= 1 or min_core >= p_core:\n",
    "        final = list(dict.fromkeys(always_keep + keep_in))\n",
    "        return final, None, info\n",
    "\n",
    "    est = make_cb_regressor_for_fs()\n",
    "    cv = DecadeBlockKFold(n_splits=PERM_N_SPLITS, random_state=RANDOM_SEED)\n",
    "\n",
    "    rfecv = RFECV(\n",
    "        estimator=est,\n",
    "        step=step,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        min_features_to_select=min_core,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    # --- try weighted RFECV; fall back if sklearn doesn't support it ---\n",
    "    try:\n",
    "        rfecv.fit(\n",
    "            X_core.values, y,\n",
    "            groups=np.asarray(years),\n",
    "            sample_weight=np.asarray(sample_weight, float)\n",
    "        )\n",
    "        info[\"rfecv_weighted_supported\"] = True\n",
    "    except TypeError:\n",
    "        print(\"[RFECV] The current scikit-learn version does not support RFECV.fit(sample_weight=...). \"\n",
    "              \"Falling back to unweighted RFECV (other stages still use weights: PI/OptunaCV/final training).\")\n",
    "        rfecv.fit(\n",
    "            X_core.values, y,\n",
    "            groups=np.asarray(years)\n",
    "        )\n",
    "        info[\"rfecv_weighted_supported\"] = False\n",
    "\n",
    "    mask = rfecv.support_\n",
    "    selected_core = list(np.array(keep_in)[mask])\n",
    "    final = list(dict.fromkeys(always_keep + selected_core))\n",
    "    return final, rfecv, info\n",
    "\n",
    "# ============================ Optuna objective (decade-block CV, weighted) ============================\n",
    "\n",
    "def objective(trial, X, y, years, sample_weights):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    years = np.asarray(years)\n",
    "    sw_all = np.asarray(sample_weights, float)\n",
    "\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 200, 2500),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.08, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\",6, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 5, 20),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.1, 0.6),\n",
    "        \"bootstrap_type\": \"Bayesian\",\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": 50,\n",
    "        \"use_best_model\": True,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    cv = DecadeBlockKFold(n_splits=5, random_state=RANDOM_SEED)\n",
    "    rmses, best_iters = [], []\n",
    "\n",
    "    for tr_idx, va_idx in cv.split(X, y, groups=years):\n",
    "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        sw_tr = sw_all[tr_idx]\n",
    "        sw_va = sw_all[va_idx]\n",
    "\n",
    "        eval_pool = Pool(X_va, y_va, weight=sw_va)\n",
    "        model = CatBoostRegressor(**params, thread_count=N_THREADS)\n",
    "        model.fit(X_tr, y_tr, sample_weight=sw_tr, eval_set=eval_pool)\n",
    "\n",
    "        preds = model.predict(eval_pool)\n",
    "\n",
    "        err = preds - y_va\n",
    "        denom = np.sum(sw_va)\n",
    "        if denom <= 0 or not np.isfinite(denom):\n",
    "            wrmse = float(np.sqrt(np.mean(err * err)))\n",
    "        else:\n",
    "            wrmse = float(np.sqrt(np.sum(sw_va * err * err) / denom))\n",
    "\n",
    "        rmses.append(wrmse)\n",
    "        try:\n",
    "            best_iters.append(int(model.get_best_iteration()))\n",
    "        except Exception:\n",
    "            best_iters.append(int(params[\"iterations\"]))\n",
    "\n",
    "    trial.set_user_attr(\"best_iters\", best_iters)\n",
    "    trial.set_user_attr(\"median_best_iter\", int(np.median(best_iters)) if best_iters else int(params[\"iterations\"]))\n",
    "    return float(np.mean(rmses)) if rmses else np.nan\n",
    "\n",
    "# ============================ Zone heatmap (unchanged) ============================\n",
    "\n",
    "def plot_zone_feature_importance_heatmap(zone_importances,\n",
    "                                         out_png,\n",
    "                                         out_csv_matrix,\n",
    "                                         out_csv_sum,\n",
    "                                         feature_order=None,\n",
    "                                         annotate=True):\n",
    "    if len(zone_importances) == 0:\n",
    "        return\n",
    "\n",
    "    zone_labels = [str(z.get(\"zone_id\", \"?\")) for z in zone_importances]\n",
    "    feat_sets = [set(z[\"feat_names\"]) for z in zone_importances]\n",
    "    all_features = set().union(*feat_sets)\n",
    "\n",
    "    if feature_order is not None:\n",
    "        features = [f for f in feature_order if f in all_features]\n",
    "    else:\n",
    "        seen = set()\n",
    "        features = []\n",
    "        for z in zone_importances:\n",
    "            for f in z[\"feat_names\"]:\n",
    "                if f not in seen:\n",
    "                    seen.add(f)\n",
    "                    features.append(f)\n",
    "\n",
    "    if len(features) == 0:\n",
    "        return\n",
    "\n",
    "    F = len(features)\n",
    "    Z = len(zone_importances)\n",
    "    M = np.full((F, Z), np.nan, dtype=float)\n",
    "\n",
    "    for j, z in enumerate(zone_importances):\n",
    "        name_to_val = dict(zip(z[\"feat_names\"], np.asarray(z[\"fi\"], float)))\n",
    "        for i, feat in enumerate(features):\n",
    "            val = name_to_val.get(feat, np.nan)\n",
    "            M[i, j] = float(val) if np.isfinite(val) else np.nan\n",
    "\n",
    "    col_sums = np.nansum(M, axis=0)\n",
    "    M_norm = np.full_like(M, np.nan, dtype=float)\n",
    "    for j in range(Z):\n",
    "        denom = col_sums[j]\n",
    "        if np.isfinite(denom) and denom > 0:\n",
    "            M_norm[:, j] = M[:, j] / denom\n",
    "\n",
    "    df_matrix = pd.DataFrame(M_norm, index=features, columns=[f\"Zone_{lbl}\" for lbl in zone_labels])\n",
    "    df_matrix.to_csv(out_csv_matrix)\n",
    "\n",
    "    row_sum = np.nansum(M_norm, axis=1)\n",
    "    presence = np.sum(np.isfinite(M), axis=1).astype(int)\n",
    "    df_sum = pd.DataFrame({\"Feature\": features, \"SumImportance\": row_sum, \"PresentCount\": presence}).set_index(\"Feature\")\n",
    "    df_sum.to_csv(out_csv_sum)\n",
    "\n",
    "    if not ENABLE_PLOTTING:\n",
    "        return\n",
    "\n",
    "    cmap = sns.color_palette(\"inferno\", as_cmap=True)\n",
    "    try:\n",
    "        cmap.set_bad(\"#f0f0f0\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    ax0 = fig.add_subplot(111)\n",
    "\n",
    "    data = df_matrix.values.astype(float)\n",
    "    mask = ~np.isfinite(data)\n",
    "    finite_vals = data[np.isfinite(data)]\n",
    "    vmax = float(np.nanmax(finite_vals)) if finite_vals.size else 1.0\n",
    "    if not np.isfinite(vmax) or vmax <= 0:\n",
    "        vmax = 1.0\n",
    "    norm = Normalize(vmin=0.0, vmax=vmax)\n",
    "\n",
    "    sns.heatmap(data, ax=ax0, mask=mask, cmap=cmap, norm=norm, cbar=True,\n",
    "                cbar_kws={\"label\": \"Normalized importance per zone\"}, linewidths=0)\n",
    "\n",
    "    fig.savefig(out_png, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# ============================ Single-zone training ============================\n",
    "\n",
    "def run_single_zone(depth_out, depth, zone, train_df, test_df):\n",
    "    odir = os.path.join(depth_out, f\"zone_{zone}\")\n",
    "    os.makedirs(odir, exist_ok=True)\n",
    "\n",
    "    tr = train_df[train_df[\"Zone_Merged\"] == zone].reset_index(drop=True)\n",
    "    te = test_df[test_df[\"Zone_Merged\"] == zone].reset_index(drop=True)\n",
    "\n",
    "    if len(tr) < MIN_SAMPLES:\n",
    "        return [], None, None\n",
    "\n",
    "    sw = tr[\"sample_weight\"].values if \"sample_weight\" in tr.columns else np.ones(len(tr), dtype=float)\n",
    "\n",
    "    candidates = [c for c in (CORE_FEATURES + AUX_FEATURES) if c in tr.columns]\n",
    "    if len(candidates) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    chla_offset = None\n",
    "    if \"Chla\" in tr.columns:\n",
    "        chla_offset = compute_chla_offset(tr[\"Chla\"].values)\n",
    "\n",
    "    X_tr_df = transform_features(tr, candidates, chla_offset)\n",
    "    y_tr = tr[TARGET].values\n",
    "    years = tr[TIME].values\n",
    "\n",
    "    always_keep_present = [f for f in ALWAYS_KEEP if f in X_tr_df.columns]\n",
    "\n",
    "    pi_score = None\n",
    "    rfecv_info = None\n",
    "\n",
    "    if ENABLE_FEATURE_SELECTION:\n",
    "        pi_score = permutation_importance_cv_weighted(\n",
    "            X_tr_df, y_tr, years, sw,\n",
    "            n_splits=PERM_N_SPLITS,\n",
    "            n_repeats=PERM_N_REPEATS,\n",
    "            max_val_samples=PERM_MAX_VAL_SAMPLES\n",
    "        )\n",
    "\n",
    "        if PERM_TOPK is None:\n",
    "            k_auto = max(10, int(2 * sqrt(len(pi_score))))\n",
    "            top_feats = list(pi_score.index[:k_auto])\n",
    "        else:\n",
    "            top_feats = list(pi_score.index[:min(int(PERM_TOPK), len(pi_score))])\n",
    "\n",
    "        rfecv_candidates = list(dict.fromkeys(top_feats + always_keep_present))\n",
    "        X_rfecv = X_tr_df[rfecv_candidates].copy()\n",
    "\n",
    "        selected_feats, rfecv_obj, rfecv_info = rfecv_with_decade_blocks(\n",
    "            X_rfecv, y_tr, years, sw,\n",
    "            always_keep=always_keep_present,\n",
    "            min_feats=max(RFECV_MIN_FEATS, len(always_keep_present)),\n",
    "            step=RFECV_STEP,\n",
    "            scoring=RFECV_SCORING\n",
    "        )\n",
    "\n",
    "        selected_feats_sorted = [f for f in pi_score.index if f in selected_feats]\n",
    "        for f in always_keep_present:\n",
    "            if f not in selected_feats_sorted:\n",
    "                selected_feats_sorted = [f] + selected_feats_sorted\n",
    "    else:\n",
    "        selected_feats_sorted = [f for f in candidates if f in X_tr_df.columns]\n",
    "        for f in always_keep_present:\n",
    "            if f not in selected_feats_sorted:\n",
    "                selected_feats_sorted.append(f)\n",
    "\n",
    "    X_tr = X_tr_df[selected_feats_sorted].values\n",
    "\n",
    "    if not te.empty:\n",
    "        X_te_df = transform_features(te, candidates, chla_offset)\n",
    "        X_te_df = X_te_df.reindex(columns=selected_feats_sorted, fill_value=np.nan)\n",
    "        X_te = X_te_df.values\n",
    "    else:\n",
    "        X_te = None\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=RANDOM_SEED))\n",
    "    study.optimize(lambda trial: objective(trial, X_tr, y_tr, years, sw),\n",
    "                   n_trials=N_TRIALS_OPTUNA, show_progress_bar=False)\n",
    "\n",
    "    best_params = study.best_trial.params.copy()\n",
    "    best_iters = study.best_trial.user_attrs.get(\"best_iters\", [])\n",
    "    median_best_iter = int(np.median(best_iters)) if len(best_iters) > 0 else int(best_params.get(\"iterations\", 800))\n",
    "\n",
    "    best_params.update({\n",
    "        \"bootstrap_type\": \"Bayesian\",\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        \"random_seed\": RANDOM_SEED\n",
    "    })\n",
    "\n",
    "    final_iters = int(np.clip(median_best_iter, 200, 2000))\n",
    "    final_params = {**best_params, \"iterations\": final_iters, \"verbose\": False}\n",
    "    for k in [\"od_type\", \"od_wait\", \"use_best_model\"]:\n",
    "        final_params.pop(k, None)\n",
    "\n",
    "    model = CatBoostRegressor(**final_params, thread_count=N_THREADS)\n",
    "    model.fit(X_tr, y_tr, sample_weight=sw)\n",
    "    model.save_model(os.path.join(odir, \"model.cbm\"))\n",
    "\n",
    "    metadata = {\n",
    "        \"features\": selected_feats_sorted,\n",
    "        \"all_candidates\": candidates,\n",
    "        \"always_keep\": always_keep_present,\n",
    "        \"feature_selection_enabled\": bool(ENABLE_FEATURE_SELECTION),\n",
    "        \"chla_offset\": float(chla_offset) if chla_offset is not None else None,\n",
    "        \"params\": model.get_params(),\n",
    "        \"n_train\": int(len(tr)),\n",
    "        \"n_test\": int(len(te)),\n",
    "        \"test_years\": TEST_YEARS,\n",
    "        \"cv_best_iterations_per_fold\": best_iters,\n",
    "        \"final_iterations\": int(final_iters),\n",
    "        \"perm_importance\": (pi_score.to_dict() if pi_score is not None else {}),\n",
    "        \"rfecv_info\": rfecv_info,\n",
    "        \"density_weighting\": {\n",
    "            \"enabled\": bool(ENABLE_DENSITY_WEIGHTING),\n",
    "            \"grid_deg\": float(WEIGHT_GRID_RESOLUTION_DEG),\n",
    "            \"time_yr\": float(WEIGHT_TIME_RESOLUTION_YR),\n",
    "            \"formula\": \"1/sqrt(count), normalized mean=1\"\n",
    "        }\n",
    "    }\n",
    "    joblib.dump(metadata, os.path.join(odir, \"metadata.pkl\"))\n",
    "\n",
    "    y_tr_pred = model.predict(X_tr)\n",
    "    metrics_rows = []\n",
    "\n",
    "    mse = mean_squared_error(y_tr, y_tr_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mae = float(mean_absolute_error(y_tr, y_tr_pred))\n",
    "    r2 = float(r2_score(y_tr, y_tr_pred))\n",
    "    metrics_rows.append([depth, zone, \"train\", mse, rmse, mae, r2])\n",
    "\n",
    "    calib_train = calibration_stats(y_tr, y_tr_pred)\n",
    "    calib_test = {}\n",
    "\n",
    "    if X_te is not None and len(te) > 0:\n",
    "        y_te = te[TARGET].values\n",
    "        y_te_pred = model.predict(X_te)\n",
    "\n",
    "        mse2 = mean_squared_error(y_te, y_te_pred)\n",
    "        rmse2 = float(np.sqrt(mse2))\n",
    "        mae2 = float(mean_absolute_error(y_te, y_te_pred))\n",
    "        r22 = float(r2_score(y_te, y_te_pred))\n",
    "        metrics_rows.append([depth, zone, \"test\", mse2, rmse2, mae2, r22])\n",
    "\n",
    "        calib_test = calibration_stats(y_te, y_te_pred)\n",
    "\n",
    "    # summary\n",
    "    valid_trials = [t for t in study.trials if (t.state == optuna.trial.TrialState.COMPLETE and t.value is not None)]\n",
    "    cv_rmses = [t.value for t in valid_trials]\n",
    "    cv_mean, cv_std = (float(np.mean(cv_rmses)), float(np.std(cv_rmses))) if len(cv_rmses) > 0 else (np.nan, np.nan)\n",
    "\n",
    "    with open(os.path.join(odir, \"zone_summary.txt\"), \"w\") as f:\n",
    "        f.write(f\"Depth: {depth}   Zone: {zone}\\n\")\n",
    "        f.write(f\"Train samples: {len(tr)}   Test samples: {len(te)}\\n\")\n",
    "        f.write(f\"Yearstart: {yearstart}\\n\")\n",
    "        f.write(f\"Test years: {TEST_YEARS}\\n\\n\")\n",
    "        f.write(\"[Density weighting]\\n\")\n",
    "        f.write(f\"  enabled: {ENABLE_DENSITY_WEIGHTING}\\n\")\n",
    "        f.write(f\"  grid_deg: {WEIGHT_GRID_RESOLUTION_DEG}\\n\")\n",
    "        f.write(f\"  time_bin_years: {WEIGHT_TIME_RESOLUTION_YR}\\n\")\n",
    "        f.write(\"  w = 1/sqrt(count), normalized to mean=1\\n\\n\")\n",
    "\n",
    "        f.write(\"[Feature Selection]\\n\")\n",
    "        f.write(f\"  enabled: {ENABLE_FEATURE_SELECTION}\\n\")\n",
    "        f.write(f\"  candidate_features: {candidates}\\n\")\n",
    "        f.write(f\"  always_keep_present: {always_keep_present}\\n\")\n",
    "        if ENABLE_FEATURE_SELECTION and (pi_score is not None):\n",
    "            f.write(f\"  permutation_importance_top: {list(pi_score.index[:min(30, len(pi_score))])}\\n\")\n",
    "            f.write(f\"  rfecv_selected: {selected_feats_sorted}\\n\")\n",
    "            if rfecv_info is not None:\n",
    "                f.write(f\"  rfecv_weighted_supported: {rfecv_info.get('rfecv_weighted_supported')}\\n\")\n",
    "                f.write(f\"  rfecv_min_features_policy: {rfecv_info.get('policy')} \"\n",
    "                        f\"(ratio={rfecv_info.get('ratio')}, abs_min={rfecv_info.get('abs_min')}, fixed_min={rfecv_info.get('fixed_min')})\\n\")\n",
    "        else:\n",
    "            f.write(\"  selection_skipped: used all available candidate features\\n\")\n",
    "            f.write(f\"  used_features: {selected_feats_sorted}\\n\")\n",
    "        if chla_offset is not None:\n",
    "            f.write(f\"  chla_log10_offset: {chla_offset:.6g}\\n\")\n",
    "\n",
    "        f.write(\"\\n[HPO / Optuna]\\n\")\n",
    "        f.write(f\"  best_trial_value (weighted decade-block CV RMSE): {study.best_value:.6f}\\n\")\n",
    "        f.write(f\"  best_trial_params: {study.best_trial.params}\\n\")\n",
    "        f.write(f\"  completed_trials: {len(valid_trials)} / {len(study.trials)}\\n\")\n",
    "        f.write(f\"  best_iterations_per_fold: {best_iters}\\n\")\n",
    "        f.write(f\"  median_best_iter: {median_best_iter}\\n\")\n",
    "        f.write(f\"  final_training_iterations: {final_iters}\\n\")\n",
    "        f.write(f\"  CV RMSE stats over trials: mean={cv_mean:.6f}, std={cv_std:.6f}\\n\\n\")\n",
    "\n",
    "        f.write(\"[Metrics]\\n\")\n",
    "        f.write(\"  Set      MSE        RMSE        MAE         R2\\n\")\n",
    "        for row in metrics_rows:\n",
    "            _, _, nm, mse_, rmse_, mae_, r2_ = row\n",
    "            f.write(f\"  {nm:<6} {mse_:10.6f} {rmse_:10.6f} {mae_:10.6f} {r2_:10.6f}\\n\")\n",
    "\n",
    "        def _fmt_c(cal):\n",
    "            return (f\"slope={cal.get('slope', np.nan):.6f}, \"\n",
    "                    f\"intercept={cal.get('intercept', np.nan):.6f}, \"\n",
    "                    f\"bias={cal.get('bias', np.nan):.6f} (Pred-Obs), \"\n",
    "                    f\"R2={cal.get('r2', np.nan):.6f}\")\n",
    "        f.write(\"\\n[Calibration]\\n\")\n",
    "        f.write(f\"  train: {_fmt_c(calib_train)}\\n\")\n",
    "        if calib_test:\n",
    "            f.write(f\"  test : {_fmt_c(calib_test)}\\n\")\n",
    "\n",
    "    fi_zone = np.asarray(model.get_feature_importance(), float)\n",
    "    zh = {\"feat_names\": selected_feats_sorted, \"fi\": fi_zone, \"weight\": len(tr), \"zone_id\": zone}\n",
    "\n",
    "    del model, X_tr, X_te\n",
    "    plt.close(\"all\")\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics_rows, None, zh\n",
    "\n",
    "# ============================ Per-depth pipeline ============================\n",
    "\n",
    "def _predict_overall_test_for_depth(depth_out: str, test_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Build overall test predictions across all zone models for this depth by loading\n",
    "    each zone model + metadata, predicting its zone rows, then writing:\n",
    "      - test_with_pred.csv\n",
    "      - overall_test_metrics.txt\n",
    "      - optional test_panel.png\n",
    "\n",
    "    Notes:\n",
    "    - This function stitches zone-wise predictions into one global test table.\n",
    "    - Metrics are computed on rows where Pred is finite.\n",
    "    \"\"\"\n",
    "    if test_df is None or len(test_df) == 0:\n",
    "        return\n",
    "\n",
    "    test_df_total = test_df.copy()\n",
    "    test_df_total[\"Pred\"] = np.nan\n",
    "\n",
    "    # robust zone list (Zone_Merged may be int or nullable int)\n",
    "    zones_in_test = (\n",
    "        pd.to_numeric(test_df_total[\"Zone_Merged\"], errors=\"coerce\")\n",
    "        .dropna()\n",
    "        .astype(int)\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "    zones_in_test = sorted(zones_in_test)\n",
    "    if len(zones_in_test) == 0:\n",
    "        return\n",
    "\n",
    "    n_skipped_zones = 0\n",
    "\n",
    "    for zone in zones_in_test:\n",
    "        odir = os.path.join(depth_out, f\"zone_{zone}\")\n",
    "        meta_path = os.path.join(odir, \"metadata.pkl\")\n",
    "        model_path = os.path.join(odir, \"model.cbm\")\n",
    "        if not (os.path.exists(meta_path) and os.path.exists(model_path)):\n",
    "            n_skipped_zones += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            meta = joblib.load(meta_path)\n",
    "        except Exception:\n",
    "            n_skipped_zones += 1\n",
    "            continue\n",
    "\n",
    "        feat_order = meta.get(\"features\", [])\n",
    "        candidates = meta.get(\"all_candidates\", [])\n",
    "        chla_offset = meta.get(\"chla_offset\", None)\n",
    "\n",
    "        if not feat_order or not candidates:\n",
    "            n_skipped_zones += 1\n",
    "            continue\n",
    "\n",
    "        mask_te = (pd.to_numeric(test_df_total[\"Zone_Merged\"], errors=\"coerce\").astype(\"Int64\") == int(zone))\n",
    "        if not bool(mask_te.any()):\n",
    "            continue\n",
    "\n",
    "        df_part = test_df_total.loc[mask_te].copy()\n",
    "\n",
    "        # Build X in the same way as training, using stored candidates and feature order\n",
    "        try:\n",
    "            Xf = transform_features(df_part, candidates, chla_offset)\n",
    "            Xf = Xf.reindex(columns=feat_order, fill_value=np.nan)\n",
    "            X = Xf.values\n",
    "        except Exception:\n",
    "            n_skipped_zones += 1\n",
    "            continue\n",
    "\n",
    "        model = CatBoostRegressor()\n",
    "        model.load_model(model_path)\n",
    "        try:\n",
    "            pred = model.predict(X)\n",
    "            test_df_total.loc[mask_te, \"Pred\"] = pred\n",
    "        except Exception:\n",
    "            n_skipped_zones += 1\n",
    "        finally:\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "    # ---- Output stitched global test set ----\n",
    "    test_csv = os.path.join(depth_out, \"test_with_pred.csv\")\n",
    "    test_df_total.to_csv(test_csv, index=False)\n",
    "\n",
    "    # ---- Overall metrics (rows with finite Pred + Oxygen) ----\n",
    "    y_true = pd.to_numeric(test_df_total[TARGET], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "    y_pred = pd.to_numeric(test_df_total[\"Pred\"], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "\n",
    "    n_total = int(len(test_df_total))\n",
    "    n_eval = int(np.sum(m))\n",
    "\n",
    "    metrics_path = os.path.join(depth_out, \"overall_test_metrics.txt\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        f.write(\"Overall test performance across all zone models (this depth)\\n\")\n",
    "        f.write(f\"  test_rows_total: {n_total}\\n\")\n",
    "        f.write(f\"  test_rows_with_pred: {n_eval}\\n\")\n",
    "        f.write(f\"  test_rows_missing_pred: {n_total - n_eval}\\n\")\n",
    "        f.write(f\"  zones_in_test: {len(zones_in_test)}\\n\")\n",
    "        f.write(f\"  zones_missing_model_or_meta_or_failed: {n_skipped_zones}\\n\\n\")\n",
    "\n",
    "        if n_eval <= 0:\n",
    "            f.write(\"No valid predictions available to compute metrics.\\n\")\n",
    "            return\n",
    "\n",
    "        yt = y_true[m]\n",
    "        yp = y_pred[m]\n",
    "\n",
    "        mse = float(mean_squared_error(yt, yp))\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        mae = float(mean_absolute_error(yt, yp))\n",
    "        r2 = float(r2_score(yt, yp))\n",
    "\n",
    "        f.write(\"[Metrics]\\n\")\n",
    "        f.write(f\"  RMSE: {rmse:.6f}\\n\")\n",
    "        f.write(f\"  MAE : {mae:.6f}\\n\")\n",
    "        f.write(f\"  R2  : {r2:.6f}\\n\\n\")\n",
    "\n",
    "        cal = calibration_stats(yt, yp)\n",
    "        f.write(\"[Calibration]\\n\")\n",
    "        f.write(f\"  slope={cal.get('slope', np.nan):.6f}, \"\n",
    "                f\"intercept={cal.get('intercept', np.nan):.6f}, \"\n",
    "                f\"bias={cal.get('bias', np.nan):.6f} (Pred-Obs), \"\n",
    "                f\"R2={cal.get('r2', np.nan):.6f}\\n\")\n",
    "\n",
    "    if ENABLE_PLOTTING and n_eval > 0:\n",
    "        plot_test_panel(\n",
    "            y_true[m], y_pred[m],\n",
    "            out_path=os.path.join(depth_out, \"test_panel.png\")\n",
    "        )\n",
    "\n",
    "def process_depth(depth):\n",
    "    global MERGE_MAP\n",
    "\n",
    "    csv_path = os.path.join(ROOT_DIR, f\"{depth}dbar\", f\"depth{depth}_TRAIN.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[Depth {depth}] Missing: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"Date\"], low_memory=False)\n",
    "\n",
    "    df = df[\n",
    "        (df[TIME] >= yearstart)\n",
    "    ].copy()\n",
    "\n",
    "    df[ZONE] = pd.to_numeric(df[ZONE], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[ZONE].between(1, 20, inclusive=\"both\")].copy()\n",
    "\n",
    "    df[\"Zone_Merged\"] = df[ZONE].apply(lambda z: MERGE_MAP.get(int(z), int(z)) if pd.notna(z) else z)\n",
    "\n",
    "    if ENFORCE_VALID_FEATURES:\n",
    "        validFEATURES = [Satname, timename, \"Latitude\", TempName, SalName]\n",
    "        features_to_check = [c for c in validFEATURES if c in df.columns]\n",
    "        before = len(df)\n",
    "        df = enforce_valid_feature_rows(\n",
    "            df,\n",
    "            features=features_to_check,\n",
    "            sentinel_values=SENTINEL_VALUES,\n",
    "            feature_ranges=FEATURE_RANGES\n",
    "        )\n",
    "        after = len(df)\n",
    "        print(f\"[Depth {depth}] Valid feature filter: kept {after}/{before}.\")\n",
    "\n",
    "    train_df = df[~df[TIME].isin(TEST_YEARS)].reset_index(drop=True).copy()\n",
    "    test_df  = df[df[TIME].isin(TEST_YEARS)].reset_index(drop=True).copy()\n",
    "\n",
    "    train_df[\"sample_weight\"] = build_density_sample_weight(train_df)\n",
    "\n",
    "    if len(train_df) > 0:\n",
    "        sw = train_df[\"sample_weight\"].values\n",
    "        print(f\"[Depth {depth}] sample_weight built. \"\n",
    "              f\"stats: min={np.min(sw):.4f} max={np.max(sw):.4f} mean={np.mean(sw):.4f} \"\n",
    "              f\"(grid={WEIGHT_GRID_RESOLUTION_DEG}°, time={WEIGHT_TIME_RESOLUTION_YR}yr)\")\n",
    "\n",
    "    depth_out = os.path.join(OUTPUT_DIR, f\"depth_{depth}\")\n",
    "    os.makedirs(depth_out, exist_ok=True)\n",
    "\n",
    "    zone_ids_sorted = sorted(train_df[\"Zone_Merged\"].dropna().unique().astype(int))\n",
    "\n",
    "    results = Parallel(\n",
    "        n_jobs=N_ZONE_WORKERS,\n",
    "        backend=\"loky\",\n",
    "        prefer=\"processes\",\n",
    "        pre_dispatch=\"n_jobs\"\n",
    "    )(\n",
    "        delayed(run_single_zone)(depth_out, depth, int(zone), train_df, test_df)\n",
    "        for zone in zone_ids_sorted\n",
    "    )\n",
    "\n",
    "    overall_metrics = []\n",
    "    zone_importances = []\n",
    "\n",
    "    for (metrics_rows, _, zh) in results:\n",
    "        overall_metrics.extend(metrics_rows or [])\n",
    "        if zh:\n",
    "            zone_importances.append(zh)\n",
    "\n",
    "    if overall_metrics:\n",
    "        pd.DataFrame(overall_metrics, columns=[\"Depth\", Zonename, \"Set\", \"MSE\", \"RMSE\", \"MAE\", \"R2\"]) \\\n",
    "            .to_csv(os.path.join(depth_out, \"all_zones_metrics.csv\"), index=False)\n",
    "\n",
    "    zone_counts = train_df.groupby(\"Zone_Merged\").size().reset_index(name=\"Train_Sample_Count\")\n",
    "    zone_counts.to_csv(os.path.join(depth_out, \"zone_train_sample_counts.csv\"), index=False)\n",
    "\n",
    "    desired_feature_order = CORE_FEATURES + AUX_FEATURES\n",
    "    plot_zone_feature_importance_heatmap(\n",
    "        zone_importances,\n",
    "        out_png=os.path.join(depth_out, \"zone_feature_importance_heatmap.png\"),\n",
    "        out_csv_matrix=os.path.join(depth_out, \"zone_feature_importance_matrix.csv\"),\n",
    "        out_csv_sum=os.path.join(depth_out, \"zone_feature_importance_sum.csv\"),\n",
    "        feature_order=desired_feature_order,\n",
    "        annotate=True\n",
    "    )\n",
    "\n",
    "    # ===== NEW: overall/global test-set prediction + overall metrics across all zones =====\n",
    "    _predict_overall_test_for_depth(depth_out, test_df)\n",
    "\n",
    "    print(f\"[Depth {depth}] Done. Outputs in: {depth_out}\")\n",
    "\n",
    "# ============================ Hard memory cleanup ============================\n",
    "\n",
    "def _hard_memory_cleanup():\n",
    "    try:\n",
    "        plt.close(\"all\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from joblib.externals.loky import get_reusable_executor\n",
    "        get_reusable_executor().shutdown(wait=True, kill_workers=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "    for _ in range(2):\n",
    "        gc.collect()\n",
    "    try:\n",
    "        import ctypes\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ============================ Main ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[Config] yearstart={yearstart}, TEST_YEARS={TEST_YEARS}\")\n",
    "    print(f\"[Config] DensityWeighting={ENABLE_DENSITY_WEIGHTING} \"\n",
    "          f\"(grid={WEIGHT_GRID_RESOLUTION_DEG}°, time={WEIGHT_TIME_RESOLUTION_YR}yr)\")\n",
    "    print(f\"[Config] CV = DecadeBlockKFold (10-year blocks)\")\n",
    "\n",
    "    for d in depthlist:\n",
    "        adjust_features_and_merge(d)\n",
    "        process_depth(d)\n",
    "\n",
    "    print(\"All depths done.\")\n",
    "    _hard_memory_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, glob, warnings, argparse\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========= Threads & parallelism (set BEFORE importing numeric libs) =========\n",
    "DEFAULT_THREADS = 16\n",
    "THREADS = int(os.getenv(\"PRED_THREADS\", str(DEFAULT_THREADS)))\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", str(THREADS))\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", str(THREADS))\n",
    "\n",
    "# ========= Numeric/data & ML =========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Optional dependencies\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    lgb = None\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception:\n",
    "    xgb = None\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ============== Optional dependencies for remap ==============\n",
    "try:\n",
    "    from netCDF4 import Dataset\n",
    "except Exception:\n",
    "    Dataset = None\n",
    "try:\n",
    "    from sklearn.neighbors import BallTree\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    BallTree = None\n",
    "    HAVE_SKLEARN = False\n",
    "try:\n",
    "    from threadpoolctl import threadpool_limits\n",
    "except Exception:\n",
    "    threadpool_limits = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration you must adapt to your environment (must match training outputs)\n",
    "# =============================================================================\n",
    "\n",
    "# Prediction input root directory (one subdir per depth, e.g. \"1dbar\", \"10dbar\"...)\n",
    "INPUT_ROOT   = \"/data/wang/Result_Data/allnodoxy\"\n",
    "\n",
    "MODELS_ROOT  = OUTPUT_DIR\n",
    "DEPTH_LIST   = depthlist\n",
    "\n",
    "CSV_NAME_TEMPLATE = \"depth{depth}_{season}_TRAIN.csv\"\n",
    "\n",
    "Zonename = Zonename\n",
    "TempName = TempName\n",
    "SalName  = SalName\n",
    "O2SAT_NAME = Satname\n",
    "TIMENAME = timename\n",
    "\n",
    "SEASONS = SEASONS\n",
    "\n",
    "# Columns to keep when writing *_with_pred.csv (adjust if needed)\n",
    "KEEP_COLS = [\"Year\", \"Month\", \"Latitude\", \"Longitude\", Zonename, \"Oxygen\"]\n",
    "\n",
    "# Whether to delete the original CSV after writing the new CSV\n",
    "DELETE_ORIGINAL = False\n",
    "\n",
    "\n",
    "# Remap switch (inference-time boundary fusion)\n",
    "REMAP_ENABLED_GLOBAL = True\n",
    "REMAP_SMOOTH_KM = float(os.getenv(\"REMAP_SMOOTH_KM\", \"300.0\"))\n",
    "REMAP_WORKERS   = min(THREADS, 16)\n",
    "\n",
    "# MeanBiomes and LoS mask paths\n",
    "BIOMES_NC       = os.getenv(\"BIOMES_NC\", \"/data/wang/Merage_Biomes_0p5deg.nc\")\n",
    "REMAP_MASK_NC   = os.getenv(\"REMAP_MASK_NC\", \"/data/wang/mask_lineofsight.nc\")\n",
    "\n",
    "EARTH_R_KM = 6371.0\n",
    "\n",
    "\n",
    "# Whether to perform seam leveling (fit per-zone bias then smooth within region)\n",
    "SEAM_LEVELING   = False\n",
    "SEAM_K          = 10\n",
    "SEAM_MAX_DIST_KM= 200.0\n",
    "SEAM_RIDGE      = 1e-3\n",
    "\n",
    "# Remap debugging\n",
    "REMAP_DEBUG      = False\n",
    "REMAP_TRACE_N    = 8\n",
    "REMAP_TRACE_SEED = 42\n",
    "REMAP_DIAG_CSV   = False\n",
    "\n",
    "\n",
    "# Candidate aliases for temperature/salinity columns in CSV\n",
    "TEMP_ALIASES = [\"Temp\", \"Temperature\"]\n",
    "SAL_ALIASES  = [\"Sal\", \"Salinity\"]\n",
    "\n",
    "# =============================================================================\n",
    "# Global state for feature/merge policy (mirrors training-side structure)\n",
    "# =============================================================================\n",
    "AUX_FEATURES = []\n",
    "MERGE_MAP = {}\n",
    "\n",
    "# =============================================================================\n",
    "# Zone merging policy (inference-side; keep consistent with training policy)\n",
    "# =============================================================================\n",
    "def adjust_features_and_mergepred(depth):\n",
    "    \"\"\"\n",
    "    Goal: remain structurally consistent with training-side adjust_features_and_merge(depth).\n",
    "\n",
    "    - Set AUX_FEATURES by depth\n",
    "    - Set MERGE_ZONES_1_TO_12 by depth\n",
    "    - Build MERGE_MAP\n",
    "        * If MERGE_ZONES_1_TO_12=True: map zones to a single zone (deep ocean), and disable remap\n",
    "        * Else: keep remap enabled\n",
    "    \"\"\"\n",
    "    global AUX_FEATURES, MERGE_ZONES_1_TO_12, MERGE_MAP\n",
    "\n",
    "    depth_int = int(depth)\n",
    "    AUX_FEATURES = [Satname, \"U\", \"V\", \"SSH\", \"pCO2\", \"DIC\", \"Chla\", \"MLD\", \"pH\", \"CO2_flux\", \"Alkalinity\"]\n",
    "\n",
    "    # Keep consistent with training: depth-based merge policy\n",
    "    if 0 < depth_int <= 3000:\n",
    "        MERGE_ZONES_1_TO_12 = False\n",
    "    else:\n",
    "        MERGE_ZONES_1_TO_12 = True\n",
    "\n",
    "    # Keep consistent with training: build MERGE_MAP\n",
    "    MERGE_MAP = {}\n",
    "    remap_enabled = True\n",
    "\n",
    "    if MERGE_ZONES_1_TO_12:\n",
    "        # Deep ocean: map multiple basins into one\n",
    "        for o in [1, 2, 3, 4, 5]:\n",
    "            MERGE_MAP[o] = 1\n",
    "        remap_enabled = False\n",
    "\n",
    "    print(\n",
    "        f\"[POLICY] depth={depth_int} | AUX_FEATURES={AUX_FEATURES} | \"\n",
    "        f\"MERGE_ZONES_1_TO_12={MERGE_ZONES_1_TO_12} | MERGE_MAP={MERGE_MAP} | \"\n",
    "        f\"remap_enabled={remap_enabled}\"\n",
    "    )\n",
    "    return MERGE_MAP, remap_enabled\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utilities: file/depth discovery\n",
    "# =============================================================================\n",
    "def parse_depth_from_dir(dirname):\n",
    "    m = re.search(r\"(\\d+)dbar$\", os.path.basename(dirname))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def find_depth_dirs(root):\n",
    "    dirs = [p for p in glob.glob(os.path.join(root, \"*\")) if os.path.isdir(p)]\n",
    "    return [d for d in dirs if re.search(r\"\\d+dbar$\", os.path.basename(d))]\n",
    "\n",
    "def list_depths_to_run(input_root, depth_list_cfg=None, depth_list_cli=None):\n",
    "    if depth_list_cli and len(depth_list_cli) > 0:\n",
    "        candidates = [str(d) for d in depth_list_cli]\n",
    "    elif depth_list_cfg and len(depth_list_cfg) > 0:\n",
    "        candidates = [str(d) for d in depth_list_cfg]\n",
    "    else:\n",
    "        ddirs = find_depth_dirs(input_root)\n",
    "        candidates = []\n",
    "        for d in ddirs:\n",
    "            dep = parse_depth_from_dir(d)\n",
    "            if dep:\n",
    "                candidates.append(dep)\n",
    "\n",
    "    uniq = sorted(set(candidates), key=lambda x: int(re.sub(r\"\\D\", \"\", x)))\n",
    "    result = []\n",
    "    for dep in uniq:\n",
    "        ddir = os.path.join(input_root, f\"{dep}dbar\")\n",
    "        if os.path.isdir(ddir):\n",
    "            result.append(dep)\n",
    "        else:\n",
    "            print(f\"[WARN] Depth directory does not exist (skipped): {ddir}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utilities: column normalization + base feature compatibility\n",
    "# =============================================================================\n",
    "def normalize_temp_sal_columns(df: pd.DataFrame,\n",
    "                               temp_target: str = TempName,\n",
    "                               sal_target: str = SalName,\n",
    "                               temp_aliases=None,\n",
    "                               sal_aliases=None) -> pd.DataFrame:\n",
    "    rename_map = {}\n",
    "\n",
    "    if (\"Temperature\" in df.columns) and (temp_target not in df.columns):\n",
    "        rename_map[\"Temperature\"] = temp_target\n",
    "    if (\"Salinity\" in df.columns) and (sal_target not in df.columns):\n",
    "        rename_map[\"Salinity\"] = sal_target\n",
    "\n",
    "    if (\"Temperature\" not in df.columns) and (temp_target not in df.columns):\n",
    "        if temp_aliases is None:\n",
    "            temp_aliases = TEMP_ALIASES\n",
    "        for c in temp_aliases:\n",
    "            if c in df.columns:\n",
    "                rename_map.setdefault(c, temp_target)\n",
    "                break\n",
    "\n",
    "    if (\"Salinity\" not in df.columns) and (sal_target not in df.columns):\n",
    "        if sal_aliases is None:\n",
    "            sal_aliases = SAL_ALIASES\n",
    "        for c in sal_aliases:\n",
    "            if c in df.columns:\n",
    "                rename_map.setdefault(c, sal_target)\n",
    "                break\n",
    "\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "        print(f\"  -> Mapped column names: {rename_map}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def wrap_lon180(lon_deg):\n",
    "    lon = (np.asarray(lon_deg) + 180.0) % 360.0 - 180.0\n",
    "    lon = np.where(lon == -180.0, 180.0, lon)\n",
    "    return lon.item() if np.ndim(lon) == 0 else lon\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model discovery & loading (consistent with training output structure)\n",
    "# =============================================================================\n",
    "def detect_model_file(zdir):\n",
    "    candidates = []\n",
    "    cbm = os.path.join(zdir, \"model.cbm\")\n",
    "    if os.path.exists(cbm):\n",
    "        candidates.append((\"catboost\", cbm))\n",
    "    lgb_pkl = os.path.join(zdir, \"model_lgbm.pkl\")\n",
    "    if os.path.exists(lgb_pkl):\n",
    "        candidates.append((\"lgbm_sklearn\", lgb_pkl))\n",
    "    for name in (\"model.lgb\", \"model.txt\"):\n",
    "        p = os.path.join(zdir, name)\n",
    "        if os.path.exists(p):\n",
    "            candidates.append((\"lgbm_booster\", p))\n",
    "    xgb_json = os.path.join(zdir, \"model.json\")\n",
    "    if os.path.exists(xgb_json):\n",
    "        candidates.append((\"xgb_booster\", xgb_json))\n",
    "    xgb_pkl = os.path.join(zdir, \"model_xgb.pkl\")\n",
    "    if os.path.exists(xgb_pkl):\n",
    "        candidates.append((\"xgb_sklearn\", xgb_pkl))\n",
    "\n",
    "    all_found = [f\"{t}:{p}\" for t, p in candidates]\n",
    "    if not candidates:\n",
    "        return None, None, all_found\n",
    "    return candidates[0][0], candidates[0][1], all_found\n",
    "\n",
    "@lru_cache(maxsize=256)\n",
    "def load_zone_cached(depth_str, zone_id):\n",
    "    zdir = os.path.join(MODELS_ROOT, f\"depth_{depth_str}\", f\"zone_{zone_id}\")\n",
    "    meta_path = os.path.join(zdir, \"metadata.pkl\")\n",
    "    if not (os.path.exists(zdir) and os.path.exists(meta_path)):\n",
    "        return None\n",
    "\n",
    "    mtype, mpath, all_found = detect_model_file(zdir)\n",
    "    if mtype is None:\n",
    "        print(f\"    [WARN] No usable model file found in {zdir}.\")\n",
    "        return None\n",
    "    if len(all_found) > 1:\n",
    "        print(f\"    [INFO] Multiple model files found; using priority: {mtype} -> {mpath}; all: {all_found}\")\n",
    "\n",
    "    meta = joblib.load(meta_path)\n",
    "\n",
    "    model = None\n",
    "    try:\n",
    "        if mtype == \"catboost\":\n",
    "            model = CatBoostRegressor()\n",
    "            model.load_model(mpath)\n",
    "\n",
    "        elif mtype == \"lgbm_sklearn\":\n",
    "            if lgb is None:\n",
    "                raise RuntimeError(\"lightgbm is not installed (pip install lightgbm).\")\n",
    "            model = joblib.load(mpath)\n",
    "            try:\n",
    "                model.set_params(n_jobs=THREADS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        elif mtype == \"lgbm_booster\":\n",
    "            if lgb is None:\n",
    "                raise RuntimeError(\"lightgbm is not installed (pip install lightgbm).\")\n",
    "            model = lgb.Booster(model_file=mpath)\n",
    "            try:\n",
    "                model.reset_parameter({\"num_threads\": THREADS})\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        elif mtype == \"xgb_sklearn\":\n",
    "            if xgb is None:\n",
    "                raise RuntimeError(\"xgboost is not installed (pip install xgboost).\")\n",
    "            model = joblib.load(mpath)\n",
    "            try:\n",
    "                model.set_params(n_jobs=THREADS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        elif mtype == \"xgb_booster\":\n",
    "            if xgb is None:\n",
    "                raise RuntimeError(\"xgboost is not installed (pip install xgboost).\")\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(mpath)\n",
    "            try:\n",
    "                booster.set_param({\"nthread\": THREADS})\n",
    "            except Exception:\n",
    "                pass\n",
    "            model = booster\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown model type: {mtype}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model ({mtype}): {e}\")\n",
    "\n",
    "    return {\n",
    "        \"meta\": meta,\n",
    "        \"model_type\": mtype,\n",
    "        \"model\": model,\n",
    "        \"model_path\": mpath,\n",
    "        \"features\": meta.get(\"features\", []),\n",
    "        \"core_cols\": meta.get(\"core_cols\", []),\n",
    "        \"aux_pool\": meta.get(\"aux_pool\", []),\n",
    "        \"selected_aux\": meta.get(\"selected_aux\", []),\n",
    "        \"chla_offset\": meta.get(\"chla_offset\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Feature matrix construction (compat: fill missing weight features if needed)\n",
    "# =============================================================================\n",
    "def apply_chla_log(arr_like, offset):\n",
    "    z = np.asarray(arr_like, float)\n",
    "    z = np.where(np.isfinite(z), z, np.nan)\n",
    "    return np.log10(np.maximum(z, 0) + (offset if offset is not None else 1e-6))\n",
    "\n",
    "def _maybe_fill_weight_feature(df_part: pd.DataFrame, col: str):\n",
    "    \"\"\"\n",
    "    Inference compatibility:\n",
    "    If training included a density-weight feature but inference CSV lacks it,\n",
    "    fill it with constant 1.0 so the model can still run.\n",
    "    \"\"\"\n",
    "    if col in df_part.columns:\n",
    "        return\n",
    "    if col.lower() in {\"w_density\", \"density_weight\", \"w_den\", \"w_dens\", \"w_density_5deg10yr\"}:\n",
    "        df_part[col] = 1.0\n",
    "\n",
    "def make_feature_matrix(df_part, zone_meta, return_names=False):\n",
    "    feat_order = zone_meta.get(\"features\", [])\n",
    "    if not feat_order:\n",
    "        core_cols    = zone_meta.get(\"core_cols\", []) or []\n",
    "        selected_aux = zone_meta.get(\"selected_aux\", []) or []\n",
    "        feat_order   = [c for c in (list(core_cols) + list(selected_aux)) if c]\n",
    "\n",
    "    DISCRETE_HARD_BOUNDARY_COLS = {Zonename, \"Zone_Merged\"}\n",
    "    feat_order = [c for c in feat_order if c not in DISCRETE_HARD_BOUNDARY_COLS]\n",
    "\n",
    "    # Compatibility: fill possible \"weight feature columns\"\n",
    "    for c in feat_order:\n",
    "        _maybe_fill_weight_feature(df_part, c)\n",
    "\n",
    "    n_rows = len(df_part)\n",
    "    n_cols = len(feat_order)\n",
    "    Xf = np.full((n_rows, n_cols), np.nan, dtype=float)\n",
    "\n",
    "    cols = df_part.columns\n",
    "    chla_offset = zone_meta.get(\"chla_offset\", None)\n",
    "\n",
    "    for j, col in enumerate(feat_order):\n",
    "        if col == \"Chla\":\n",
    "            if \"Chla\" in cols:\n",
    "                Xf[:, j] = apply_chla_log(df_part[\"Chla\"], chla_offset)\n",
    "            continue\n",
    "        if col in cols:\n",
    "            Xf[:, j] = pd.to_numeric(df_part[col], errors=\"coerce\").to_numpy(dtype=float, copy=False)\n",
    "\n",
    "    if return_names:\n",
    "        return Xf, feat_order\n",
    "    return Xf\n",
    "\n",
    "def predict_zone_block(df_part, zone_meta):\n",
    "    mtype = zone_meta[\"model_type\"]\n",
    "    model = zone_meta[\"model\"]\n",
    "    X, feat_names = make_feature_matrix(df_part, zone_meta, return_names=True)\n",
    "    try:\n",
    "        if mtype == \"catboost\":\n",
    "            yhat = model.predict(X, thread_count=THREADS)\n",
    "            return np.asarray(yhat, float)\n",
    "        elif mtype == \"lgbm_sklearn\":\n",
    "            try:\n",
    "                yhat = model.predict(X, num_threads=THREADS)\n",
    "            except TypeError:\n",
    "                yhat = model.predict(X)\n",
    "            return np.asarray(yhat, float)\n",
    "        elif mtype == \"lgbm_booster\":\n",
    "            try:\n",
    "                yhat = model.predict(X, num_threads=THREADS)\n",
    "            except TypeError:\n",
    "                yhat = model.predict(X)\n",
    "            return np.asarray(yhat, float)\n",
    "        elif mtype == \"xgb_sklearn\":\n",
    "            yhat = model.predict(X)\n",
    "            return np.asarray(yhat, float)\n",
    "        elif mtype == \"xgb_booster\":\n",
    "            dmat = xgb.DMatrix(\n",
    "                X, feature_names=feat_names,\n",
    "                nthread=THREADS, enable_categorical=False, missing=np.nan\n",
    "            )\n",
    "            yhat = model.predict(dmat, validate_features=False)\n",
    "            return np.asarray(yhat, float)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown model type: {mtype}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"{mtype} prediction failed: {e}\")\n",
    "\n",
    "\n",
    "def apply_zone_merge(df, zone_col=Zonename, merge_map=None, out_col=\"Zone_Merged\"):\n",
    "    if merge_map is None:\n",
    "        merge_map = {}\n",
    "    mm = {}\n",
    "    for k, v in merge_map.items():\n",
    "        try:\n",
    "            mm[int(k)] = int(v)\n",
    "        except Exception:\n",
    "            pass\n",
    "    s_num  = pd.to_numeric(df[zone_col], errors=\"coerce\").astype(\"Int64\")\n",
    "    merged = s_num.replace(mm)\n",
    "    df[out_col] = merged\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Remap: MeanBiomes reading, adjacency, and shared-boundary distances\n",
    "# =============================================================================\n",
    "@lru_cache(maxsize=4)\n",
    "def load_biomes(nc_path):\n",
    "    if Dataset is None:\n",
    "        raise RuntimeError(\"netCDF4 is not installed; cannot load MeanBiomes (pip install netCDF4).\")\n",
    "    ds = Dataset(nc_path, \"r\")\n",
    "    lon = ds[\"lon\"][:].astype(np.float64)          # (360,)\n",
    "    lat = ds[\"lat\"][:].astype(np.float64)          # (180,)\n",
    "    Z   = ds[\"MeanBiomes\"][:].astype(np.float64)   # (lon, lat)\n",
    "    ds.close()\n",
    "    Z = Z.T  # -> (lat, lon)\n",
    "    return lat, lon, Z\n",
    "\n",
    "def apply_merge_map_to_grid(Z, merge_map):\n",
    "    if not merge_map:\n",
    "        return Z\n",
    "    Zm = Z.copy()\n",
    "    for old, new in merge_map.items():\n",
    "        try:\n",
    "            old_i = int(old); new_i = int(new)\n",
    "        except Exception:\n",
    "            continue\n",
    "        mask = (Zm == old_i)\n",
    "        if np.any(mask):\n",
    "            Zm[mask] = new_i\n",
    "    return Zm\n",
    "\n",
    "def compute_pair_adjacency_and_shared_boundaries(lat, lon, Z, region_ids):\n",
    "    nlat, nlon = Z.shape\n",
    "    region_ids = set(region_ids)\n",
    "    valid = np.isin(Z, list(region_ids))\n",
    "\n",
    "    adj = {rid: set() for rid in region_ids}\n",
    "    pair_coords = {}\n",
    "\n",
    "    def _add_pair(j, i, ilat, ilon):\n",
    "        key = (int(j), int(i))\n",
    "        pair_coords.setdefault(key, []).append((int(ilat), int(ilon)))\n",
    "\n",
    "    # Horizontal neighbors\n",
    "    for jj in range(nlon - 1):\n",
    "        a = Z[:, jj]\n",
    "        b = Z[:, jj + 1]\n",
    "        m = (a != b) & valid[:, jj] & valid[:, jj + 1]\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        rows = np.where(m)[0]\n",
    "        for ii in rows:\n",
    "            ra = int(a[ii]); rb = int(b[ii])\n",
    "            if (ra not in region_ids) or (rb not in region_ids):\n",
    "                continue\n",
    "            adj[ra].add(rb); adj[rb].add(ra)\n",
    "            _add_pair(ra, rb, ii, jj + 1)  # i-side boundary points\n",
    "            _add_pair(rb, ra, ii, jj)\n",
    "\n",
    "    # Vertical neighbors\n",
    "    for ii in range(nlat - 1):\n",
    "        a = Z[ii, :]\n",
    "        b = Z[ii + 1, :]\n",
    "        m = (a != b) & valid[ii, :] & valid[ii + 1, :]\n",
    "        if not np.any(m):\n",
    "            continue\n",
    "        cols = np.where(m)[0]\n",
    "        for jj in cols:\n",
    "            ra = int(a[jj]); rb = int(b[jj])\n",
    "            if (ra not in region_ids) or (rb not in region_ids):\n",
    "                continue\n",
    "            adj[ra].add(rb); adj[rb].add(ra)\n",
    "            _add_pair(ra, rb, ii + 1, jj)\n",
    "            _add_pair(rb, ra, ii, jj)\n",
    "\n",
    "    # Wrap-around neighbors (lon=0 with lon=-1)\n",
    "    a = Z[:, 0]\n",
    "    b = Z[:, -1]\n",
    "    m = (a != b) & valid[:, 0] & valid[:, -1]\n",
    "    if np.any(m):\n",
    "        rows = np.where(m)[0]\n",
    "        for ii in rows:\n",
    "            ra = int(a[ii]); rb = int(b[ii])\n",
    "            if (ra not in region_ids) or (rb not in region_ids):\n",
    "                continue\n",
    "            adj[ra].add(rb); adj[rb].add(ra)\n",
    "            _add_pair(ra, rb, ii, nlon - 1)\n",
    "            _add_pair(rb, ra, ii, 0)\n",
    "\n",
    "    lat_rad = np.deg2rad(lat.astype(np.float64))\n",
    "    lon180  = wrap_lon180(lon.astype(np.float64))\n",
    "    lon_rad = np.deg2rad(lon180.astype(np.float64))\n",
    "\n",
    "    pair_bcoords = {}\n",
    "    pair_btrees  = {}\n",
    "\n",
    "    for key, ij_list in pair_coords.items():\n",
    "        if not ij_list:\n",
    "            pair_bcoords[key] = None\n",
    "            pair_btrees[key]  = None\n",
    "            continue\n",
    "        ii = np.fromiter((p[0] for p in ij_list), dtype=np.int64, count=len(ij_list))\n",
    "        jj = np.fromiter((p[1] for p in ij_list), dtype=np.int64, count=len(ij_list))\n",
    "        coords = np.c_[lat_rad[ii], lon_rad[jj]]\n",
    "        pair_bcoords[key] = coords\n",
    "        if HAVE_SKLEARN and coords.shape[0] >= 2:\n",
    "            pair_btrees[key] = BallTree(coords, metric=\"haversine\")\n",
    "        else:\n",
    "            pair_btrees[key] = None\n",
    "\n",
    "    return adj, pair_btrees, pair_bcoords\n",
    "\n",
    "def nearest_index_nonuniform(axis, values):\n",
    "    values = np.asarray(values)\n",
    "    idx = np.searchsorted(axis, values, side=\"left\")\n",
    "    idx0 = np.clip(idx - 1, 0, axis.size - 1)\n",
    "    idx1 = np.clip(idx,     0, axis.size - 1)\n",
    "    choose_right = np.abs(axis[idx1] - values) < np.abs(axis[idx0] - values)\n",
    "    out = np.where(choose_right, idx1, idx0).astype(np.int64)\n",
    "    return out if values.ndim > 0 else int(out)\n",
    "\n",
    "def map_lon_to_grid(lon_vals_deg, grid_lons_deg):\n",
    "    lon_vals_deg = np.asarray(lon_vals_deg)\n",
    "    if (np.nanmin(grid_lons_deg) >= 0.0) and (np.nanmax(grid_lons_deg) <= 360.0):\n",
    "        out = np.mod(lon_vals_deg, 360.0)\n",
    "    else:\n",
    "        out = wrap_lon180(lon_vals_deg)\n",
    "    return out if lon_vals_deg.ndim > 0 else float(out)\n",
    "\n",
    "def _load_mask_indices(mask_nc_path, lat_vals, lon_vals):\n",
    "    if (mask_nc_path is None) or (not os.path.isfile(mask_nc_path)):\n",
    "        raise RuntimeError(f\"Mask file not found: {mask_nc_path}\")\n",
    "    with Dataset(mask_nc_path, \"r\") as ds:\n",
    "        mlat = ds[\"lat\"][:].astype(np.float64)\n",
    "        mlon = ds[\"lon\"][:].astype(np.float64)\n",
    "        mmask = ds[\"remap_mask_los\"][:].astype(bool)\n",
    "\n",
    "    ii = nearest_index_nonuniform(mlat, lat_vals.astype(np.float64))\n",
    "    lon_vals_grid = map_lon_to_grid(lon_vals.astype(np.float64), mlon)\n",
    "    jj = nearest_index_nonuniform(mlon, lon_vals_grid)\n",
    "    allow = mmask[ii, jj]\n",
    "    return allow\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2.0 * EARTH_R_KM * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def remap_smooth_zoneaware_inplace(\n",
    "    df, nc_path,\n",
    "    depth_str,\n",
    "    smooth_km=250.0,\n",
    "    workers=8,\n",
    "    mask_nc_path=None,\n",
    "    merge_map=None,\n",
    "    debug=False,\n",
    "    trace_n=0,\n",
    "    trace_seed=42,\n",
    "    diag_csv=False\n",
    "):\n",
    "    if smooth_km is None or smooth_km <= 0:\n",
    "        df[\"RegionID\"]     = pd.to_numeric(df.get(Zonename, None), errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"Oxygen_remap\"] = pd.to_numeric(df[\"Oxygen\"], errors=\"coerce\")\n",
    "        return\n",
    "\n",
    "    needed = [\"Latitude\", \"Longitude\", \"Oxygen\", Zonename]\n",
    "    for c in needed:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"Remap requires column: {c}\")\n",
    "\n",
    "    df[\"RegionID\"]     = pd.to_numeric(df[Zonename], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"Oxygen_remap\"] = pd.to_numeric(df[\"Oxygen\"], errors=\"coerce\")\n",
    "\n",
    "    lat_g, lon_g, Z = load_biomes(nc_path)\n",
    "    Z = apply_merge_map_to_grid(Z, merge_map)\n",
    "\n",
    "    regions_from_grid = set(\n",
    "        int(v) for v in np.unique(Z[np.isfinite(Z)])\n",
    "        if (v > 0) and (float(int(v)) == float(v))\n",
    "    )\n",
    "    regions_from_data = set(\n",
    "        int(v) for v in df[\"RegionID\"].dropna().unique()\n",
    "        if v > 0\n",
    "    )\n",
    "    region_ids = sorted((regions_from_grid & regions_from_data) or regions_from_data or regions_from_grid)\n",
    "    if not region_ids:\n",
    "        return\n",
    "\n",
    "    df_valid = df[\n",
    "        df[\"RegionID\"].isin(region_ids)\n",
    "        & df[\"Oxygen_remap\"].notna()\n",
    "        & df[\"Latitude\"].notna()\n",
    "        & df[\"Longitude\"].notna()\n",
    "    ].copy()\n",
    "    if df_valid.empty:\n",
    "        return\n",
    "\n",
    "    df_valid[\"Latitude\"]  = df_valid[\"Latitude\"].astype(\"float32\")\n",
    "    df_valid[\"Longitude\"] = wrap_lon180(df_valid[\"Longitude\"].astype(\"float32\").values)\n",
    "    df_valid[\"Oxygen\"]    = df_valid[\"Oxygen_remap\"].astype(\"float32\")\n",
    "    df_valid[\"RegionID\"]  = df_valid[\"RegionID\"].astype(int)\n",
    "\n",
    "    if mask_nc_path is None:\n",
    "        raise RuntimeError(\"mask_nc_path must be provided (LoS mask NetCDF).\")\n",
    "\n",
    "    allow_valid = _load_mask_indices(\n",
    "        mask_nc_path,\n",
    "        df_valid[\"Latitude\"].values,\n",
    "        df_valid[\"Longitude\"].values\n",
    "    )\n",
    "\n",
    "    n_valid = len(df_valid)\n",
    "    n_allow = int(allow_valid.sum())\n",
    "    if debug:\n",
    "        print(f\"[REMAP] Samples inside mask: {n_allow}/{n_valid} ({(n_allow/max(1,n_valid)):.1%})\")\n",
    "    if not np.any(allow_valid):\n",
    "        return\n",
    "\n",
    "    adj, pair_btrees, pair_bcoords = compute_pair_adjacency_and_shared_boundaries(lat_g, lon_g, Z, region_ids)\n",
    "\n",
    "    valid_lat   = df_valid[\"Latitude\"].astype(np.float64).to_numpy()\n",
    "    valid_lon   = df_valid[\"Longitude\"].astype(np.float64).to_numpy()\n",
    "    valid_oxy   = df_valid[\"Oxygen\"].astype(np.float64).to_numpy()\n",
    "    valid_rid   = df_valid[\"RegionID\"].to_numpy()\n",
    "    valid_index = df_valid.index.to_numpy()\n",
    "\n",
    "    zone_meta_cache = {}\n",
    "    def _get_zone_meta(zid: int):\n",
    "        zmeta = zone_meta_cache.get(zid, None)\n",
    "        if zmeta is None:\n",
    "            zmeta = load_zone_cached(depth_str, zid)\n",
    "            zone_meta_cache[zid] = zmeta\n",
    "        return zmeta\n",
    "\n",
    "    contrib_counts = []\n",
    "    wsum_list = []\n",
    "    dist_all = []\n",
    "    w_all = []\n",
    "    n_smoothed = 0\n",
    "\n",
    "    def process_one_region(rid_j: int):\n",
    "        nonlocal n_smoothed\n",
    "\n",
    "        rel = np.where((valid_rid == rid_j) & allow_valid)[0]\n",
    "        if rel.size == 0:\n",
    "            return (np.empty(0, dtype=int), np.empty(0, dtype=float))\n",
    "\n",
    "        idx_global = valid_index[rel]\n",
    "        lat_rad = np.deg2rad(valid_lat[rel])\n",
    "        lon_rad = np.deg2rad(valid_lon[rel])\n",
    "\n",
    "        y_sum = valid_oxy[rel].astype(np.float64).copy()\n",
    "        w_sum = np.ones(rel.size, dtype=np.float64)\n",
    "\n",
    "        Xi = np.c_[lat_rad, lon_rad]\n",
    "        neighs = sorted(adj.get(rid_j, set()))\n",
    "        per_point_contrib_n = np.zeros(rel.size, dtype=int)\n",
    "\n",
    "        for rid_i in neighs:\n",
    "            key = (int(rid_j), int(rid_i))\n",
    "            coords_ji = pair_bcoords.get(key, None)\n",
    "            if coords_ji is None or (isinstance(coords_ji, np.ndarray) and coords_ji.size == 0):\n",
    "                continue\n",
    "\n",
    "            tree_ji = pair_btrees.get(key, None)\n",
    "            if tree_ji is not None:\n",
    "                dd, _ = tree_ji.query(Xi, k=1)\n",
    "                d_km = dd[:, 0] * EARTH_R_KM\n",
    "            else:\n",
    "                latb = coords_ji[:, 0]\n",
    "                lonb = coords_ji[:, 1]\n",
    "                d_km = np.empty(Xi.shape[0], dtype=np.float64)\n",
    "                for s in range(0, Xi.shape[0], 4096):\n",
    "                    e = min(s + 4096, Xi.shape[0])\n",
    "                    d = haversine_km(\n",
    "                        lat_rad[s:e, None], lon_rad[s:e, None],\n",
    "                        latb[None, :],      lonb[None, :]\n",
    "                    )\n",
    "                    d_km[s:e] = d.min(axis=1)\n",
    "\n",
    "            in_band = d_km <= smooth_km\n",
    "            if not in_band.any():\n",
    "                continue\n",
    "\n",
    "            wi = ((smooth_km - d_km[in_band]) / max(smooth_km, 1e-6)) ** 2\n",
    "            rows_sel = idx_global[in_band]\n",
    "\n",
    "            zmeta_i = _get_zone_meta(int(rid_i))\n",
    "            if zmeta_i is None:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                y_hat_i = predict_zone_block(df.loc[rows_sel], zmeta_i)\n",
    "            except Exception as e:\n",
    "                print(f\"    [WARN] Neighbor zone {rid_i} prediction failed (skipped): {e}\")\n",
    "                continue\n",
    "\n",
    "            y_sum[in_band] += wi * y_hat_i\n",
    "            w_sum[in_band] += wi\n",
    "            per_point_contrib_n[in_band] += 1\n",
    "\n",
    "            dist_all.extend(d_km[in_band].tolist())\n",
    "            w_all.extend(wi.tolist())\n",
    "\n",
    "        contrib_counts.extend(per_point_contrib_n.tolist())\n",
    "        n_smoothed += int(np.sum(per_point_contrib_n > 0))\n",
    "        wsum_list.extend(w_sum.tolist())\n",
    "\n",
    "        out = y_sum / np.where(w_sum == 0.0, 1.0, w_sum)\n",
    "        return (idx_global, out)\n",
    "\n",
    "    def _run_parallel():\n",
    "        if workers > 1:\n",
    "            return Parallel(n_jobs=workers, prefer=\"threads\")(\n",
    "                delayed(process_one_region)(rid_j) for rid_j in region_ids\n",
    "            )\n",
    "        else:\n",
    "            return [process_one_region(rid_j) for rid_j in region_ids]\n",
    "\n",
    "    if threadpool_limits is not None:\n",
    "        with threadpool_limits(limits=1):\n",
    "            results = _run_parallel()\n",
    "    else:\n",
    "        results = _run_parallel()\n",
    "\n",
    "    oxy_out = df[\"Oxygen_remap\"].astype(np.float64).values.copy()\n",
    "    for idx_global, vals in results:\n",
    "        if idx_global.size:\n",
    "            oxy_out[idx_global] = vals\n",
    "    df[\"Oxygen_remap\"] = oxy_out\n",
    "\n",
    "    if debug:\n",
    "        if len(contrib_counts) == 0:\n",
    "            print(\"[REMAP] No masked points entered the smoothing band; output equals in-zone predictions.\")\n",
    "        else:\n",
    "            cc = np.array(contrib_counts, int)\n",
    "            ws = np.array(wsum_list, float)\n",
    "            def _q(a, q):\n",
    "                return float(np.percentile(a, q)) if a.size else float('nan')\n",
    "            print(f\"[REMAP] Samples with neighbor contributions: {n_smoothed}/{n_allow} ({(n_smoothed/max(1,n_allow)):.1%})\")\n",
    "            print(f\"[REMAP] Neighbor-count per sample mean/median/p95 = {cc.mean():.2f} / {_q(cc,50):.2f} / {_q(cc,95):.2f}\")\n",
    "            print(f\"[REMAP] w_sum mean/median/p95/min/max = {ws.mean():.3f} / {_q(ws,50):.3f} / {_q(ws,95):.3f} / {ws.min():.3f} / {ws.max():.3f}\")\n",
    "            if dist_all:\n",
    "                da = np.array(dist_all, float); wa = np.array(w_all, float)\n",
    "                print(f\"[REMAP] Neighbor distance d(km) mean/median/p95 = {da.mean():.1f} / {_q(da,50):.1f} / {_q(da,95):.1f}\")\n",
    "                print(f\"[REMAP] Weight w mean/median/p95 = {wa.mean():.4f} / {_q(wa,50):.4f} / {_q(wa,95):.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Writer: chunked output to *_with_pred.csv\n",
    "# =============================================================================\n",
    "def save_new_csv(orig_path,\n",
    "                 df,\n",
    "                 delete_original=False,\n",
    "                 keep_cols=None,\n",
    "                 chunk_size=1_000_000):\n",
    "    out_path = os.path.splitext(orig_path)[0] + \"_with_pred.csv\"\n",
    "\n",
    "    if keep_cols is None or len(keep_cols) == 0:\n",
    "        cols_final = list(df.columns)\n",
    "    else:\n",
    "        missing = [c for c in keep_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            print(f\"  -> [WARN] The following requested output columns are missing and will be ignored: {missing}\")\n",
    "        cols_final = [c for c in keep_cols if c in df.columns]\n",
    "\n",
    "    na_critical_cols = [c for c in cols_final if c not in (\"Oxygen\", \"Oxygen_remap\")]\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        try:\n",
    "            os.remove(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Cannot remove existing target file {out_path}: {e}\")\n",
    "\n",
    "    n_total = len(df)\n",
    "    first_chunk = True\n",
    "\n",
    "    for start in range(0, n_total, chunk_size):\n",
    "        end = min(start + chunk_size, n_total)\n",
    "        chunk = df.iloc[start:end][cols_final].copy()\n",
    "\n",
    "        if na_critical_cols:\n",
    "            before = len(chunk)\n",
    "            chunk = chunk.dropna(subset=na_critical_cols)\n",
    "            after = len(chunk)\n",
    "            if after < before:\n",
    "                print(f\"  -> [INFO] chunk {start}:{end} dropped {before - after} rows containing NaN in required columns\")\n",
    "\n",
    "        for oxy_col in [\"Oxygen\", \"Oxygen_remap\"]:\n",
    "            if oxy_col in chunk.columns:\n",
    "                vals = pd.to_numeric(chunk[oxy_col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "                vals = np.round(vals, 3)\n",
    "                zero_mask = np.isfinite(vals) & (vals == 0.0)\n",
    "                if np.any(zero_mask):\n",
    "                    vals[zero_mask] = 0.001\n",
    "\n",
    "                def _fmt3_arr(v):\n",
    "                    if np.isnan(v):\n",
    "                        return \"\"\n",
    "                    return f\"{float(v):.3f}\"\n",
    "\n",
    "                chunk[oxy_col] = [_fmt3_arr(v) for v in vals]\n",
    "\n",
    "        chunk.to_csv(\n",
    "            out_path,\n",
    "            mode='w' if first_chunk else 'a',\n",
    "            header=first_chunk,\n",
    "            index=False\n",
    "        )\n",
    "        first_chunk = False\n",
    "        del chunk\n",
    "\n",
    "    print(f\"  -> Wrote new file: {out_path}\")\n",
    "\n",
    "    if delete_original:\n",
    "        try:\n",
    "            os.remove(orig_path)\n",
    "            print(f\"  -> Deleted original file: {orig_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Failed to delete original file {orig_path}: {e}\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Fast CSV reader (optional pyarrow)\n",
    "# =============================================================================\n",
    "def read_csv_fast(csv_path):\n",
    "    try:\n",
    "        return pd.read_csv(csv_path, engine=\"pyarrow\")  # pandas>=2.0\n",
    "    except Exception:\n",
    "        return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main pipeline\n",
    "# =============================================================================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Zone-only oxygen prediction (cached models) + optional remap smoothing (LoS-mask limited).\"\n",
    "    )\n",
    "    parser.add_argument(\"--delete\", action=\"store_true\",\n",
    "                        help=\"Delete original CSV after writing the new file.\")\n",
    "    parser.add_argument(\"--depths\", nargs=\"*\",\n",
    "                        help=\"Override DEPTH_LIST in config, e.g. --depths 1 10 50 100\")\n",
    "    parser.add_argument(\"--mask\", type=str, default=REMAP_MASK_NC,\n",
    "                        help=\"LoS mask NetCDF path (must contain variable remap_mask_los).\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    delete_flag = args.delete or DELETE_ORIGINAL\n",
    "    mask_path   = args.mask\n",
    "\n",
    "    depths = list_depths_to_run(INPUT_ROOT, DEPTH_LIST, args.depths)\n",
    "    if not depths:\n",
    "        print(f\"[ERROR] No depth directories found under root: {INPUT_ROOT}\")\n",
    "        return\n",
    "\n",
    "    print(f\"CWD = {os.getcwd()}\")\n",
    "    print(f\"THREADS={THREADS}\")\n",
    "    print(f\"INPUT_ROOT={INPUT_ROOT}\")\n",
    "    print(f\"MODELS_ROOT={MODELS_ROOT}\")\n",
    "    print(f\"Zonename={Zonename}, TempName={TempName}, SalName={SalName}, O2SAT_NAME={O2SAT_NAME}, TIMENAME={TIMENAME}\")\n",
    "    print(\"SEASONS:\", SEASONS)\n",
    "    print(\"Depths:\", depths)\n",
    "\n",
    "    for depth in depths:\n",
    "        merge_map, remap_enabled_depth = adjust_features_and_mergepred(depth)\n",
    "        remap_enabled = bool(REMAP_ENABLED_GLOBAL and remap_enabled_depth)\n",
    "\n",
    "        if remap_enabled:\n",
    "            print(f\"[REMAP] enabled: S={REMAP_SMOOTH_KM} km, workers={REMAP_WORKERS}, mask={mask_path}\")\n",
    "        else:\n",
    "            print(f\"[REMAP] disabled for depth={depth} (policy-driven).\")\n",
    "\n",
    "        ddir = os.path.join(INPUT_ROOT, f\"{depth}dbar\")\n",
    "        print(f\"\\n[Depth {depth}] dir: {ddir}\")\n",
    "\n",
    "        csv_files = []\n",
    "        for season in SEASONS:\n",
    "            candidate = os.path.join(ddir, CSV_NAME_TEMPLATE.format(depth=depth, season=season))\n",
    "            if os.path.isfile(candidate):\n",
    "                csv_files.append(candidate)\n",
    "            else:\n",
    "                print(f\"  [INFO] Missing {os.path.basename(candidate)} ({depth}dbar); skipping this season.\")\n",
    "\n",
    "        if not csv_files:\n",
    "            print(f\"  [INFO] No input CSV found for {depth}dbar; skipping this depth.\")\n",
    "            continue\n",
    "\n",
    "        model_depth_dir = os.path.join(MODELS_ROOT, f\"depth_{depth}\")\n",
    "        if not os.path.isdir(model_depth_dir):\n",
    "            print(f\"  [WARN] Model directory not found: {model_depth_dir}; skipping all files for this depth.\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_files:\n",
    "            try:\n",
    "                df = read_csv_fast(csv_path)\n",
    "                df[\"source_csv\"] = csv_path\n",
    "                df = normalize_temp_sal_columns(df, TempName, SalName)\n",
    "            except Exception as e:\n",
    "                print(f\"  [WARN] Failed to read/preprocess {csv_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Zone merge: write Zone_Merged and make zone_col point to it\n",
    "            if isinstance(merge_map, dict) and len(merge_map) > 0:\n",
    "                if Zonename in df.columns:\n",
    "                    df = apply_zone_merge(df, zone_col=Zonename, merge_map=merge_map, out_col=\"Zone_Merged\")\n",
    "                    zone_col = \"Zone_Merged\"\n",
    "                else:\n",
    "                    zone_col = Zonename\n",
    "            else:\n",
    "                zone_col = Zonename if Zonename in df.columns else Zonename\n",
    "\n",
    "            if zone_col not in df.columns:\n",
    "                print(f\"  [SKIP] {os.path.basename(csv_path)} missing zone column {zone_col}/{Zonename}; cannot predict.\")\n",
    "                continue\n",
    "\n",
    "            # Expose zone column consistently as Zonename (for KEEP_COLS and remap)\n",
    "            if zone_col != Zonename:\n",
    "                df[Zonename] = df[zone_col]\n",
    "\n",
    "            # Minimal required columns\n",
    "            required_min = [\"Latitude\", \"Longitude\", Zonename, O2SAT_NAME]\n",
    "            missing_min = [c for c in required_min if c not in df.columns]\n",
    "            if missing_min:\n",
    "                print(f\"  [SKIP] {os.path.basename(csv_path)} missing minimal required columns: {missing_min}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"  File: {os.path.basename(csv_path)}  n={len(df)}  zone_col={zone_col}\")\n",
    "\n",
    "            # --- Predict per zone ---\n",
    "            y_pred = np.full(len(df), np.nan, dtype=float)\n",
    "            groups = df.groupby(df[zone_col]).groups\n",
    "\n",
    "            for zval, idx in groups.items():\n",
    "                try:\n",
    "                    zid = int(zval)\n",
    "                except Exception:\n",
    "                    print(f\"    [WARN] Zone value {zval} is not an integer; skipping (kept as NaN).\")\n",
    "                    continue\n",
    "\n",
    "                zmeta = load_zone_cached(depth, zid)\n",
    "                if zmeta is None:\n",
    "                    print(f\"    [WARN] Model not found depth_{depth}/zone_{zid}; zone samples kept as NaN.\")\n",
    "                    continue\n",
    "\n",
    "                sub = df.loc[idx]\n",
    "                try:\n",
    "                    y_pred[idx] = predict_zone_block(sub, zmeta)\n",
    "                except Exception as e:\n",
    "                    print(f\"    [WARN] zone {zid} prediction failed (NaN): {e}\")\n",
    "\n",
    "            df[\"Oxygen\"] = np.asarray(y_pred, float)\n",
    "\n",
    "            # Round and avoid 0.000\n",
    "            y_out = np.asarray(df[\"Oxygen\"].to_numpy(), float)\n",
    "            y_out = np.round(y_out, 3)\n",
    "            zero_mask = np.isfinite(y_out) & (y_out == 0.0)\n",
    "            if zero_mask.any():\n",
    "                y_out[zero_mask] = 0.001\n",
    "                print(f\"    [INFO] {int(zero_mask.sum())} values rounded to 0.000; replaced by 0.001.\")\n",
    "            df[\"Oxygen\"] = y_out.astype(\"float32\")\n",
    "\n",
    "            # --- Remap fusion (mask-limited) ---\n",
    "            if remap_enabled:\n",
    "                try:\n",
    "                    remap_smooth_zoneaware_inplace(\n",
    "                        df,\n",
    "                        nc_path=BIOMES_NC,\n",
    "                        depth_str=depth,\n",
    "                        smooth_km=REMAP_SMOOTH_KM,\n",
    "                        workers=REMAP_WORKERS,\n",
    "                        mask_nc_path=mask_path,\n",
    "                        merge_map=merge_map,\n",
    "                        debug=REMAP_DEBUG,\n",
    "                        trace_n=REMAP_TRACE_N,\n",
    "                        trace_seed=REMAP_TRACE_SEED,\n",
    "                        diag_csv=REMAP_DIAG_CSV\n",
    "                    )\n",
    "\n",
    "                    if \"Oxygen_remap\" in df.columns:\n",
    "                        yr = np.asarray(df[\"Oxygen_remap\"].to_numpy(), float)\n",
    "                        yr = np.round(yr, 3)\n",
    "                        zrm = np.isfinite(yr) & (yr == 0.0)\n",
    "                        if zrm.any():\n",
    "                            yr[zrm] = 0.001\n",
    "                            print(f\"    [INFO] After remap, {int(zrm.sum())} values are 0.000; replaced by 0.001.\")\n",
    "                        df[\"Oxygen_remap\"] = yr.astype(\"float32\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARN] Remap fusion failed: {e} (skipping remap; keeping Oxygen).\")\n",
    "\n",
    "            # Output columns\n",
    "            keep_cols_out = KEEP_COLS.copy()\n",
    "            if remap_enabled:\n",
    "                for c in [\"RegionID\", \"Oxygen_remap\"]:\n",
    "                    if c not in keep_cols_out:\n",
    "                        keep_cols_out.append(c)\n",
    "\n",
    "            save_new_csv(\n",
    "                csv_path,\n",
    "                df,\n",
    "                delete_original=delete_flag,\n",
    "                keep_cols=keep_cols_out,\n",
    "                chunk_size=1_000_000\n",
    "            )\n",
    "\n",
    "    print(\"\\nAll done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "make_monthly_do_netcdf_cmemsgrid_mask_smooth_fill_floor_NOOBS.py\n",
    "\n",
    "Fixed pipeline (NO observation residual blending):\n",
    "- Output grid axes are strictly identical to the CMEMS TEMP grid (latitude/longitude read from TEMP_NC).\n",
    "- For each (year, month, depth):\n",
    "    1) Bin scattered predictions onto the target grid;\n",
    "    2) Build the Mallow mask (TEMP-valid & latitude cut & optional shallow coastal exclusion);\n",
    "    3) Keep the background prediction field only where (Mallow & prediction coverage);\n",
    "    4) Apply Gaussian smoothing on the masked field (preserve footprint; do not expand into NaNs);\n",
    "    5) Apply a minimum-value clamp (floor) to all valid values;\n",
    "    6) Write into the corresponding monthly NetCDF file (time/depth unlimited; depth inserted/overwritten in ascending order).\n",
    "\n",
    "- Variable OXY is stored as int16 with scale_factor/add_offset packing (handled by netCDF4).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "from datetime import datetime, timezone\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ================= Optional: BallTree (accelerate haversine kNN) =================\n",
    "try:\n",
    "    from sklearn.neighbors import BallTree\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    BallTree = None\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "# ===================== Default paths (override via CLI) =====================\n",
    "PRED_ROOT = \"/data/wang/Result_Data/allnodoxy\"\n",
    "OUT_DIR   = \"/data/wang/Result_Data/models_ML/Datasets\"\n",
    "\n",
    "# NOTE: depthlist must exist in your runtime environment (as in your original setup).\n",
    "DEPTH_LIST_DEFAULT = depthlist\n",
    "\n",
    "SEASONS_DEFAULT = [\"Spring\", \"Summer\", \"Autumn\", \"Winter\", \"NewYear\"]\n",
    "SEASON_MONTHS = {\n",
    "    \"Spring\": {3, 4, 5},\n",
    "    \"Summer\": {6, 7, 8},\n",
    "    \"Autumn\": {9, 10, 11},\n",
    "    \"Winter\": {12, 1, 2},\n",
    "    \"NewYear\": {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12},\n",
    "}\n",
    "\n",
    "# ===================== Mask / smoothing / interpolation parameters =====================\n",
    "TEMP_NC_DEFAULT                  = \"/data/wang/CMEMS/TEMP/OA_CORA5.2_19600115_fld_TEMP.nc\"\n",
    "MASK_NC_DEFAULT                  = \"/data/wang/Mask_File/land_mask_0m.nc\"    # 1=land/shallow; 0=other (lon:0..360)\n",
    "SHALLOW_MASK_APPLY_MAX_M_DEFAULT = 0.0\n",
    "ARCTIC_CUT_LAT_DEFAULT           = -77.0105\n",
    "LL_TOL_DEG_DEFAULT               = 1e-3\n",
    "DEPTH_TOL_DEFAULT                = 1e-3\n",
    "\n",
    "# Smoothing: fixed Gaussian on\n",
    "GAUSS_RADIUS_KM_DEFAULT    = 50.0\n",
    "MIN_SUPPORT_FRAC_DEFAULT   = 0.5\n",
    "\n",
    "FILL_KERNEL_DEFAULT        = \"gauss\"          # 'idw' | 'gauss'\n",
    "FILL_KMAX_DEFAULT          = 8\n",
    "BASE_RADIUS_KM_DEFAULT     = 60.0\n",
    "MAX_RADIUS_KM_DEFAULT      = 120.0\n",
    "FILL_POWER_DEFAULT         = 2\n",
    "\n",
    "# Minimum-value clamp\n",
    "MIN_VALUE_FLOOR_DEFAULT    = 0.001\n",
    "\n",
    "# ================= Metadata (CF-1.8 + ACDD-1.3) =================\n",
    "TITLE        = \"GEOXYGEN v1.1\"\n",
    "KEYWORDS     = \"Ocean dissolved oxygen, Machine learning, 0.5deg, Long time series\"\n",
    "INSTITUTION  = \"Fudan University\"\n",
    "CREATOR_NAME = \"Wang et al.\"\n",
    "PROJECT      = \"24ZR1404500\"\n",
    "ANALYSIS     = \"DO_\"\n",
    "SOURCE = (\n",
    "    \"Multi-source in situ dissolved oxygen observations (CCHDO, GLODAP, GEOTRACES IDP2021, \"\n",
    "    \"OceanSITES, and OSD/CTD + Argo internally consistent calibrated product) and machine-learning reconstruction\"\n",
    ")\n",
    "LICENSE      = \"Creative Commons Attribution 4.0 International (CC BY 4.0)\"\n",
    "NAMING_AUTH  = \"cn.edu.fudan\"\n",
    "STANDARD_NAME_VOC = \"CF Standard Name Table\"\n",
    "REFS         = \"Documentation forthcoming\"\n",
    "REF_DATE     = \"1950-01-01T00:00:00Z\"\n",
    "CONVENTIONS  = \"CF-1.8, ACDD-1.3\"\n",
    "\n",
    "# ================= Constants =================\n",
    "DEPTH_MATCH_TOL = 1e-4\n",
    "EARTH_R_KM = 6371.0\n",
    "\n",
    "# ================= OXY packing parameters (CORA-like) =================\n",
    "OXY_SCALE_FACTOR = 0.02           # 1 LSB = 0.02 µmol/kg\n",
    "OXY_ADD_OFFSET   = 0.0\n",
    "OXY_VALID_MIN    = 0\n",
    "OXY_VALID_MAX    = 20000          # -> 400 µmol/kg\n",
    "OXY_FILL_VALUE   = np.int16(32767)\n",
    "\n",
    "# ================= Utility functions =================\n",
    "def normalize_lon_to_m180_180(lon_arr: np.ndarray) -> np.ndarray:\n",
    "    return (lon_arr + 180.0) % 360.0 - 180.0\n",
    "\n",
    "def lon_to_0_360(lon_deg):\n",
    "    x = np.asarray(lon_deg, np.float64)\n",
    "    out = np.mod(x, 360.0)\n",
    "    out[out < 0] += 360.0\n",
    "    return out\n",
    "\n",
    "def days_since_ref(dt_utc: datetime, ref=REF_DATE) -> float:\n",
    "    ref_dt = datetime.fromisoformat(ref.replace(\"Z\", \"+00:00\"))\n",
    "    return (dt_utc - ref_dt).total_seconds() / 86400.0\n",
    "\n",
    "def mid_month_dt(y: int, m: int) -> datetime:\n",
    "    return datetime(int(y), int(m), 15, tzinfo=timezone.utc)\n",
    "\n",
    "def nearest_index_nonuniform(axis: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
    "    idx = np.searchsorted(axis, values, side=\"left\")\n",
    "    idx0 = np.clip(idx - 1, 0, axis.size - 1)\n",
    "    idx1 = np.clip(idx,     0, axis.size - 1)\n",
    "    choose_right = np.abs(axis[idx1] - values) < np.abs(axis[idx0] - values)\n",
    "    return np.where(choose_right, idx1, idx0).astype(np.int64)\n",
    "\n",
    "def bincount_mean(i: np.ndarray, j: np.ndarray, val: np.ndarray, ny: int, nx: int):\n",
    "    flat = i * nx + j\n",
    "    m = np.isfinite(val)\n",
    "    CNT = np.bincount(flat[m], minlength=ny*nx)\n",
    "    if not np.any(m):\n",
    "        return np.full((ny, nx), np.nan, dtype=np.float32), CNT.reshape(ny, nx).astype(np.int32)\n",
    "    SUM = np.bincount(flat[m], weights=val[m].astype(np.float64), minlength=ny*nx)\n",
    "    SUM = SUM.reshape(ny, nx)\n",
    "    CNT = CNT.reshape(ny, nx)\n",
    "    with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "        MEAN = SUM / np.where(CNT == 0, 1, CNT)\n",
    "    MEAN[CNT == 0] = np.nan\n",
    "    return MEAN.astype(np.float32), CNT.astype(np.int32)\n",
    "\n",
    "# ================= Gaussian smoothing (fix 0*NaN contamination) =================\n",
    "def _gaussian_kernel1d(sigma_gp, truncate=3.0):\n",
    "    sigma = float(max(1e-6, sigma_gp))\n",
    "    half = int(max(1, np.ceil(truncate * sigma)))\n",
    "    x = np.arange(-half, half+1, dtype=np.float64)\n",
    "    k = np.exp(-0.5 * (x / sigma) ** 2)\n",
    "    k /= k.sum() + 1e-12\n",
    "    return k\n",
    "\n",
    "def _km_to_grid_sigma(radius_km: float, ddeg: float):\n",
    "    if ddeg == 0:\n",
    "        return 1.0\n",
    "    return max(0.5, (radius_km / 111.0) / abs(ddeg))\n",
    "\n",
    "def _reflect_indices(idx, N):\n",
    "    idx = idx.copy()\n",
    "    idx[idx < 0] = -idx[idx < 0] - 1\n",
    "    idx[idx >= N] = 2 * N - idx[idx >= N] - 1\n",
    "    return np.clip(idx, 0, N-1)\n",
    "\n",
    "def _norm_conv_line_wrap(x, mask01, kernel):\n",
    "    acc = np.zeros_like(x, float)\n",
    "    wsum = np.zeros_like(x, float)\n",
    "    m = kernel.size // 2\n",
    "    for r, w in enumerate(kernel):\n",
    "        shift = r - m\n",
    "        xs = np.roll(x, shift)\n",
    "        ms = np.roll(mask01, shift)\n",
    "        xs = np.where(ms > 0, xs, 0.0)\n",
    "        acc += w * xs\n",
    "        wsum += w * ms\n",
    "    out = np.where(wsum > 0, acc / np.maximum(wsum, 1e-12), np.nan)\n",
    "    return out, wsum\n",
    "\n",
    "def _norm_conv_line_reflect(x, mask01, kernel):\n",
    "    acc = np.zeros_like(x, float)\n",
    "    wsum = np.zeros_like(x, float)\n",
    "    N = x.shape[0]\n",
    "    m = kernel.size // 2\n",
    "    base = np.arange(N, dtype=np.int64)\n",
    "    for r, w in enumerate(kernel):\n",
    "        shift = r - m\n",
    "        idx = _reflect_indices(base + shift, N)\n",
    "        xs = x[idx]\n",
    "        ms = mask01[idx]\n",
    "        xs = np.where(ms > 0, xs, 0.0)\n",
    "        acc += w * xs\n",
    "        wsum += w * ms\n",
    "    out = np.where(wsum > 0, acc / np.maximum(wsum, 1e-12), np.nan)\n",
    "    return out, wsum\n",
    "\n",
    "def gaussian_smooth_preserve(V: np.ndarray, lats: np.ndarray, lons: np.ndarray,\n",
    "                             radius_km: float, min_support_frac: float = 0.5) -> np.ndarray:\n",
    "    V = V.astype(np.float64)\n",
    "    Ny, Nx = V.shape\n",
    "    mask0 = np.isfinite(V).astype(np.float64)\n",
    "    out = V.copy()\n",
    "\n",
    "    dlon = float(np.nanmedian(np.diff(lons))) if Nx > 1 else 1.0\n",
    "    for i in range(Ny):\n",
    "        cosphi = max(0.1, abs(np.cos(np.deg2rad(float(lats[i])))))\n",
    "        sigma_lon = _km_to_grid_sigma(radius_km / max(cosphi, 1e-6), dlon)\n",
    "        k_lon = _gaussian_kernel1d(sigma_lon, 3.0)\n",
    "        sm, ws = _norm_conv_line_wrap(out[i, :], mask0[i, :], k_lon)\n",
    "        good = ws >= (min_support_frac * k_lon.sum())\n",
    "        sm = np.where((mask0[i, :] > 0) & (~good), out[i, :], sm)\n",
    "        sm = np.where((mask0[i, :] > 0) & np.isnan(sm), out[i, :], sm)\n",
    "        sm = np.where(mask0[i, :] > 0, sm, np.nan)\n",
    "        out[i, :] = sm\n",
    "\n",
    "    mask1 = np.isfinite(out).astype(np.float64)\n",
    "    dlat = float(np.nanmedian(np.diff(lats))) if Ny > 1 else 1.0\n",
    "    sigma_lat = _km_to_grid_sigma(radius_km, dlat)\n",
    "    k_lat = _gaussian_kernel1d(sigma_lat, 3.0)\n",
    "    final = out.copy()\n",
    "    for j in range(Nx):\n",
    "        sm, ws = _norm_conv_line_reflect(out[:, j], mask1[:, j], k_lat)\n",
    "        good = ws >= (min_support_frac * k_lat.sum())\n",
    "        sm = np.where((mask0[:, j] > 0) & (~good), out[:, j], sm)\n",
    "        sm = np.where((mask0[:, j] > 0) & np.isnan(sm), out[:, j], sm)\n",
    "        sm = np.where(mask0[:, j] > 0, sm, np.nan)\n",
    "        final[:, j] = sm\n",
    "\n",
    "    final = np.where(mask0 > 0, final, np.nan)\n",
    "    return final.astype(np.float32)\n",
    "\n",
    "# ================= TEMP / offshore mask cache and mapping =================\n",
    "_TEMP_CACHE = {\"path\": None, \"lats\": None, \"lons\": None, \"deps\": None, \"mask3d\": None}\n",
    "_MASK_CACHE = {\"path\": None, \"lat\": None, \"lon\": None, \"mask2d\": None,\n",
    "               \"lat_sorted\": None, \"lat_inv\": None, \"lon_sorted\": None, \"lon_inv\": None}\n",
    "\n",
    "def _as_sorted_axis_and_index(axis):\n",
    "    axis = np.asarray(axis, np.float64)\n",
    "    order = np.argsort(axis)\n",
    "    inv = np.empty_like(order)\n",
    "    inv[order] = np.arange(order.size)\n",
    "    return axis[order], order, inv\n",
    "\n",
    "def nearest_index_on_sorted_axis(sorted_axis, values):\n",
    "    v = np.asarray(values, np.float64)\n",
    "    idx = np.searchsorted(sorted_axis, v, side=\"left\")\n",
    "    idx0 = np.clip(idx-1, 0, sorted_axis.size-1)\n",
    "    idx1 = np.clip(idx,   0, sorted_axis.size-1)\n",
    "    choose_right = np.abs(sorted_axis[idx1] - v) < np.abs(sorted_axis[idx0] - v)\n",
    "    return np.where(choose_right, idx1, idx0).astype(np.int64)\n",
    "\n",
    "def match_axis_indices_subset(axis_full, axis_sub, tol_deg):\n",
    "    full_sorted, order_full, inv_full = _as_sorted_axis_and_index(axis_full)\n",
    "    sub_sorted,  order_sub,  inv_sub  = _as_sorted_axis_and_index(axis_sub)\n",
    "    idx_sorted = nearest_index_on_sorted_axis(full_sorted, sub_sorted)\n",
    "    diff = np.abs(full_sorted[idx_sorted] - sub_sorted)\n",
    "    if diff.size > 0 and np.nanmax(diff) > float(tol_deg):\n",
    "        raise RuntimeError(f\"Axis matching failed: max |Δ|={np.nanmax(diff):.6f} > tol={tol_deg}\")\n",
    "    idx_full_orig = inv_full[idx_sorted]\n",
    "    idx_final = idx_full_orig[order_sub.argsort()]\n",
    "    return idx_final.astype(np.int64)\n",
    "\n",
    "def load_temp_cache(temp_nc_path):\n",
    "    global _TEMP_CACHE\n",
    "    if _TEMP_CACHE[\"path\"] == temp_nc_path:\n",
    "        return\n",
    "    with Dataset(temp_nc_path, \"r\") as nc:\n",
    "        try:\n",
    "            nc.set_auto_maskandscale(True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        lats = nc[\"latitude\"][:].astype(np.float64)\n",
    "        lons = nc[\"longitude\"][:].astype(np.float64)\n",
    "        deps = nc[\"depth\"][:].astype(np.float64)\n",
    "        T0 = nc[\"TEMP\"][0, :, :, :]   # (depth, lat, lon) masked\n",
    "        mask3d = np.isfinite(np.array(T0.filled(np.nan), dtype=np.float32))\n",
    "    _TEMP_CACHE = {\"path\": temp_nc_path, \"lats\": lats, \"lons\": lons,\n",
    "                   \"deps\": deps, \"mask3d\": mask3d}\n",
    "\n",
    "def depth_index_match(temp_deps, depth_m, tol=DEPTH_TOL_DEFAULT):\n",
    "    idx = int(np.argmin(np.abs(temp_deps - float(depth_m))))\n",
    "    if abs(float(temp_deps[idx]) - float(depth_m)) > float(tol):\n",
    "        raise RuntimeError(f\"No matching depth: {depth_m} m (nearest={temp_deps[idx]} m)\")\n",
    "    return idx\n",
    "\n",
    "def load_offshore_mask(mask_nc_path):\n",
    "    global _MASK_CACHE\n",
    "    if _MASK_CACHE[\"path\"] == mask_nc_path:\n",
    "        return\n",
    "    with Dataset(mask_nc_path, \"r\") as nc:\n",
    "        lat = nc[\"lat\"][:].astype(np.float64)\n",
    "        lon = nc[\"lon\"][:].astype(np.float64)\n",
    "        m   = nc[\"offshore_mask\"][:].astype(np.uint8)  # 1=land/shallow; 0=other\n",
    "    lat_sorted, _, lat_inv = _as_sorted_axis_and_index(lat)\n",
    "    lon_sorted, _, lon_inv = _as_sorted_axis_and_index(lon)\n",
    "    _MASK_CACHE = {\"path\": mask_nc_path, \"lat\": lat, \"lon\": lon, \"mask2d\": m,\n",
    "                   \"lat_sorted\": lat_sorted, \"lat_inv\": lat_inv,\n",
    "                   \"lon_sorted\": lon_sorted, \"lon_inv\": lon_inv}\n",
    "\n",
    "def map_offshore_mask_to_grid(mask_nc_path, do_lats, do_lons):\n",
    "    load_offshore_mask(mask_nc_path)\n",
    "    lat_sorted = _MASK_CACHE[\"lat_sorted\"]\n",
    "    lon_sorted = _MASK_CACHE[\"lon_sorted\"]\n",
    "    lat_inv    = _MASK_CACHE[\"lat_inv\"]\n",
    "    lon_inv    = _MASK_CACHE[\"lon_inv\"]\n",
    "    M2         = _MASK_CACHE[\"mask2d\"]\n",
    "    idx_lat_sorted = nearest_index_on_sorted_axis(lat_sorted, do_lats)\n",
    "    idx_lon_sorted = nearest_index_on_sorted_axis(lon_sorted, lon_to_0_360(do_lons))\n",
    "    lat_idx = lat_inv[idx_lat_sorted]\n",
    "    lon_idx = lon_inv[idx_lon_sorted]\n",
    "    mapped = M2[np.ix_(lat_idx, lon_idx)].astype(np.uint8)\n",
    "    return mapped\n",
    "\n",
    "def build_pred_mask_for_grid(depth_m: float, target_lats: np.ndarray, target_lons: np.ndarray,\n",
    "                             temp_nc_path: str,\n",
    "                             mask_nc_path: str,\n",
    "                             shallow_mask_max_m: float,\n",
    "                             arctic_cut_lat: float,\n",
    "                             ll_tol_deg: float,\n",
    "                             depth_tol: float) -> np.ndarray:\n",
    "    load_temp_cache(temp_nc_path)\n",
    "    tlats = _TEMP_CACHE[\"lats\"]; tlons = _TEMP_CACHE[\"lons\"]\n",
    "    tdeps = _TEMP_CACHE[\"deps\"]; tmask3d = _TEMP_CACHE[\"mask3d\"]\n",
    "    lat_idx_temp = match_axis_indices_subset(tlats, target_lats, tol_deg=ll_tol_deg)\n",
    "    lon_idx_temp = match_axis_indices_subset(tlons, target_lons, tol_deg=ll_tol_deg)\n",
    "    tz = depth_index_match(tdeps, float(depth_m), tol=depth_tol)\n",
    "    Mtemp_do = tmask3d[tz, :, :][np.ix_(lat_idx_temp, lon_idx_temp)]\n",
    "    Mallow = Mtemp_do.copy()\n",
    "\n",
    "    lat_ok = (target_lats < float(arctic_cut_lat))\n",
    "    if np.any(lat_ok):\n",
    "        Mallow[lat_ok, :] = False\n",
    "\n",
    "    if float(depth_m) <= float(shallow_mask_max_m):\n",
    "        offshore_mask = map_offshore_mask_to_grid(mask_nc_path, target_lats, target_lons)\n",
    "        Mallow = Mallow & (offshore_mask == 0)\n",
    "\n",
    "    return Mallow.astype(bool)\n",
    "\n",
    "# ================= NetCDF: create / write (time/depth unlimited) =================\n",
    "def ensure_month_file(path_nc: str, lats: np.ndarray, lons: np.ndarray, year: int, month: int):\n",
    "    if not os.path.exists(path_nc):\n",
    "        os.makedirs(os.path.dirname(path_nc), exist_ok=True)\n",
    "        ds = Dataset(path_nc, \"w\", format=\"NETCDF4\")\n",
    "        Ny, Nx = len(lats), len(lons)\n",
    "        ds.createDimension(\"time\", None)\n",
    "        ds.createDimension(\"depth\", None)\n",
    "        ds.createDimension(\"latitude\",  Ny)\n",
    "        ds.createDimension(\"longitude\", Nx)\n",
    "\n",
    "        vtime  = ds.createVariable(\"time\", \"f4\", (\"time\",))\n",
    "        vdepth = ds.createVariable(\"depth\", \"f4\", (\"depth\",))\n",
    "        vlat   = ds.createVariable(\"latitude\",  \"f4\", (\"latitude\",))\n",
    "        vlon   = ds.createVariable(\"longitude\", \"f4\", (\"longitude\",))\n",
    "        vlat[:] = lats.astype(np.float32)\n",
    "        vlon[:] = lons.astype(np.float32)\n",
    "\n",
    "        vtime.standard_name = \"time\"\n",
    "        vtime.units = f\"days since {REF_DATE}\"\n",
    "        vtime.calendar = \"gregorian\"\n",
    "        vtime.axis = \"T\"\n",
    "\n",
    "        vdepth.standard_name = \"depth\"\n",
    "        vdepth.units = \"m\"\n",
    "        vdepth.positive = \"down\"\n",
    "        vdepth.axis = \"Z\"\n",
    "\n",
    "        vlat.standard_name = \"latitude\"\n",
    "        vlat.units = \"degree_north\"\n",
    "        vlat.axis = \"Y\"\n",
    "\n",
    "        vlon.standard_name = \"longitude\"\n",
    "        vlon.units = \"degree_east\"\n",
    "        vlon.axis = \"X\"\n",
    "\n",
    "        chunks_lat = min(200, Ny if Ny > 0 else 1)\n",
    "        chunks_lon = min(200, Nx if Nx > 0 else 1)\n",
    "\n",
    "        voxy = ds.createVariable(\n",
    "            \"OXY\", \"i2\", (\"time\", \"depth\", \"latitude\", \"longitude\"),\n",
    "            zlib=True, complevel=4, shuffle=True, fill_value=OXY_FILL_VALUE,\n",
    "            chunksizes=(1, 8, max(1, chunks_lat), max(1, chunks_lon))\n",
    "        )\n",
    "        voxy.long_name     = \"Dissolved Oxygen\"\n",
    "        voxy.standard_name = \"mole_concentration_of_dissolved_molecular_oxygen_in_sea_water\"\n",
    "        voxy.units         = \"umol kg-1\"\n",
    "        voxy.scale_factor  = np.float32(OXY_SCALE_FACTOR)\n",
    "        voxy.add_offset    = np.float32(OXY_ADD_OFFSET)\n",
    "        voxy.valid_min     = np.int16(OXY_VALID_MIN)\n",
    "        voxy.valid_max     = np.int16(OXY_VALID_MAX)\n",
    "        voxy.missing_value = OXY_FILL_VALUE\n",
    "\n",
    "        vtime[0:1] = np.float32(days_since_ref(mid_month_dt(year, month)))\n",
    "\n",
    "        ds.Conventions              = CONVENTIONS\n",
    "        ds.title                    = TITLE\n",
    "        ds.keywords                 = KEYWORDS\n",
    "        ds.institution              = INSTITUTION\n",
    "        ds.creator_name             = CREATOR_NAME\n",
    "        ds.project                  = PROJECT\n",
    "        ds.project_name             = PROJECT\n",
    "        ds.analysis_name            = ANALYSIS\n",
    "        ds.source                   = SOURCE\n",
    "        ds.license                  = LICENSE\n",
    "        ds.naming_authority         = NAMING_AUTH\n",
    "        ds.standard_name_vocabulary = STANDARD_NAME_VOC\n",
    "        ds.references               = REFS\n",
    "        ds.history                  = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\") + \" : Creation\"\n",
    "        ds.id                       = f\"DO_{year:04d}{month:02d}\"\n",
    "        ds.dataset_id               = f\"DO_{year:04d}{month:02d}\"\n",
    "        ds.cdm_data_type            = \"Grid\"\n",
    "        ds.featureType              = \"grid\"\n",
    "        ds.time_coverage_start      = f\"{year:04d}-{month:02d}-15T00:00:00Z\"\n",
    "        ds.time_coverage_end        = f\"{year:04d}-{month:02d}-15T00:00:00Z\"\n",
    "        ds.geospatial_lat_min       = float(np.nanmin(lats))\n",
    "        ds.geospatial_lat_max       = float(np.nanmax(lats))\n",
    "        ds.geospatial_lon_min       = float(np.nanmin(lons))\n",
    "        ds.geospatial_lon_max       = float(np.nanmax(lons))\n",
    "        if len(lats) > 1:\n",
    "            ds.geospatial_lat_resolution = float(np.nanmedian(np.abs(np.diff(lats))))\n",
    "        if len(lons) > 1:\n",
    "            ds.geospatial_lon_resolution = float(np.nanmedian(np.abs(np.diff(lons))))\n",
    "        ds.comment = (\n",
    "            \"Monthly mean DO; CMEMS grid; \"\n",
    "            \"mask(Mallow)→smooth(finite)→fill(Mallow NaNs)→floor(min)→write.\"\n",
    "        )\n",
    "\n",
    "        ds.close()\n",
    "        return lats.astype(np.float32), lons.astype(np.float32)\n",
    "\n",
    "    with Dataset(path_nc, \"r\") as ds:\n",
    "        return ds[\"latitude\"][:].astype(np.float32), ds[\"longitude\"][:].astype(np.float32)\n",
    "\n",
    "def insert_or_overwrite_depth_sorted(ds: Dataset, depth_m: float, data2d: np.ndarray):\n",
    "    vdep = ds[\"depth\"]\n",
    "    voxy = ds[\"OXY\"]\n",
    "    Ny = ds.dimensions[\"latitude\"].size\n",
    "    Nx = ds.dimensions[\"longitude\"].size\n",
    "\n",
    "    if data2d.shape != (Ny, Nx):\n",
    "        raise RuntimeError(\"data2d does not match the file grid shape.\")\n",
    "\n",
    "    data2d_ma = np.ma.masked_invalid(np.array(data2d, dtype=np.float32, copy=False))\n",
    "\n",
    "    if vdep.size == 0:\n",
    "        vdep[0:1] = float(depth_m)\n",
    "        voxy[0:1, 0:1, :, :] = data2d_ma[np.newaxis, np.newaxis, :, :]\n",
    "        return\n",
    "\n",
    "    depths = vdep[:].astype(float)\n",
    "    hit = np.where(np.abs(depths - float(depth_m)) < DEPTH_MATCH_TOL)[0]\n",
    "    if hit.size > 0:\n",
    "        idx = int(hit[0])\n",
    "        voxy[0:1, idx:idx+1, :, :] = data2d_ma[np.newaxis, np.newaxis, :, :]\n",
    "        return\n",
    "\n",
    "    idx = int(np.searchsorted(depths, float(depth_m), side=\"left\"))\n",
    "    N = vdep.size\n",
    "    if idx >= N:\n",
    "        vdep[N:N+1] = float(depth_m)\n",
    "        voxy[0:1, N:N+1, :, :] = data2d_ma[np.newaxis, np.newaxis, :, :]\n",
    "        return\n",
    "\n",
    "    vdep[N:N+1] = depths[-1]\n",
    "    for d in range(N-1, idx-1, -1):\n",
    "        vdep[d+1] = vdep[d]\n",
    "        slab = voxy[0, d, :, :].copy()\n",
    "        voxy[0, d+1, :, :] = slab\n",
    "    vdep[idx] = float(depth_m)\n",
    "    voxy[0:1, idx:idx+1, :, :] = data2d_ma[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "def write_or_append_depth_sorted(path_nc, year, month, depth_m, lats_target, lons_target, field2d_target):\n",
    "    ds = Dataset(path_nc, \"r+\")\n",
    "    try:\n",
    "        vtime = ds[\"time\"]\n",
    "        intended = days_since_ref(mid_month_dt(year, month))\n",
    "        if vtime.size == 0:\n",
    "            vtime[0:1] = np.float32(intended)\n",
    "        else:\n",
    "            if abs(float(vtime[0]) - float(intended)) > 1e-3:\n",
    "                raise RuntimeError(f\"{path_nc}: time[0] does not match {year}-{month:02d}.\")\n",
    "\n",
    "        lat_nc = ds[\"latitude\"][:]\n",
    "        lon_nc = ds[\"longitude\"][:]\n",
    "        if lat_nc.shape[0] != lats_target.shape[0] or lon_nc.shape[0] != lons_target.shape[0]:\n",
    "            raise RuntimeError(\"Field axes do not match the file axes (CMEMS grid must not change).\")\n",
    "\n",
    "        insert_or_overwrite_depth_sorted(ds, float(depth_m), np.array(field2d_target, dtype=np.float32, copy=True))\n",
    "\n",
    "        valid = np.isfinite(field2d_target)\n",
    "        if valid.any():\n",
    "            mn = float(np.nanmin(field2d_target))\n",
    "            mx = float(np.nanmax(field2d_target))\n",
    "            ds.setncattr(\"last_write_stats\", f\"{year}-{month:02d} depth={depth_m:g}m min={mn:.3f} max={mx:.3f}\")\n",
    "        else:\n",
    "            ds.setncattr(\"last_write_stats\", f\"{year}-{month:02d} depth={depth_m:g}m ALL_NaN\")\n",
    "    finally:\n",
    "        ds.close()\n",
    "\n",
    "# ================= CSV reader (predictions) =================\n",
    "def read_pred_csv(csv_path: str):\n",
    "    if not os.path.isfile(csv_path):\n",
    "        return [], None, None\n",
    "    header = pd.read_csv(csv_path, nrows=0)\n",
    "    cols = set(header.columns)\n",
    "    oxy_col = \"Oxygen_remap\" if \"Oxygen_remap\" in cols else (\"Oxygen\" if \"Oxygen\" in cols else None)\n",
    "    if oxy_col is None or \"Latitude\" not in cols or \"Longitude\" not in cols or \"Year\" not in cols or \"Month\" not in cols:\n",
    "        print(f\"[SKIP-PRED] {os.path.basename(csv_path)} is missing required columns.\")\n",
    "        return [], None, None\n",
    "    usecols = [\"Year\", \"Month\", \"Latitude\", \"Longitude\", oxy_col]\n",
    "    df = pd.read_csv(csv_path, usecols=usecols)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=usecols)\n",
    "    if oxy_col != \"Oxygen\":\n",
    "        df = df.rename(columns={oxy_col: \"Oxygen\"})\n",
    "    df[\"Longitude\"] = normalize_lon_to_m180_180(df[\"Longitude\"].astype(float).values)\n",
    "    df = df[(df[\"Latitude\"].between(-90, 90, inclusive=\"both\"))]\n",
    "    df[\"Year\"]  = df[\"Year\"].astype(int)\n",
    "    df[\"Month\"] = df[\"Month\"].astype(int)\n",
    "    months = sorted(set(zip(df[\"Year\"].tolist(), df[\"Month\"].tolist())))\n",
    "    return months, df, oxy_col\n",
    "\n",
    "# ================= Neighbor interpolation =================\n",
    "def _deg2rad(x):\n",
    "    return np.deg2rad(np.asarray(x, np.float64))\n",
    "\n",
    "def points_idw_or_gauss(src_lat, src_lon, src_val, tgt_lat, tgt_lon,\n",
    "                        kmax=12, radius_km=None, kernel=\"idw\", power=2):\n",
    "    src_lat = np.asarray(src_lat, np.float64)\n",
    "    src_lon = np.asarray(src_lon, np.float64)\n",
    "    src_val = np.asarray(src_val, np.float64)\n",
    "    tgt_lat = np.asarray(tgt_lat, np.float64)\n",
    "    tgt_lon = np.asarray(tgt_lon, np.float64)\n",
    "\n",
    "    m = np.isfinite(src_val)\n",
    "    if not np.any(m) or tgt_lat.size == 0:\n",
    "        return np.full(tgt_lat.shape, np.nan, dtype=np.float32)\n",
    "    src_lat = src_lat[m]; src_lon = src_lon[m]; src_val = src_val[m]\n",
    "\n",
    "    P = np.c_[_deg2rad(src_lat), _deg2rad(src_lon)]\n",
    "    X = np.c_[_deg2rad(tgt_lat), _deg2rad(tgt_lon)]\n",
    "    k = max(1, min(int(kmax), P.shape[0]))\n",
    "\n",
    "    if HAVE_SKLEARN and (P.shape[0] >= 2):\n",
    "        tree = BallTree(P, metric=\"haversine\")\n",
    "        dd, ii = tree.query(X, k=k)\n",
    "        d_km = dd * EARTH_R_KM\n",
    "        neigh_vals = src_val[ii]\n",
    "    else:\n",
    "        d_all = 2.0 * EARTH_R_KM * np.arcsin(np.sqrt(\n",
    "            np.sin((X[:, 0, None] - P[None, :, 0]) / 2.0) ** 2 +\n",
    "            np.cos(X[:, 0, None]) * np.cos(P[None, :, 0]) *\n",
    "            np.sin((X[:, 1, None] - P[None, :, 1]) / 2.0) ** 2\n",
    "        ))\n",
    "        k = min(k, d_all.shape[1])\n",
    "        ordk = np.argpartition(d_all, k-1, axis=1)[:, :k]\n",
    "        sort_idx = np.take_along_axis(d_all, ordk, axis=1).argsort(axis=1)\n",
    "        idxk = np.take_along_axis(ordk, sort_idx, axis=1)\n",
    "        d_km = np.take_along_axis(d_all, idxk, axis=1)\n",
    "        neigh_vals = src_val[idxk]\n",
    "\n",
    "    if str(kernel).lower() == \"gauss\":\n",
    "        s = max((radius_km if radius_km else float(np.nanmedian(d_km))) / 2.0, 1e-3)\n",
    "        W = np.exp(-(d_km / s) ** 2)\n",
    "    else:\n",
    "        W = 1.0 / (d_km + 1e-6) ** max(1, int(power))\n",
    "\n",
    "    if radius_km is not None:\n",
    "        W = np.where(d_km <= float(radius_km), W, 0.0)\n",
    "\n",
    "    Wsum = W.sum(axis=1)\n",
    "    Wsafe = np.where(Wsum == 0.0, 1.0, Wsum)\n",
    "    pred = (W * neigh_vals).sum(axis=1) / Wsafe\n",
    "    pred[Wsum == 0.0] = np.nan\n",
    "    return pred.astype(np.float32)\n",
    "\n",
    "def staged_fill(src_lat, src_lon, src_val, tgt_lat, tgt_lon,\n",
    "                kmax, base_radius_km, max_radius_km, kernel, power):\n",
    "    R0 = float(base_radius_km)\n",
    "    R1 = float(min(2.0 * R0, max_radius_km))\n",
    "    pred = points_idw_or_gauss(src_lat, src_lon, src_val, tgt_lat, tgt_lon,\n",
    "                               kmax=kmax, radius_km=R0, kernel=kernel, power=power)\n",
    "    miss = ~np.isfinite(pred)\n",
    "    if np.any(miss) and (R1 > R0 + 1e-6):\n",
    "        pred2 = points_idw_or_gauss(src_lat, src_lon, src_val,\n",
    "                                    tgt_lat[miss], tgt_lon[miss],\n",
    "                                    kmax=kmax, radius_km=R1,\n",
    "                                    kernel=kernel, power=power)\n",
    "        pred[miss] = pred2\n",
    "    return pred\n",
    "\n",
    "# ================= Floor clamp =================\n",
    "def apply_floor(A: np.ndarray, floor_val: float) -> np.ndarray:\n",
    "    X = A.copy()\n",
    "    m = np.isfinite(X)\n",
    "    if np.any(m):\n",
    "        X[m] = np.maximum(X[m], float(floor_val))\n",
    "    return X\n",
    "\n",
    "# ================= Child process: handle one (year, month) =================\n",
    "def _process_one_month_task(task):\n",
    "    yy = task['yy']; mm = task['mm']\n",
    "    depth_m = float(task['depth_m'])\n",
    "    out_dir = task['out_dir']\n",
    "    floor   = float(task['min_value_floor'])\n",
    "\n",
    "    plat = task['pred_lat'].astype(np.float64)\n",
    "    plon = normalize_lon_to_m180_180(task['pred_lon'].astype(np.float64))\n",
    "    poxy = task['pred_oxy'].astype(np.float64)\n",
    "\n",
    "    load_temp_cache(task[\"temp_nc\"])\n",
    "    tlats = _TEMP_CACHE[\"lats\"].astype(np.float32)\n",
    "    tlons = _TEMP_CACHE[\"lons\"].astype(np.float32)\n",
    "\n",
    "    out_name = f\"GLOBAL_DO_{yy:04d}{mm:02d}15_0p5deg_v1.nc\"\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "\n",
    "    if os.path.exists(out_path):\n",
    "        lats_nc, lons_nc = ensure_month_file(out_path, tlats, tlons, yy, mm)\n",
    "        if (np.max(np.abs(lats_nc - tlats)) > 1e-6) or (np.max(np.abs(lons_nc - tlons)) > 1e-6):\n",
    "            raise RuntimeError(f\"{out_path}: file axes do not match CMEMS axes.\")\n",
    "        target_lats, target_lons = lats_nc.astype(np.float32), lons_nc.astype(np.float32)\n",
    "    else:\n",
    "        target_lats, target_lons = tlats, tlons\n",
    "        ensure_month_file(out_path, target_lats, target_lons, yy, mm)\n",
    "\n",
    "    Ny, Nx = len(target_lats), len(target_lons)\n",
    "\n",
    "    # 1) Bin to grid (mean)\n",
    "    ii = nearest_index_nonuniform(target_lats, plat)\n",
    "    jj = nearest_index_nonuniform(target_lons, plon)\n",
    "    V_pred_raw_on_cmems, CNT_pred = bincount_mean(ii, jj, poxy, Ny, Nx)\n",
    "    have_pred = (CNT_pred > 0)\n",
    "\n",
    "    # 2) Mallow mask\n",
    "    Mallow = build_pred_mask_for_grid(\n",
    "        depth_m=depth_m,\n",
    "        target_lats=target_lats.astype(np.float64),\n",
    "        target_lons=target_lons.astype(np.float64),\n",
    "        temp_nc_path=task[\"temp_nc\"],\n",
    "        mask_nc_path=task[\"mask_nc\"],\n",
    "        shallow_mask_max_m=task[\"shallow_mask_max_m\"],\n",
    "        arctic_cut_lat=task[\"arctic_cut_lat\"],\n",
    "        ll_tol_deg=task[\"ll_tol_deg\"],\n",
    "        depth_tol=task[\"depth_tol\"]\n",
    "    )\n",
    "\n",
    "    # 3) Masked background prediction field\n",
    "    V = np.where(Mallow & have_pred, V_pred_raw_on_cmems, np.nan).astype(np.float32)\n",
    "    V = apply_floor(V, floor)\n",
    "\n",
    "    # 4) Gaussian smoothing (finite-only; footprint-preserving)\n",
    "    V = gaussian_smooth_preserve(\n",
    "        V, target_lats, target_lons,\n",
    "        radius_km=task[\"gauss_radius_km\"],\n",
    "        min_support_frac=task[\"gauss_min_support\"]\n",
    "    )\n",
    "    V = apply_floor(V, floor)\n",
    "\n",
    "    # 5) Fill remaining NaNs inside Mallow (two-stage radius)\n",
    "    need_fill = Mallow & ~np.isfinite(V)\n",
    "    src_mask  = np.isfinite(V)\n",
    "    if np.any(need_fill) and np.any(src_mask):\n",
    "        JJ, II = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "        lat_grid = target_lats[II]\n",
    "        lon_grid = target_lons[JJ]\n",
    "\n",
    "        si, sj = np.where(src_mask)\n",
    "        ti, tj = np.where(need_fill)\n",
    "\n",
    "        src_lat = lat_grid[si, sj]\n",
    "        src_lon = lon_grid[si, sj]\n",
    "        src_val = V[si, sj]\n",
    "        tgt_lat = lat_grid[ti, tj]\n",
    "        tgt_lon = lon_grid[ti, tj]\n",
    "\n",
    "        pred = staged_fill(\n",
    "            src_lat, src_lon, src_val,\n",
    "            tgt_lat, tgt_lon,\n",
    "            kmax=task[\"fill_kmax\"],\n",
    "            base_radius_km=task[\"fill_base_radius_km\"],\n",
    "            max_radius_km=task[\"fill_max_radius_km\"],\n",
    "            kernel=task[\"fill_kernel\"],\n",
    "            power=task[\"fill_power\"]\n",
    "        )\n",
    "        ok = np.isfinite(pred)\n",
    "        if np.any(ok):\n",
    "            V[ti[ok], tj[ok]] = pred[ok]\n",
    "\n",
    "    # 6) Final: valid only inside Mallow + floor\n",
    "    V_out = np.where(Mallow, V, np.nan).astype(np.float32)\n",
    "    V_out = apply_floor(V_out, floor)\n",
    "\n",
    "    # 7) Write NetCDF\n",
    "    write_or_append_depth_sorted(out_path, yy, mm, float(depth_m),\n",
    "                                 target_lats, target_lons, V_out)\n",
    "\n",
    "    vmin = float(np.nanmin(V_out)) if np.isfinite(V_out).any() else np.nan\n",
    "    vmax = float(np.nanmax(V_out)) if np.isfinite(V_out).any() else np.nan\n",
    "    return {\n",
    "        \"yy\": yy, \"mm\": mm, \"depth\": depth_m,\n",
    "        \"out\": out_path, \"vmin\": vmin, \"vmax\": vmax,\n",
    "        \"count_valid\": int(np.isfinite(V_out).sum()),\n",
    "        \"grid\": (Ny, Nx)\n",
    "    }\n",
    "\n",
    "# ================= Main =================\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Monthly DO NetCDF maker on CMEMS grid: mask(Mallow)→smooth→fill→floor→write (NO OBS).\",\n",
    "        allow_abbrev=False\n",
    "    )\n",
    "    parser.add_argument(\"--pred_root\", default=PRED_ROOT)\n",
    "    parser.add_argument(\"--out\",       default=OUT_DIR)\n",
    "    parser.add_argument(\"--depths\", nargs=\"*\", default=DEPTH_LIST_DEFAULT, required=True,\n",
    "                        help=\"Depth list must be provided explicitly, e.g.: --depths 10 20 50 100\")\n",
    "    parser.add_argument(\"--seasons\", nargs=\"*\", default=SEASONS_DEFAULT)\n",
    "\n",
    "    parser.add_argument(\"--workers\", type=int, default=min(24, os.cpu_count() or 1))\n",
    "\n",
    "    parser.add_argument(\"--temp_nc\", default=TEMP_NC_DEFAULT)\n",
    "    parser.add_argument(\"--mask_nc\", default=MASK_NC_DEFAULT)\n",
    "    parser.add_argument(\"--shallow_mask_max_m\", type=float, default=SHALLOW_MASK_APPLY_MAX_M_DEFAULT)\n",
    "    parser.add_argument(\"--arctic_cut_lat\", type=float, default=ARCTIC_CUT_LAT_DEFAULT)\n",
    "    parser.add_argument(\"--ll_tol_deg\", type=float, default=LL_TOL_DEG_DEFAULT)\n",
    "    parser.add_argument(\"--depth_tol\", type=float, default=DEPTH_TOL_DEFAULT)\n",
    "\n",
    "    parser.add_argument(\"--gauss_radius_km\", type=float, default=GAUSS_RADIUS_KM_DEFAULT)\n",
    "    parser.add_argument(\"--gauss_min_support\", type=float, default=MIN_SUPPORT_FRAC_DEFAULT)\n",
    "\n",
    "    parser.add_argument(\"--fill_kernel\", type=str, default=FILL_KERNEL_DEFAULT, choices=[\"idw\", \"gauss\"])\n",
    "    parser.add_argument(\"--fill_kmax\", type=int, default=FILL_KMAX_DEFAULT)\n",
    "    parser.add_argument(\"--fill_base_radius_km\", type=float, default=BASE_RADIUS_KM_DEFAULT)\n",
    "    parser.add_argument(\"--fill_max_radius_km\", type=float, default=MAX_RADIUS_KM_DEFAULT)\n",
    "    parser.add_argument(\"--fill_power\", type=int, default=FILL_POWER_DEFAULT)\n",
    "\n",
    "    parser.add_argument(\"--min_value_floor\", type=float, default=MIN_VALUE_FLOOR_DEFAULT)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    pred_root = str(args.pred_root)\n",
    "    out_dir   = str(args.out)\n",
    "    depths    = [str(d) for d in (args.depths or [])]\n",
    "    seasons   = list(args.seasons or [])\n",
    "    workers   = max(1, int(args.workers))\n",
    "\n",
    "    temp_nc_path       = str(args.temp_nc)\n",
    "    mask_nc_path       = str(args.mask_nc)\n",
    "    shallow_mask_max_m = float(args.shallow_mask_max_m)\n",
    "    arctic_cut_lat     = float(args.arctic_cut_lat)\n",
    "    ll_tol_deg         = float(args.ll_tol_deg)\n",
    "    depth_tol          = float(args.depth_tol)\n",
    "\n",
    "    gauss_radius_km    = float(args.gauss_radius_km)\n",
    "    gauss_min_support  = float(args.gauss_min_support)\n",
    "\n",
    "    fill_kernel        = str(args.fill_kernel).lower()\n",
    "    fill_kmax          = int(args.fill_kmax)\n",
    "    fill_base_radius_km= float(args.fill_base_radius_km)\n",
    "    fill_max_radius_km = float(args.fill_max_radius_km)\n",
    "    fill_power         = int(args.fill_power)\n",
    "\n",
    "    min_value_floor    = float(args.min_value_floor)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    mp_context = mp.get_context(\"fork\") if hasattr(mp, \"get_context\") else None\n",
    "\n",
    "    # Warm up TEMP cache\n",
    "    load_temp_cache(temp_nc_path)\n",
    "\n",
    "    total_written = 0\n",
    "\n",
    "    for depth in depths:\n",
    "        pred_dir = os.path.join(pred_root, f\"{depth}dbar\")\n",
    "        if not os.path.isdir(pred_dir):\n",
    "            print(f\"[WARN] Prediction directory not found: {pred_dir} (skip this depth)\")\n",
    "            continue\n",
    "\n",
    "        for season in seasons:\n",
    "            if season not in SEASON_MONTHS:\n",
    "                print(f\"[WARN] Unknown season name: {season} (skip)\")\n",
    "                continue\n",
    "\n",
    "            base_pred = os.path.join(pred_dir, f\"depth{depth}_{season}_TRAIN\")\n",
    "            pred_candidates = [\n",
    "                base_pred + \"_with_pred_remap.csv\",\n",
    "                base_pred + \"_with_pred.csv\",\n",
    "                base_pred + \".csv\",\n",
    "            ]\n",
    "            pred_csv = next((p for p in pred_candidates if os.path.isfile(p)), None)\n",
    "            if not pred_csv:\n",
    "                print(f\"[INFO] depth={depth} season={season}: no prediction file found; skip.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n[PROC] depth={depth}m season={season}\")\n",
    "            print(f\"  - pred: {os.path.basename(pred_csv)}\")\n",
    "\n",
    "            months_pred, df_pred, _ = read_pred_csv(pred_csv)\n",
    "\n",
    "            # Keep your original month window logic (adjust as needed for your paper version)\n",
    "            def _ym(y, m): return int(y) * 12 + int(m)\n",
    "            START_YM = 1960 * 12 + 1\n",
    "            END_YM   = 2030 * 12 + 12\n",
    "            months = [(y, m) for (y, m) in sorted(set(months_pred))\n",
    "                      if START_YM <= _ym(y, m) <= END_YM]\n",
    "            if not months:\n",
    "                print(\"  - No months within the selection window for this season; skip.\")\n",
    "                continue\n",
    "\n",
    "            pred_groups = {(y, m): g for (y, m), g in df_pred.groupby([\"Year\", \"Month\"], sort=False)}\n",
    "\n",
    "            tasks = []\n",
    "            for (yy, mm) in months:\n",
    "                g_pred = pred_groups.get((yy, mm), None)\n",
    "                if g_pred is None or g_pred.empty:\n",
    "                    continue\n",
    "                tasks.append({\n",
    "                    \"yy\": int(yy), \"mm\": int(mm),\n",
    "                    \"depth_m\": float(depth),\n",
    "                    \"out_dir\": out_dir,\n",
    "                    \"pred_lat\": g_pred[\"Latitude\"].to_numpy(np.float64, copy=False),\n",
    "                    \"pred_lon\": g_pred[\"Longitude\"].to_numpy(np.float64, copy=False),\n",
    "                    \"pred_oxy\": g_pred[\"Oxygen\"].to_numpy(np.float64, copy=False),\n",
    "\n",
    "                    \"temp_nc\": temp_nc_path,\n",
    "                    \"mask_nc\": mask_nc_path,\n",
    "                    \"shallow_mask_max_m\": shallow_mask_max_m,\n",
    "                    \"arctic_cut_lat\": arctic_cut_lat,\n",
    "                    \"ll_tol_deg\": ll_tol_deg,\n",
    "                    \"depth_tol\": depth_tol,\n",
    "\n",
    "                    \"gauss_radius_km\": gauss_radius_km,\n",
    "                    \"gauss_min_support\": gauss_min_support,\n",
    "\n",
    "                    \"fill_kernel\": fill_kernel,\n",
    "                    \"fill_kmax\": fill_kmax,\n",
    "                    \"fill_base_radius_km\": fill_base_radius_km,\n",
    "                    \"fill_max_radius_km\": fill_max_radius_km,\n",
    "                    \"fill_power\": fill_power,\n",
    "\n",
    "                    \"min_value_floor\": min_value_floor,\n",
    "                })\n",
    "\n",
    "            if not tasks:\n",
    "                print(\"  - No writable months for this season.\")\n",
    "                continue\n",
    "\n",
    "            executor_kwargs = {\"max_workers\": workers}\n",
    "            if mp_context is not None:\n",
    "                executor_kwargs[\"mp_context\"] = mp_context\n",
    "\n",
    "            with ProcessPoolExecutor(**executor_kwargs) as ex:\n",
    "                futs = {ex.submit(_process_one_month_task, t): (t[\"yy\"], t[\"mm\"]) for t in tasks}\n",
    "                for fut in as_completed(futs):\n",
    "                    yy_, mm_ = futs[fut]\n",
    "                    try:\n",
    "                        info = fut.result()\n",
    "                        total_written += 1\n",
    "                        print(\n",
    "                            f\"    [OK] {info['yy']}-{info['mm']:02d} \"\n",
    "                            f\"depth={info['depth']:g}m -> {os.path.basename(info['out'])} \"\n",
    "                            f\"grid={info['grid']} out[min,max]=[{info['vmin']:.3f},{info['vmax']:.3f}] \"\n",
    "                            f\"valid={info['count_valid']}\"\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"    [ERR] {yy_}-{mm_:02d} depth={depth}m : {e}\")\n",
    "\n",
    "    if total_written == 0:\n",
    "        print(\"\\n[DONE] No writable data found (check input paths/naming/columns).\")\n",
    "    else:\n",
    "        print(f\"\\n[DONE] Successfully wrote/updated {total_written} (month, depth) slices.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Limit BLAS thread counts (avoid oversubscription under multiprocessing)\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
