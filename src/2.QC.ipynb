{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch clean + dedup per-depth CSVs under:\n",
    "  /data/wang/Result_Data/alldoxy/{depth}dbar/*.csv\n",
    "\n",
    "Outputs (per folder):\n",
    "  depthX.csv  ->  depthX_TRAIN.csv   (same folder)\n",
    "\n",
    "IMPORTANT (carried requirements)\n",
    "- Preserve existing field: sigma_interp\n",
    "- Output column order forces sigma_interp placed right after Source:\n",
    "    Date,Time,Pressure,Latitude,Longitude,Temperature,Salinity,Oxygen,Source,sigma_interp\n",
    "- sigma_interp NOT used in dedup comparisons; carried from surviving record\n",
    "- Drop rows where sigma_interp > 3 (sigma_interp NA kept)\n",
    "- Oxygen QC strictly positive: Oxygen > 0 and < 600 (0 not kept)\n",
    "- If output file exists, delete it before writing\n",
    "\n",
    "NEW (this version)\n",
    "- Final output rounding:\n",
    "    Latitude/Longitude: round to 4 decimals (max 4)\n",
    "    Temperature/Salinity/Oxygen: round to 2 decimals (max 2)\n",
    "\n",
    "CHANGE (requested)\n",
    "- Argo and OceanSITES do NOT participate in cross-source dedup (Rule D).\n",
    "  i.e., they will NOT be linked into cross-source clusters with any other sources,\n",
    "  and will never be dropped by cross-source dedup.\n",
    "  Implementation: when building cross-source edges, skip any pair where either side\n",
    "  has SourceLabel in {\"Argo\",\"OceanSITES\"}.\n",
    "\n",
    "ADDED (requested)\n",
    "- Switch to enable/disable cross-source dedup (Rule D). Default OFF.\n",
    "\n",
    "Logs written to:\n",
    "  /data/wang/Result_Data/alldoxy/_logs/\n",
    "    dedup_file_summary.csv\n",
    "    dedup_source_summary.csv\n",
    "    crosssrc_cluster_size_hist.csv\n",
    "\n",
    "Python: 3.9\n",
    "\"\"\"\n",
    "\n",
    "import os, re, math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "\n",
    "ROOT_DIR = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "LOG_DIR  = ROOT_DIR / \"_logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WRITE_ENCODING = \"utf-8-sig\"\n",
    "\n",
    "LAT_MIN, LAT_MAX = -90.0, 90.0\n",
    "O2_MIN, O2_MAX   = 0.0, 600.0\n",
    "PRESSURE_MIN     = 0.0\n",
    "\n",
    "SIGMA_MAX = 3.0  # drop sigma_interp > 3 (NA kept)\n",
    "\n",
    "R_KM     = 1.0\n",
    "DT_HOURS = 24.0\n",
    "\n",
    "# Candidate bucketing (coarse prefilter; exact check uses haversine + dt)\n",
    "TIME_BUCKET_HOURS = 6\n",
    "SPACE_BUCKET_DEG  = 0.1\n",
    "\n",
    "EXCLUDE_FROM_PRESSURE_COLLAPSE = {\"Argo\", \"OSDCTD\"}  # SourceLabel exclusions\n",
    "EXCLUDE_FROM_CROSSSRC = {\"Argo\", \"OceanSITES\"}       # SourceLabel exclusions for cross-source dedup\n",
    "\n",
    "# >>> Cross-source dedup (Rule D) switch (DEFAULT OFF) <<<\n",
    "ENABLE_CROSS_SOURCE_DEDUP = False\n",
    "\n",
    "SOURCE_PRIORITY = {\n",
    "    \"OSDCTD\": 0,\n",
    "    \"GLODAPV2 2022\": 1,\n",
    "    \"CCHDO_Bottle\": 2,\n",
    "    \"CCHDO_CTD\": 3,\n",
    "    \"Argo\": 4,\n",
    "    \"Geotraces IDP\": 5,\n",
    "    \"OceanSITES\": 6,\n",
    "    \"Other\": 99,\n",
    "}\n",
    "\n",
    "# Output column order (force sigma_interp after Source)\n",
    "OUT_COLS_ORDER = [\n",
    "    \"Date\", \"Time\", \"Pressure\", \"Latitude\", \"Longitude\",\n",
    "    \"Temperature\", \"Salinity\", \"Oxygen\", \"Source\", \"sigma_interp\"\n",
    "]\n",
    "\n",
    "# Rounding for final output\n",
    "OUT_LATLON_DECIMALS = 4\n",
    "OUT_TSO2_DECIMALS   = 2\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def normalize_source_label(src) -> str:\n",
    "    if src is None or (isinstance(src, float) and np.isnan(src)):\n",
    "        return \"Other\"\n",
    "    s = str(src).strip()\n",
    "    su = s.upper()\n",
    "\n",
    "    if \"OSDCTD\" in su:\n",
    "        return \"OSDCTD\"\n",
    "    if su in {\"OSD\", \"CTD\"}:\n",
    "        return \"OSDCTD\"\n",
    "\n",
    "    if \"GLODAP\" in su:\n",
    "        return \"GLODAPV2 2022\"\n",
    "\n",
    "    if \"CCHDO\" in su and \"BOTTLE\" in su:\n",
    "        return \"CCHDO_Bottle\"\n",
    "    if \"CCHDO\" in su and \"CTD\" in su:\n",
    "        return \"CCHDO_CTD\"\n",
    "\n",
    "    if \"OCEANSITES\" in su:\n",
    "        return \"OceanSITES\"\n",
    "\n",
    "    if \"GEOTRACES\" in su or \"IDP\" in su:\n",
    "        return \"Geotraces IDP\"\n",
    "\n",
    "    if \"ARGO\" in su:\n",
    "        return \"Argo\"\n",
    "\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def normalize_lon_to_180(lon: pd.Series) -> pd.Series:\n",
    "    lonv = pd.to_numeric(lon, errors=\"coerce\")\n",
    "    return ((lonv + 180.0) % 360.0) - 180.0\n",
    "\n",
    "\n",
    "def parse_date_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize:\n",
    "      Date -> YYYY-MM-DD\n",
    "      Time -> HH:MM (NA if missing or 00:00)\n",
    "    Adds:\n",
    "      dt (datetime64[ns]) for time-difference calculations\n",
    "    \"\"\"\n",
    "    if \"Time\" not in df.columns:\n",
    "        df[\"Time\"] = pd.NA\n",
    "\n",
    "    date_raw = df[\"Date\"].astype(\"string\")\n",
    "    time_raw = df[\"Time\"].astype(\"string\")\n",
    "\n",
    "    has_time_in_date = date_raw.str.contains(\":\", regex=False, na=False)\n",
    "    dt_from_date = pd.to_datetime(date_raw, errors=\"coerce\")\n",
    "\n",
    "    time_norm = time_raw.str.slice(0, 5)\n",
    "    time_norm = time_norm.mask(time_norm.isna() | (time_norm.str.strip() == \"\"), pd.NA)\n",
    "    time_norm = time_norm.mask(time_norm == \"00:00\", pd.NA)\n",
    "\n",
    "    time_from_date = dt_from_date.dt.strftime(\"%H:%M\").astype(\"string\")\n",
    "    time_from_date = time_from_date.mask(~has_time_in_date, pd.NA)\n",
    "    time_from_date = time_from_date.mask(time_from_date == \"00:00\", pd.NA)\n",
    "\n",
    "    time_final = time_norm.fillna(time_from_date)\n",
    "    date_final = dt_from_date.dt.strftime(\"%Y-%m-%d\").astype(\"string\")\n",
    "\n",
    "    time_for_dt = time_final.fillna(\"00:00\")\n",
    "    dt_calc = pd.to_datetime(date_final + \" \" + time_for_dt, errors=\"coerce\")\n",
    "\n",
    "    df[\"Date\"] = date_final\n",
    "    df[\"Time\"] = time_final\n",
    "    df[\"dt\"]   = dt_calc\n",
    "    return df\n",
    "\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
    "    R = 6371.0\n",
    "    phi1 = math.radians(lat1); phi2 = math.radians(lat2)\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlmb = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlmb/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "\n",
    "def build_buckets(df: pd.DataFrame) -> None:\n",
    "    dt_ns = df[\"dt\"].astype(\"datetime64[ns]\").view(\"int64\")\n",
    "    t_hours = (dt_ns // 10**9) / 3600.0\n",
    "    df[\"t_bucket\"] = np.floor(t_hours / TIME_BUCKET_HOURS).astype(np.int64)\n",
    "    df[\"lat_bucket\"] = np.floor(df[\"Latitude\"].astype(np.float64) / SPACE_BUCKET_DEG).astype(np.int64)\n",
    "    df[\"lon_bucket\"] = np.floor(df[\"Longitude\"].astype(np.float64) / SPACE_BUCKET_DEG).astype(np.int64)\n",
    "\n",
    "\n",
    "def uf_init(n: int):\n",
    "    parent = np.arange(n, dtype=np.int64)\n",
    "    rank = np.zeros(n, dtype=np.int8)\n",
    "\n",
    "    def find(x: int) -> int:\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(a: int, b: int) -> None:\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        if rank[ra] < rank[rb]:\n",
    "            parent[ra] = rb\n",
    "        elif rank[ra] > rank[rb]:\n",
    "            parent[rb] = ra\n",
    "        else:\n",
    "            parent[rb] = ra\n",
    "            rank[ra] += 1\n",
    "\n",
    "    return find, union, parent\n",
    "\n",
    "\n",
    "def get_target_depth_from_folder(folder: Path) -> float:\n",
    "    m = re.search(r\"(\\d+)dbar$\", folder.name)\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    return float(m.group(1))\n",
    "\n",
    "\n",
    "def pick_input_csv_in_folder(folder: Path) -> Path:\n",
    "    # Expect one CSV; ignore already produced *_TRAIN.csv\n",
    "    cands = sorted([p for p in folder.glob(\"*.csv\") if not p.name.endswith(\"_TRAIN.csv\")])\n",
    "    if len(cands) == 0:\n",
    "        raise FileNotFoundError(f\"No csv found in {folder}\")\n",
    "    if len(cands) > 1:\n",
    "        depth_like = [p for p in cands if p.name.lower().startswith(\"depth\")]\n",
    "        if len(depth_like) == 1:\n",
    "            return depth_like[0]\n",
    "        return cands[0]\n",
    "    return cands[0]\n",
    "\n",
    "\n",
    "def out_train_path(in_csv: Path) -> Path:\n",
    "    return in_csv.with_name(in_csv.stem + \"_TRAIN.csv\")\n",
    "\n",
    "\n",
    "def ensure_out_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in OUT_COLS_ORDER:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "    return df[OUT_COLS_ORDER].copy()\n",
    "\n",
    "\n",
    "def finalize_rounding(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply final rounding constraints:\n",
    "      - Latitude/Longitude: 4 decimals\n",
    "      - Temperature/Salinity/Oxygen: 2 decimals\n",
    "    Keep NA as NA.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    for c in [\"Latitude\", \"Longitude\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").round(OUT_LATLON_DECIMALS)\n",
    "\n",
    "    for c in [\"Temperature\", \"Salinity\", \"Oxygen\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").round(OUT_TSO2_DECIMALS)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Per-file processing\n",
    "# =========================\n",
    "\n",
    "def process_depth_file(csv_path: Path, target_depth: float):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      df_out: cleaned+dedup df (includes sigma_interp preserved)\n",
    "      stats: dict\n",
    "      per_label_drop: dict for drop counts by SourceLabel for each category\n",
    "      cluster_hist: Counter cluster size distribution (cross-source)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    n_raw = len(df)\n",
    "\n",
    "    need = [\"Date\", \"Latitude\", \"Longitude\", \"Pressure\", \"Oxygen\", \"Source\", \"sigma_interp\"]\n",
    "    for c in need:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing column '{c}' in {csv_path}\")\n",
    "\n",
    "    # Parse/standardize Date & Time\n",
    "    df = parse_date_time(df)\n",
    "\n",
    "    # Numeric conversions + lon normalization\n",
    "    df[\"Latitude\"]  = pd.to_numeric(df[\"Latitude\"], errors=\"coerce\")\n",
    "    df[\"Longitude\"] = normalize_lon_to_180(df[\"Longitude\"])\n",
    "    df[\"Pressure\"]  = pd.to_numeric(df[\"Pressure\"], errors=\"coerce\")\n",
    "    df[\"Oxygen\"]    = pd.to_numeric(df[\"Oxygen\"], errors=\"coerce\")\n",
    "\n",
    "    # Keep sigma_interp (numeric if possible; NA allowed)\n",
    "    df[\"sigma_interp\"] = pd.to_numeric(df[\"sigma_interp\"], errors=\"coerce\")\n",
    "\n",
    "    # Hard QC (sigma_interp NA kept, but if non-NA must be <= SIGMA_MAX)\n",
    "    mask = (\n",
    "        df[\"Date\"].notna() &\n",
    "        df[\"dt\"].notna() &\n",
    "        df[\"Latitude\"].notna() &\n",
    "        df[\"Longitude\"].notna() &\n",
    "        df[\"Pressure\"].notna() &\n",
    "        df[\"Oxygen\"].notna() &\n",
    "        df[\"Source\"].notna()\n",
    "    )\n",
    "    mask &= df[\"Latitude\"].between(LAT_MIN, LAT_MAX, inclusive=\"both\")\n",
    "    mask &= (df[\"Longitude\"] >= -180.0) & (df[\"Longitude\"] < 180.0)\n",
    "    mask &= df[\"Pressure\"] >= PRESSURE_MIN\n",
    "    mask &= (df[\"Oxygen\"] > O2_MIN) & (df[\"Oxygen\"] < O2_MAX)\n",
    "    mask &= (df[\"sigma_interp\"].isna() | (df[\"sigma_interp\"] <= SIGMA_MAX))\n",
    "\n",
    "    n_qc_drop = int((~mask).sum())\n",
    "    df = df.loc[mask].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        stats = dict(\n",
    "            n_raw=int(n_raw),\n",
    "            n_qc_drop=int(n_qc_drop),\n",
    "            n_exactdup_drop=0,\n",
    "            n_presscollapse_drop=0,\n",
    "            n_crosssrc_drop=0,\n",
    "            n_final=0,\n",
    "            qc_drop_ratio=float(n_qc_drop / n_raw) if n_raw else 0.0,\n",
    "            exactdup_drop_ratio=0.0,\n",
    "            presscollapse_drop_ratio=0.0,\n",
    "            crosssrc_drop_ratio=0.0,\n",
    "            final_ratio=0.0,\n",
    "            crosssrc_enabled=bool(ENABLE_CROSS_SOURCE_DEDUP),\n",
    "        )\n",
    "        df_out = ensure_out_columns(df)\n",
    "        df_out = finalize_rounding(df_out)\n",
    "        return df_out, stats, {}, Counter()\n",
    "\n",
    "    # Source label & rank\n",
    "    df[\"SourceLabel\"] = df[\"Source\"].map(normalize_source_label)\n",
    "    df[\"src_rank\"] = df[\"SourceLabel\"].map(lambda x: SOURCE_PRIORITY.get(x, SOURCE_PRIORITY[\"Other\"])).astype(np.int64)\n",
    "\n",
    "    # Grouping keys (use 4-decimal lat/lon keys to align with output precision)\n",
    "    df[\"Lat_key\"] = df[\"Latitude\"].round(OUT_LATLON_DECIMALS)\n",
    "    df[\"Lon_key\"] = df[\"Longitude\"].round(OUT_LATLON_DECIMALS)\n",
    "    df[\"Time_key\"] = df[\"Time\"].astype(\"string\").fillna(\"\")\n",
    "\n",
    "    # -------------------------\n",
    "    # B) same-source exact duplicates\n",
    "    # -------------------------\n",
    "    df = df.sort_values(\n",
    "        [\"Source\", \"Date\", \"Time_key\", \"Pressure\", \"Lat_key\", \"Lon_key\", \"Oxygen\"],\n",
    "        kind=\"mergesort\"\n",
    "    ).copy()\n",
    "\n",
    "    dup_mask = df.duplicated(\n",
    "        subset=[\"Source\", \"Date\", \"Time_key\", \"Pressure\", \"Lat_key\", \"Lon_key\", \"Oxygen\"],\n",
    "        keep=\"first\"\n",
    "    )\n",
    "    n_exactdup_drop = int(dup_mask.sum())\n",
    "    df = df.loc[~dup_mask].copy()\n",
    "\n",
    "    # -------------------------\n",
    "    # C) same-source multi-pressure collapse (exclude Argo & OSDCTD)\n",
    "    # -------------------------\n",
    "    per_label_drop = defaultdict(int)\n",
    "\n",
    "    if np.isfinite(target_depth):\n",
    "        can_collapse = ~df[\"SourceLabel\"].isin(EXCLUDE_FROM_PRESSURE_COLLAPSE)\n",
    "\n",
    "        df[\"grp_key\"] = (\n",
    "            df[\"Source\"].astype(\"string\") + \"||\" +\n",
    "            df[\"Date\"].astype(\"string\") + \"||\" +\n",
    "            df[\"Time_key\"].astype(\"string\") + \"||\" +\n",
    "            df[\"Lat_key\"].astype(\"string\") + \"||\" +\n",
    "            df[\"Lon_key\"].astype(\"string\")\n",
    "        )\n",
    "\n",
    "        eligible = df.loc[can_collapse].copy()\n",
    "        if not eligible.empty:\n",
    "            eligible[\"p_dist\"] = (eligible[\"Pressure\"] - target_depth).abs()\n",
    "            eligible = eligible.sort_values(\n",
    "                [\"grp_key\", \"p_dist\", \"dt\"],\n",
    "                ascending=[True, True, True],\n",
    "                kind=\"mergesort\"\n",
    "            )\n",
    "            keep_idx = eligible.groupby(\"grp_key\", sort=False).head(1).index\n",
    "            drop_idx = eligible.index.difference(keep_idx)\n",
    "\n",
    "            n_presscollapse_drop = int(len(drop_idx))\n",
    "            if n_presscollapse_drop > 0:\n",
    "                dropped = df.loc[drop_idx, \"SourceLabel\"].value_counts()\n",
    "                for k, v in dropped.to_dict().items():\n",
    "                    per_label_drop[f\"presscollapse_drop::{k}\"] += int(v)\n",
    "\n",
    "            df = df.drop(index=drop_idx).copy()\n",
    "        else:\n",
    "            n_presscollapse_drop = 0\n",
    "\n",
    "        df.drop(columns=[\"grp_key\"], inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        n_presscollapse_drop = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # D) cross-source near-duplicates (OPTIONAL; default OFF)\n",
    "    # -------------------------\n",
    "    if (not ENABLE_CROSS_SOURCE_DEDUP) or (len(df) <= 1):\n",
    "        n_crosssrc_drop = 0\n",
    "        cluster_hist = Counter({1: int(len(df))}) if len(df) else Counter()\n",
    "        df_out = df.copy()\n",
    "    else:\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        df[\"dt_ns\"] = df[\"dt\"].astype(\"datetime64[ns]\").view(\"int64\")\n",
    "\n",
    "        # meta completeness for tie-break\n",
    "        meta_score = np.zeros(len(df), dtype=np.int8)\n",
    "        if \"Temperature\" in df.columns:\n",
    "            meta_score += pd.to_numeric(df[\"Temperature\"], errors=\"coerce\").notna().to_numpy(dtype=np.int8)\n",
    "        if \"Salinity\" in df.columns:\n",
    "            meta_score += pd.to_numeric(df[\"Salinity\"], errors=\"coerce\").notna().to_numpy(dtype=np.int8)\n",
    "        df[\"meta_score\"] = meta_score\n",
    "\n",
    "        build_buckets(df)\n",
    "\n",
    "        bucket = defaultdict(list)\n",
    "        for i, key in enumerate(zip(df[\"t_bucket\"].to_numpy(),\n",
    "                                    df[\"lat_bucket\"].to_numpy(),\n",
    "                                    df[\"lon_bucket\"].to_numpy())):\n",
    "            bucket[(int(key[0]), int(key[1]), int(key[2]))].append(i)\n",
    "\n",
    "        find, union, parent = uf_init(len(df))\n",
    "        dt_max_sec = DT_HOURS * 3600.0\n",
    "\n",
    "        # Create cross-source edges\n",
    "        for i in range(len(df)):\n",
    "            tb = int(df.at[i, \"t_bucket\"])\n",
    "            lb = int(df.at[i, \"lat_bucket\"])\n",
    "            ob = int(df.at[i, \"lon_bucket\"])\n",
    "\n",
    "            src_i = df.at[i, \"Source\"]\n",
    "            lbl_i = df.at[i, \"SourceLabel\"]\n",
    "\n",
    "            # Excluded labels do not participate\n",
    "            if lbl_i in EXCLUDE_FROM_CROSSSRC:\n",
    "                continue\n",
    "\n",
    "            for dtb in (tb - 1, tb, tb + 1):\n",
    "                for dlb in (lb - 1, lb, lb + 1):\n",
    "                    for dob in (ob - 1, ob, ob + 1):\n",
    "                        cand = bucket.get((dtb, dlb, dob), [])\n",
    "                        for j in cand:\n",
    "                            if j >= i:\n",
    "                                continue\n",
    "                            if src_i == df.at[j, \"Source\"]:\n",
    "                                continue\n",
    "\n",
    "                            lbl_j = df.at[j, \"SourceLabel\"]\n",
    "                            if lbl_j in EXCLUDE_FROM_CROSSSRC:\n",
    "                                continue\n",
    "\n",
    "                            # time filter\n",
    "                            dts = abs((df.at[i, \"dt_ns\"] - df.at[j, \"dt_ns\"]) / 1e9)\n",
    "                            if dts > dt_max_sec:\n",
    "                                continue\n",
    "\n",
    "                            # distance filter\n",
    "                            dist = haversine_km(\n",
    "                                float(df.at[i, \"Latitude\"]), float(df.at[i, \"Longitude\"]),\n",
    "                                float(df.at[j, \"Latitude\"]), float(df.at[j, \"Longitude\"])\n",
    "                            )\n",
    "                            if dist > R_KM:\n",
    "                                continue\n",
    "\n",
    "                            union(i, j)\n",
    "\n",
    "        # Build clusters\n",
    "        clusters = defaultdict(list)\n",
    "        for i in range(len(df)):\n",
    "            clusters[find(i)].append(i)\n",
    "\n",
    "        cluster_hist = Counter()\n",
    "        keep_mask = np.zeros(len(df), dtype=bool)\n",
    "        n_crosssrc_drop = 0\n",
    "\n",
    "        for root, idxs in clusters.items():\n",
    "            if len(idxs) == 1:\n",
    "                keep_mask[idxs[0]] = True\n",
    "                cluster_hist[1] += 1\n",
    "                continue\n",
    "\n",
    "            # If this cluster contains only ONE unique Source, do NOT dedup by cross-source rule\n",
    "            srcs = df.loc[idxs, \"Source\"].astype(\"string\")\n",
    "            if srcs.nunique(dropna=False) <= 1:\n",
    "                for k in idxs:\n",
    "                    keep_mask[k] = True\n",
    "                cluster_hist[len(idxs)] += 1\n",
    "                continue\n",
    "\n",
    "            cluster_hist[len(idxs)] += 1\n",
    "\n",
    "            sub = df.loc[idxs].copy()\n",
    "            sub[\"p_dist\"] = (sub[\"Pressure\"] - target_depth).abs() if np.isfinite(target_depth) else 0.0\n",
    "\n",
    "            sub = sub.sort_values(\n",
    "                [\"src_rank\", \"meta_score\", \"p_dist\", \"dt_ns\"],\n",
    "                ascending=[True, False, True, True],\n",
    "                kind=\"mergesort\"\n",
    "            )\n",
    "            winner = int(sub.index[0])\n",
    "            keep_mask[winner] = True\n",
    "\n",
    "            dropped_idxs = [k for k in idxs if k != winner]\n",
    "            n_crosssrc_drop += len(dropped_idxs)\n",
    "\n",
    "            dropped_lbl = df.loc[dropped_idxs, \"SourceLabel\"].value_counts()\n",
    "            for k, v in dropped_lbl.to_dict().items():\n",
    "                per_label_drop[f\"crosssrc_drop::{k}\"] += int(v)\n",
    "\n",
    "        df_out = df.loc[keep_mask].copy()\n",
    "        df_out.drop(\n",
    "            columns=[\"dt_ns\", \"t_bucket\", \"lat_bucket\", \"lon_bucket\", \"meta_score\"],\n",
    "            inplace=True,\n",
    "            errors=\"ignore\"\n",
    "        )\n",
    "\n",
    "    # Final cleanup\n",
    "    df_out.drop(columns=[\"Lat_key\", \"Lon_key\", \"Time_key\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Force output order (sigma_interp after Source)\n",
    "    df_out = ensure_out_columns(df_out)\n",
    "\n",
    "    # Apply rounding at the very end (output constraint)\n",
    "    df_out = finalize_rounding(df_out)\n",
    "\n",
    "    stats = dict(\n",
    "        n_raw=int(n_raw),\n",
    "        n_qc_drop=int(n_qc_drop),\n",
    "        n_exactdup_drop=int(n_exactdup_drop),\n",
    "        n_presscollapse_drop=int(n_presscollapse_drop),\n",
    "        n_crosssrc_drop=int(n_crosssrc_drop),\n",
    "        n_final=int(len(df_out)),\n",
    "        qc_drop_ratio=float(n_qc_drop / n_raw) if n_raw else 0.0,\n",
    "        exactdup_drop_ratio=float(n_exactdup_drop / max(1, (n_raw - n_qc_drop))),\n",
    "        presscollapse_drop_ratio=float(n_presscollapse_drop / max(1, (n_raw - n_qc_drop - n_exactdup_drop))),\n",
    "        crosssrc_drop_ratio=float(n_crosssrc_drop / max(1, (n_raw - n_qc_drop - n_exactdup_drop - n_presscollapse_drop))),\n",
    "        final_ratio=float(len(df_out) / n_raw) if n_raw else 0.0,\n",
    "        crosssrc_enabled=bool(ENABLE_CROSS_SOURCE_DEDUP),\n",
    "    )\n",
    "    return df_out, stats, dict(per_label_drop), cluster_hist\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batch run\n",
    "# =========================\n",
    "\n",
    "depth_folders = sorted([p for p in ROOT_DIR.iterdir()\n",
    "                        if p.is_dir() and re.match(r\"^\\d+dbar$\", p.name)])\n",
    "print(f\"Found {len(depth_folders)} depth folders under {ROOT_DIR}\")\n",
    "print(f\"Cross-source dedup enabled: {ENABLE_CROSS_SOURCE_DEDUP}\")\n",
    "\n",
    "file_rows = []\n",
    "source_rows = defaultdict(lambda: Counter())\n",
    "cluster_hist_all = Counter()\n",
    "\n",
    "for folder in tqdm(depth_folders, desc=\"Process folders\", unit=\"folder\"):\n",
    "    target_depth = get_target_depth_from_folder(folder)\n",
    "\n",
    "    try:\n",
    "        in_csv = pick_input_csv_in_folder(folder)\n",
    "    except Exception as e:\n",
    "        file_rows.append({\"folder\": str(folder), \"error\": f\"input_csv_not_found: {e}\"})\n",
    "        continue\n",
    "\n",
    "    out_csv = out_train_path(in_csv)\n",
    "\n",
    "    try:\n",
    "        df_out, stats, per_label_drop, cl_hist = process_depth_file(in_csv, target_depth)\n",
    "    except Exception as e:\n",
    "        file_rows.append({\n",
    "            \"folder\": str(folder),\n",
    "            \"input_csv\": str(in_csv),\n",
    "            \"output_csv\": str(out_csv),\n",
    "            \"depth_dbar\": target_depth,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # If output exists, delete it first\n",
    "    try:\n",
    "        if out_csv.exists():\n",
    "            out_csv.unlink()\n",
    "    except Exception:\n",
    "        if os.path.exists(str(out_csv)):\n",
    "            os.remove(str(out_csv))\n",
    "\n",
    "    # Write output\n",
    "    df_out.to_csv(out_csv, index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "    row = {\n",
    "        \"folder\": str(folder),\n",
    "        \"depth_dbar\": int(target_depth) if np.isfinite(target_depth) else None,\n",
    "        \"input_csv\": str(in_csv),\n",
    "        \"output_csv\": str(out_csv),\n",
    "        **stats\n",
    "    }\n",
    "    file_rows.append(row)\n",
    "\n",
    "    for k, v in per_label_drop.items():\n",
    "        depth_tag = f\"depth_{int(target_depth)}dbar\" if np.isfinite(target_depth) else \"depth_nan\"\n",
    "        source_rows[k].update({depth_tag: int(v)})\n",
    "\n",
    "    cluster_hist_all.update(cl_hist)\n",
    "\n",
    "# =========================\n",
    "# Write logs (robust to empty)\n",
    "# =========================\n",
    "\n",
    "df_file = pd.DataFrame(file_rows)\n",
    "df_file.to_csv(LOG_DIR / \"dedup_file_summary.csv\", index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "# --- dedup_source_summary.csv (FIX KeyError: drop_category) ---\n",
    "src_sum = []\n",
    "for key, cnt in source_rows.items():\n",
    "    total = sum(cnt.values())\n",
    "    src_sum.append({\"drop_category\": key, \"total_dropped\": int(total)})\n",
    "\n",
    "if len(src_sum) == 0:\n",
    "    df_src = pd.DataFrame(columns=[\"drop_category\", \"total_dropped\"])\n",
    "else:\n",
    "    df_src = pd.DataFrame(src_sum).sort_values([\"drop_category\"])\n",
    "\n",
    "df_src.to_csv(LOG_DIR / \"dedup_source_summary.csv\", index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "# --- crosssrc_cluster_size_hist.csv (robust) ---\n",
    "if len(cluster_hist_all) == 0:\n",
    "    df_hist = pd.DataFrame(columns=[\"cluster_size\", \"n_clusters\"])\n",
    "else:\n",
    "    df_hist = pd.DataFrame(\n",
    "        [{\"cluster_size\": int(k), \"n_clusters\": int(v)} for k, v in sorted(cluster_hist_all.items())]\n",
    "    )\n",
    "df_hist.to_csv(LOG_DIR / \"crosssrc_cluster_size_hist.csv\", index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "print(\"\\n[DONE]\")\n",
    "print(\"Outputs written as *_TRAIN.csv in each depth folder (existing outputs overwritten).\")\n",
    "print(f\"Logs written to: {LOG_DIR}\")\n",
    "print(f\"  - {LOG_DIR / 'dedup_file_summary.csv'}\")\n",
    "print(f\"  - {LOG_DIR / 'dedup_source_summary.csv'}\")\n",
    "print(f\"  - {LOG_DIR / 'crosssrc_cluster_size_hist.csv'}\")\n",
    "print(\"Output columns enforced:\", \",\".join(OUT_COLS_ORDER))\n",
    "print(f\"Output rounding: Lat/Lon={OUT_LATLON_DECIMALS} decimals; T/S/O2={OUT_TSO2_DECIMALS} decimals\")\n",
    "print(\"Cross-source dedup EXCLUDED labels:\", \",\".join(sorted(EXCLUDE_FROM_CROSSSRC)))\n",
    "print(\"Cross-source dedup ENABLED:\", ENABLE_CROSS_SOURCE_DEDUP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Compute oxygen saturation percent Sat(%) and DROP Sat>=120 for depth>200 dbar\n",
    "for *TRAIN.csv under:\n",
    "  /data/wang/Result_Data/alldoxy/{depth}dbar/\n",
    "\n",
    "Sat definition:\n",
    "  Sat(%) = 100 * Oxygen / O2_sat_umolkg\n",
    "O2_sat_umolkg computed by:\n",
    "  - TEOS-10 (gsw) preferred: SP + pt0 -> gsw.O2sol_SP_pt (μmol/kg)\n",
    "  - fallback: Weiss(1970) (ml/L) + EOS-80 density -> μmol/kg\n",
    "\n",
    "BEHAVIOR (this version):\n",
    "- Always compute/overwrite column \"Sat\" (percent) when possible.\n",
    "- For folders with depth_dbar > 200:\n",
    "    DROP rows where Sat >= 120 (only if Sat is finite).\n",
    "  (depth <= 200: keep all rows; still compute Sat)\n",
    "- Keep rows with Sat=NA (due to missing T/S or missing O2sat), never drop by Sat.\n",
    "- Write per-file logs + aggregated summary logs into:\n",
    "    /data/wang/Result_Data/alldoxy/_logs/\n",
    "      sat_qc_drop_log.csv\n",
    "      sat_qc_drop_log_summary.csv\n",
    "\n",
    "NOTE:\n",
    "- This script edits files in-place (atomic replace).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "ROOT = Path(\"/data/wang/Result_Data/alldoxy\")\n",
    "LOG_DIR = ROOT / \"_logs\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process all these depth folders (you can trim if you only want a subset)\n",
    "TARGET_DEPTHS = [\n",
    "    1,10,20,30,40,50,60,70,80,90,100,\n",
    "    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "    5000,5100,5200,5300,5400,5500\n",
    "]\n",
    "\n",
    "TEMP_COL = \"Temperature\"  # °C\n",
    "SAL_COL  = \"Salinity\"     # PSU/SP\n",
    "OXY_COL  = \"Oxygen\"       # μmol/kg\n",
    "SAT_COL  = \"Sat\"          # %\n",
    "\n",
    "WRITE_ENCODING = \"utf-8-sig\"\n",
    "SAT_DROP_THR = 120.0      # drop if Sat >= 120 (depth>200 only)\n",
    "\n",
    "# =========================\n",
    "# Fallback: Weiss(1970) + EOS-80 rho\n",
    "# =========================\n",
    "def _rho_eos80_kg_m3(S, T):\n",
    "    \"\"\"EOS-80 density (kg/m^3) near 0 dbar; S=PSU, T=°C (ITS-90).\"\"\"\n",
    "    S = np.asarray(S, float); T = np.asarray(T, float)\n",
    "    rho_w = (999.842594 + 6.793952e-2*T - 9.095290e-3*T**2\n",
    "             + 1.001685e-4*T**3 - 1.120083e-6*T**4 + 6.536332e-9*T**5)\n",
    "    A = (0.824493 - 4.0899e-3*T + 7.6438e-5*T**2 - 8.2467e-7*T**3 + 5.3875e-9*T**4)\n",
    "    B = (-5.72466e-3 + 1.0227e-4*T - 1.6546e-6*T**2)\n",
    "    C = 4.8314e-4\n",
    "    return rho_w + A*S + B*(S**1.5) + C*(S**2)\n",
    "\n",
    "def _o2sol_weiss_ml_per_L(T, S):\n",
    "    \"\"\"Weiss (1970) O2 solubility (ml/L), T=°C (ITS-90) with IPTS-68 conversion inside.\"\"\"\n",
    "    T = np.asarray(T, float); S = np.asarray(S, float)\n",
    "    Tk = T*1.00024 + 273.15  # ITS-90 -> IPTS-68 -> Kelvin\n",
    "    A1, A2, A3, A4 = -173.4292, 249.6339, 143.3483, -21.8492\n",
    "    B1, B2, B3 = -0.033096, 0.014259, -0.0017000\n",
    "    lnC = (A1 + A2*(100.0/Tk) + A3*np.log(Tk/100.0) + A4*(Tk/100.0)\n",
    "           + S*(B1 + B2*(Tk/100.0) + B3*(Tk/100.0)**2))\n",
    "    return np.exp(lnC)\n",
    "\n",
    "def o2_sat_umolkg_weiss(T, S):\n",
    "    \"\"\"Weiss saturation O2 converted to μmol/kg.\"\"\"\n",
    "    mlL = _o2sol_weiss_ml_per_L(T, S)\n",
    "    rho = _rho_eos80_kg_m3(S, T)            # kg/m^3\n",
    "    return mlL * 44.659 * (1000.0 / rho)    # μmol/kg\n",
    "\n",
    "# =========================\n",
    "# TEOS-10 (gsw) path\n",
    "# =========================\n",
    "def o2_sat_umolkg_teos10(SP, t, p, lon, lat):\n",
    "    \"\"\"\n",
    "    TEOS-10: O2 saturation solubility (μmol/kg, referenced to 0 dbar with pt0).\n",
    "    Needs SP, in-situ t, p(dbar), lon, lat.\n",
    "    \"\"\"\n",
    "    import gsw\n",
    "    SP = np.asarray(SP, float); t = np.asarray(t, float)\n",
    "    p = np.asarray(p, float); lon = np.asarray(lon, float); lat = np.asarray(lat, float)\n",
    "    SA  = gsw.SA_from_SP(SP, p, lon, lat)\n",
    "    pt0 = gsw.pt0_from_t(SA, t, p)\n",
    "    return gsw.O2sol_SP_pt(SP, pt0)  # μmol/kg\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def atomic_write_csv(df: pd.DataFrame, path: Path, encoding=WRITE_ENCODING):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df.to_csv(tmp, index=False, encoding=encoding)\n",
    "    tmp.replace(path)\n",
    "\n",
    "def parse_depth_from_folder(folder: Path):\n",
    "    m = re.match(r\"^(\\d+)dbar$\", folder.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def find_train_csvs(depth_folder: Path):\n",
    "    return sorted(depth_folder.glob(\"*TRAIN.csv\"))\n",
    "\n",
    "def compute_sat_percent(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute Sat(%) = 100 * Oxygen / O2_sat_umolkg.\n",
    "    If Temperature/Salinity missing -> Sat stays NaN.\n",
    "    TEOS-10 preferred; fallback Weiss.\n",
    "\n",
    "    Returns:\n",
    "      sat_percent (float array)\n",
    "      used_teos (bool array)\n",
    "      used_weiss (bool array)\n",
    "      o2sat (float array)\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    sat_percent = np.full(n, np.nan, dtype=float)\n",
    "    o2sat = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    T = pd.to_numeric(df.get(TEMP_COL, np.nan), errors=\"coerce\").to_numpy()\n",
    "    S = pd.to_numeric(df.get(SAL_COL,  np.nan), errors=\"coerce\").to_numpy()\n",
    "    O = pd.to_numeric(df.get(OXY_COL,  np.nan), errors=\"coerce\").to_numpy()\n",
    "\n",
    "    m_ts = np.isfinite(T) & np.isfinite(S)\n",
    "    used_teos = np.zeros(n, dtype=bool)\n",
    "    used_weiss = np.zeros(n, dtype=bool)\n",
    "\n",
    "    if not m_ts.any():\n",
    "        return sat_percent, used_teos, used_weiss, o2sat\n",
    "\n",
    "    pres = pd.to_numeric(df.get(\"Pressure\", np.nan), errors=\"coerce\").to_numpy()\n",
    "    lon  = pd.to_numeric(df.get(\"Longitude\", np.nan), errors=\"coerce\").to_numpy()\n",
    "    lat  = pd.to_numeric(df.get(\"Latitude\", np.nan), errors=\"coerce\").to_numpy()\n",
    "\n",
    "    # TEOS-10\n",
    "    try:\n",
    "        import gsw  # noqa: F401\n",
    "        m_teos = m_ts & np.isfinite(pres) & np.isfinite(lon) & np.isfinite(lat)\n",
    "        if m_teos.any():\n",
    "            o2sat[m_teos] = o2_sat_umolkg_teos10(S[m_teos], T[m_teos], pres[m_teos], lon[m_teos], lat[m_teos])\n",
    "            used_teos[m_teos] = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Weiss fallback\n",
    "    m_weiss = m_ts & ~np.isfinite(o2sat)\n",
    "    if m_weiss.any():\n",
    "        o2sat[m_weiss] = o2_sat_umolkg_weiss(T[m_weiss], S[m_weiss])\n",
    "        used_weiss[m_weiss] = True\n",
    "\n",
    "    # Sat(%)\n",
    "    m_sat = np.isfinite(O) & np.isfinite(o2sat) & (o2sat > 0)\n",
    "    sat_percent[m_sat] = 100.0 * (O[m_sat] / o2sat[m_sat])\n",
    "\n",
    "    return sat_percent, used_teos, used_weiss, o2sat\n",
    "\n",
    "def process_one_csv(csv_path: Path, depth_dbar: int):\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    n_in = int(len(df))\n",
    "\n",
    "    # default Sat NA if missing columns\n",
    "    if TEMP_COL not in df.columns or SAL_COL not in df.columns or OXY_COL not in df.columns:\n",
    "        df[SAT_COL] = pd.NA\n",
    "        n_out = int(len(df))\n",
    "        atomic_write_csv(df, csv_path)\n",
    "        return {\n",
    "            \"file\": str(csv_path),\n",
    "            \"depth_dbar\": depth_dbar,\n",
    "            \"rows_in\": n_in,\n",
    "            \"rows_out\": n_out,\n",
    "            \"sat_finite\": 0,\n",
    "            \"sat_ge120\": 0,\n",
    "            \"dropped_sat_ge120\": 0,\n",
    "            \"teos_used\": 0,\n",
    "            \"weiss_used\": 0,\n",
    "            \"note\": f\"missing required cols ({TEMP_COL}/{SAL_COL}/{OXY_COL}); Sat=NA; no drop\"\n",
    "        }\n",
    "\n",
    "    sat, used_teos, used_weiss, _ = compute_sat_percent(df)\n",
    "    df[SAT_COL] = sat  # overwrite/add\n",
    "\n",
    "    m_finite = np.isfinite(sat)\n",
    "    n_finite = int(m_finite.sum())\n",
    "    n_ge120 = int((m_finite & (sat >= SAT_DROP_THR)).sum())\n",
    "\n",
    "    dropped = 0\n",
    "    if depth_dbar is not None and depth_dbar > 200:\n",
    "        keep_mask = ~(m_finite & (sat >= SAT_DROP_THR))\n",
    "        dropped = int((~keep_mask).sum())\n",
    "        df = df.loc[keep_mask].copy()\n",
    "\n",
    "    n_out = int(len(df))\n",
    "    atomic_write_csv(df, csv_path)\n",
    "\n",
    "    return {\n",
    "        \"file\": str(csv_path),\n",
    "        \"depth_dbar\": depth_dbar,\n",
    "        \"rows_in\": n_in,\n",
    "        \"rows_out\": n_out,\n",
    "        \"sat_finite\": n_finite,\n",
    "        \"sat_ge120\": n_ge120,\n",
    "        \"dropped_sat_ge120\": dropped,\n",
    "        \"teos_used\": int(used_teos.sum()),\n",
    "        \"weiss_used\": int(used_weiss.sum()),\n",
    "        \"note\": \"ok\" if (depth_dbar is not None and depth_dbar > 200) else \"ok (depth<=200: no drop)\"\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logs = []\n",
    "\n",
    "    for d in TARGET_DEPTHS:\n",
    "        folder = ROOT / f\"{d}dbar\"\n",
    "        if not folder.exists():\n",
    "            logs.append({\n",
    "                \"file\": \"\",\n",
    "                \"depth_dbar\": d,\n",
    "                \"rows_in\": 0,\n",
    "                \"rows_out\": 0,\n",
    "                \"sat_finite\": 0,\n",
    "                \"sat_ge120\": 0,\n",
    "                \"dropped_sat_ge120\": 0,\n",
    "                \"teos_used\": 0,\n",
    "                \"weiss_used\": 0,\n",
    "                \"note\": f\"folder_not_found:{folder}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        depth_val = parse_depth_from_folder(folder)\n",
    "        files = find_train_csvs(folder)\n",
    "        if not files:\n",
    "            logs.append({\n",
    "                \"file\": \"\",\n",
    "                \"depth_dbar\": depth_val,\n",
    "                \"rows_in\": 0,\n",
    "                \"rows_out\": 0,\n",
    "                \"sat_finite\": 0,\n",
    "                \"sat_ge120\": 0,\n",
    "                \"dropped_sat_ge120\": 0,\n",
    "                \"teos_used\": 0,\n",
    "                \"weiss_used\": 0,\n",
    "                \"note\": f\"no_train_csv:{folder}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for fp in files:\n",
    "            logs.append(process_one_csv(fp, depth_val))\n",
    "\n",
    "    df_log = pd.DataFrame(logs)\n",
    "    log_path = LOG_DIR / \"sat_qc_drop_log.csv\"\n",
    "    df_log.to_csv(log_path, index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "    # summary\n",
    "    df_files = df_log[df_log[\"file\"].astype(str).str.len() > 0].copy()\n",
    "    df_deep = df_files[df_files[\"depth_dbar\"].astype(float) > 200].copy()\n",
    "\n",
    "    def summarize(df_in: pd.DataFrame, tag: str):\n",
    "        if df_in.empty:\n",
    "            return {\n",
    "                \"group\": tag,\n",
    "                \"n_files\": 0,\n",
    "                \"rows_in_total\": 0,\n",
    "                \"rows_out_total\": 0,\n",
    "                \"dropped_total\": 0,\n",
    "                \"sat_finite_total\": 0,\n",
    "                \"sat_ge120_total\": 0,\n",
    "                \"drop_ratio_over_rows_in\": np.nan,\n",
    "                \"ge120_ratio_over_sat_finite\": np.nan\n",
    "            }\n",
    "        rows_in_total = int(df_in[\"rows_in\"].sum())\n",
    "        rows_out_total = int(df_in[\"rows_out\"].sum())\n",
    "        dropped_total = int(df_in[\"dropped_sat_ge120\"].sum())\n",
    "        sat_finite_total = int(df_in[\"sat_finite\"].sum())\n",
    "        sat_ge120_total = int(df_in[\"sat_ge120\"].sum())\n",
    "        return {\n",
    "            \"group\": tag,\n",
    "            \"n_files\": int(len(df_in)),\n",
    "            \"rows_in_total\": rows_in_total,\n",
    "            \"rows_out_total\": rows_out_total,\n",
    "            \"dropped_total\": dropped_total,\n",
    "            \"sat_finite_total\": sat_finite_total,\n",
    "            \"sat_ge120_total\": sat_ge120_total,\n",
    "            \"drop_ratio_over_rows_in\": (dropped_total / rows_in_total) if rows_in_total else np.nan,\n",
    "            \"ge120_ratio_over_sat_finite\": (sat_ge120_total / sat_finite_total) if sat_finite_total else np.nan\n",
    "        }\n",
    "\n",
    "    df_sum = pd.DataFrame([\n",
    "        summarize(df_files, \"ALL_TARGET_FILES\"),\n",
    "        summarize(df_deep,  \"DEPTH_GT_200_ONLY\"),\n",
    "    ])\n",
    "    sum_path = LOG_DIR / \"sat_qc_drop_log_summary.csv\"\n",
    "    df_sum.to_csv(sum_path, index=False, encoding=WRITE_ENCODING)\n",
    "\n",
    "    deep = df_sum[df_sum[\"group\"] == \"DEPTH_GT_200_ONLY\"].iloc[0].to_dict()\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "    print(f\"  Per-file log : {log_path}\")\n",
    "    print(f\"  Summary log  : {sum_path}\")\n",
    "    print(\"  ---- DEPTH>200 (drop Sat>=120) ----\")\n",
    "    print(f\"  files={deep['n_files']}, rows_in_total={deep['rows_in_total']}, rows_out_total={deep['rows_out_total']}\")\n",
    "    print(f\"  dropped_total={deep['dropped_total']} (ratio={deep['drop_ratio_over_rows_in']})\")\n",
    "    print(f\"  sat_ge120_total={deep['sat_ge120_total']}, sat_finite_total={deep['sat_finite_total']}, \"\n",
    "          f\"ge120_ratio_over_sat_finite={deep['ge120_ratio_over_sat_finite']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
