{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TARGET_DEPTHS = [\n",
    "   10,20,30,40,50,60,70,80,90,100,\n",
    "\t    110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,\n",
    "\t    270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,\n",
    "\t    430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,\n",
    "\t    590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,\n",
    "\t    750,760,770,780,790,800,820,840,860,880,900,920,940,960,980,1000,\n",
    "\t    1020,1040,1060,1080,1100,1120,1140,1160,1180,1200,1220,1240,1260,\n",
    "\t    1280,1300,1320,1340,1360,1380,1400,1420,1440,1460,1480,1500,1520,\n",
    "\t    1540,1560,1580,1600,1620,1640,1660,1680,1700,1720,1740,1760,1780,\n",
    "\t    1800,1820,1840,1860,1880,1900,1920,1940,1960,1980,2000,2100,2200,\n",
    "\t    2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,\n",
    "\t    3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,\n",
    "\t    5000,5100,5200,5300,5400,5500\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import warnings\n",
    "from typing import Tuple, Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "# Optional progress bar (falls back to plain-text percentage if tqdm is not installed)\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    HAS_TQDM = True\n",
    "except Exception:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "# ===================== Configurable section =====================\n",
    "\n",
    "# Input base directory (subfolders by depth)\n",
    "ALLOXY_BASE = \"/data/wang/Result_Data/alldoxy\"\n",
    "\n",
    "# Only process these depth levels (CORA5.2 depth unit is meters; 1 ≈ 1 dbar)\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "# CMEMS data directories\n",
    "TEMP_DIR = \"/data/wang/CMEMS/TEMP\"\n",
    "PSAL_DIR = \"/data/wang/CMEMS/PSAL\"\n",
    "\n",
    "# Tolerances\n",
    "LATLON_TOL_DEG = 0.1     # nearest-neighbor lat/lon tolerance (degrees)\n",
    "DEPTH_TOL = 1.1          # nearest-neighbor depth tolerance (meters)\n",
    "\n",
    "# Write strategy\n",
    "INPLACE_UPDATE = True    # True: read/write the same depthX_TRAIN.csv (in-place update with atomic replace)\n",
    "OVERWRITE_TRAIN = True   # True: overwrite Temp/Sal; False: only fill Temp/Sal where they are NaN\n",
    "\n",
    "# Parallelism\n",
    "MAX_WORKERS = 48  # tune based on I/O / CPU\n",
    "# Prevent extra BLAS/OMP parallelism from oversubscribing cores\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "# After all group matches, optionally fill remaining NaN Temp/Sal from original columns (Temperature/Salinity -> Temp/Sal)\n",
    "FALLBACK_COPY_FROM_ORIGINAL = False\n",
    "\n",
    "# =====================================================\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -------------------- Utility functions -------------------- #\n",
    "\n",
    "def ensure_month_str(m):\n",
    "    m = str(m).zfill(2)\n",
    "    if len(m) != 2 or not m.isdigit():\n",
    "        raise ValueError(f\"Invalid month: {m}\")\n",
    "    return m\n",
    "\n",
    "def get_year_month_strs(year, month) -> Tuple[str, str]:\n",
    "    y = str(int(year))\n",
    "    m = ensure_month_str(month)\n",
    "    return y, m\n",
    "\n",
    "def nc_paths_for_year_month(year: str, month: str) -> Tuple[str, str]:\n",
    "    date_token = f\"{year}{month}15\"\n",
    "    temp_path = os.path.join(TEMP_DIR, f\"OA_CORA5.2_{date_token}_fld_TEMP.nc\")\n",
    "    psal_path = os.path.join(PSAL_DIR, f\"OA_CORA5.2_{date_token}_fld_PSAL.nc\")\n",
    "    return temp_path, psal_path\n",
    "\n",
    "def normalize_lon_to_grid(lon_vals: np.ndarray, grid_min: float, grid_max: float) -> np.ndarray:\n",
    "    lons = lon_vals.astype(float).copy()\n",
    "    # CORA5.2 longitudes are typically in [-180, 180)\n",
    "    if grid_min < 0 <= grid_max <= 180:\n",
    "        lons = ((lons + 180.0) % 360.0) - 180.0\n",
    "    elif 0 <= grid_min < 360:\n",
    "        lons = lons % 360.0\n",
    "    else:\n",
    "        span = grid_max - grid_min\n",
    "        lons = ((lons - grid_min) % span) + grid_min\n",
    "    return lons\n",
    "\n",
    "def nearest_indices_with_tol(grid: np.ndarray, values: np.ndarray, tol: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    grid = np.asarray(grid)\n",
    "    values = np.asarray(values)\n",
    "    reversed_grid = False\n",
    "    if grid[0] > grid[-1]:\n",
    "        grid = grid[::-1]\n",
    "        reversed_grid = True\n",
    "\n",
    "    pos = np.searchsorted(grid, values, side=\"left\")\n",
    "    pos0 = np.clip(pos - 1, 0, len(grid) - 1)\n",
    "    pos1 = np.clip(pos, 0, len(grid) - 1)\n",
    "\n",
    "    v0 = grid[pos0]\n",
    "    v1 = grid[pos1]\n",
    "    choose_left = np.abs(values - v0) <= np.abs(values - v1)\n",
    "    idx = np.where(choose_left, pos0, pos1)\n",
    "    nearest_val = np.where(choose_left, v0, v1)\n",
    "    abs_diff = np.abs(values - nearest_val)\n",
    "\n",
    "    if reversed_grid:\n",
    "        idx = (len(grid) - 1) - idx\n",
    "    return idx.astype(int), nearest_val, abs_diff\n",
    "\n",
    "def open_nc_pair(temp_path: str, psal_path: str) -> Tuple[xr.Dataset, xr.Dataset]:\n",
    "    if not os.path.exists(temp_path):\n",
    "        raise FileNotFoundError(f\"Temperature file not found: {temp_path}\")\n",
    "    if not os.path.exists(psal_path):\n",
    "        raise FileNotFoundError(f\"Salinity file not found: {psal_path}\")\n",
    "    # decode_cf=True applies scale_factor / add_offset automatically\n",
    "    ds_t = xr.open_dataset(temp_path, decode_cf=True)\n",
    "    ds_s = xr.open_dataset(psal_path, decode_cf=True)\n",
    "    return ds_t, ds_s\n",
    "\n",
    "def pick_depth_da(da: xr.DataArray, target_depth: float, tol: float) -> Tuple[xr.DataArray, float]:\n",
    "    if \"time\" in da.dims:\n",
    "        da = da.isel(time=0)\n",
    "    if \"depth\" not in da.dims:\n",
    "        raise ValueError(\"No 'depth' dimension found in the data array\")\n",
    "    picked = da.sel(depth=target_depth, method=\"nearest\", tolerance=tol)\n",
    "    actual_depth = float(picked.coords[\"depth\"].values)\n",
    "    if abs(actual_depth - target_depth) > tol:\n",
    "        raise KeyError(f\"Nearest depth {actual_depth} exceeds tolerance {tol} (target {target_depth})\")\n",
    "    return picked, actual_depth\n",
    "\n",
    "def extract_values_at_points(field2d: xr.DataArray, lat_vals: np.ndarray, lon_vals: np.ndarray,\n",
    "                             latlon_tol: float) -> np.ndarray:\n",
    "    lat_name = \"latitude\" if \"latitude\" in field2d.dims else (\"lat\" if \"lat\" in field2d.dims else None)\n",
    "    lon_name = \"longitude\" if \"longitude\" in field2d.dims else (\"lon\" if \"lon\" in field2d.dims else None)\n",
    "    if lat_name is None or lon_name is None:\n",
    "        raise ValueError(f\"Cannot identify lat/lon dimension names: {field2d.dims}\")\n",
    "\n",
    "    grid_lats = field2d.coords[lat_name].values\n",
    "    grid_lons = field2d.coords[lon_name].values\n",
    "\n",
    "    lon_vals_norm = normalize_lon_to_grid(lon_vals, float(grid_lons.min()), float(grid_lons.max()))\n",
    "    lat_idx, lat_nn, lat_diff = nearest_indices_with_tol(grid_lats, lat_vals, latlon_tol)\n",
    "    lon_idx, lon_nn, lon_diff = nearest_indices_with_tol(grid_lons, lon_vals_norm, latlon_tol)\n",
    "\n",
    "    ok = (lat_diff <= latlon_tol) & (lon_diff <= latlon_tol)\n",
    "    out = np.full(len(lat_vals), np.nan, dtype=float)\n",
    "    if np.any(ok):\n",
    "        f = field2d.values  # (nlat, nlon)\n",
    "        sel = np.where(ok)[0]\n",
    "        out[ok] = f[lat_idx[sel], lon_idx[sel]]\n",
    "    return out\n",
    "\n",
    "# -------------------- Group-parallel worker -------------------- #\n",
    "\n",
    "def worker_group(\n",
    "    year: str,\n",
    "    month: str,\n",
    "    depth_target: int,\n",
    "    lat_list: List[float],\n",
    "    lon_list: List[float],\n",
    "    row_index: List[int],\n",
    "    latlon_tol_deg: float,\n",
    "    depth_tol: float,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single (year, month) group:\n",
    "      - open TEMP/PSAL netCDF for that month\n",
    "      - select nearest depth (tolerance=depth_tol)\n",
    "      - extract nearest gridpoint values (tolerance=latlon_tol_deg)\n",
    "      - return {'index': row_index, 'Temp': ..., 'Sal': ..., 'msg': '...'}\n",
    "    \"\"\"\n",
    "    temp_nc, psal_nc = nc_paths_for_year_month(year, month)\n",
    "    n = len(row_index)\n",
    "    result = {\"index\": row_index, \"Temp\": [math.nan] * n, \"Sal\": [math.nan] * n, \"msg\": \"\"}\n",
    "\n",
    "    if not (os.path.exists(temp_nc) and os.path.exists(psal_nc)):\n",
    "        # Missing monthly nc: silently return NaNs (no per-group printing)\n",
    "        return result\n",
    "\n",
    "    ds_t, ds_s = None, None\n",
    "    try:\n",
    "        ds_t, ds_s = open_nc_pair(temp_nc, psal_nc)\n",
    "        t2d, actual_depth_t = pick_depth_da(ds_t[\"TEMP\"], depth_target, depth_tol)\n",
    "        s2d, actual_depth_s = pick_depth_da(ds_s[\"PSAL\"], actual_depth_t, depth_tol)\n",
    "\n",
    "        lat_arr = np.asarray(lat_list, dtype=float)\n",
    "        lon_arr = np.asarray(lon_list, dtype=float)\n",
    "        temp_vals = extract_values_at_points(t2d, lat_arr, lon_arr, latlon_tol_deg)\n",
    "        sal_vals  = extract_values_at_points(s2d, lat_arr, lon_arr, latlon_tol_deg)\n",
    "\n",
    "        result[\"Temp\"] = temp_vals.tolist()\n",
    "        result[\"Sal\"] = sal_vals.tolist()\n",
    "        # Do not print per-group OK messages\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Carry error message in result, no per-group printing\n",
    "        result[\"msg\"] = f\"[Exception] {year}-{month}: {e}\"\n",
    "        return result\n",
    "    finally:\n",
    "        if ds_t is not None: ds_t.close()\n",
    "        if ds_s is not None: ds_s.close()\n",
    "\n",
    "# -------------------- Per-depth processing (parallel by Year/Month) -------------------- #\n",
    "\n",
    "def process_depth(depth_target: int):\n",
    "    depth_dir = os.path.join(ALLOXY_BASE, f\"{int(depth_target)}dbar\")\n",
    "    print(f\"\\n===== Processing depth {depth_target} -> dir {depth_dir} =====\")\n",
    "\n",
    "    if not os.path.isdir(depth_dir):\n",
    "        print(f\"[Info] Directory does not exist, skipping depth: {depth_dir}\")\n",
    "        return\n",
    "\n",
    "    out_name = f\"depth{int(depth_target)}_TRAIN.csv\"\n",
    "    out_path = os.path.join(depth_dir, out_name)\n",
    "\n",
    "    # ——— Read input ———\n",
    "    if INPLACE_UPDATE:\n",
    "        if not os.path.exists(out_path):\n",
    "            print(f\"[Info] In-place update requested, but input file not found: {out_path}. Skipping this depth.\")\n",
    "            return\n",
    "        try:\n",
    "            df_all = pd.read_csv(out_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to read: {out_path} -> {e}\")\n",
    "            return\n",
    "        src_files = [out_name]\n",
    "    else:\n",
    "        src_files = sorted(\n",
    "            p for p in glob.glob(os.path.join(depth_dir, \"*_TRAIN.csv\"))\n",
    "            if os.path.basename(p) != out_name\n",
    "        )\n",
    "        if not src_files:\n",
    "            print(f\"[Info] No *_TRAIN.csv found in {depth_dir}, skipping.\")\n",
    "            return\n",
    "        frames = []\n",
    "        for p in src_files:\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to read, skipping: {p} -> {e}\")\n",
    "                continue\n",
    "            frames.append(df)\n",
    "        if not frames:\n",
    "            print(f\"[Info] No usable *_TRAIN.csv in {depth_dir}, skipping.\")\n",
    "            return\n",
    "        df_all = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "    # Required columns\n",
    "    required = {\"Year\", \"Month\", \"Latitude\", \"Longitude\"}\n",
    "    if not required.issubset(df_all.columns):\n",
    "        print(f\"[Error] Missing required columns {required}, skipping this depth.\")\n",
    "        return\n",
    "\n",
    "    # Ensure output columns exist\n",
    "    if \"Temp\" not in df_all.columns: df_all[\"Temp\"] = np.nan\n",
    "    if \"Sal\"  not in df_all.columns: df_all[\"Sal\"]  = np.nan\n",
    "\n",
    "    # ——— Parallel processing by (Year, Month) ———\n",
    "    groups = df_all.groupby([\"Year\", \"Month\"], dropna=False)\n",
    "    print(f\"Source files: {[os.path.basename(p) for p in src_files]}\")\n",
    "    print(f\"Number of groups: {len(groups)} (by Year, Month) -> parallel, MAX_WORKERS={MAX_WORKERS}\")\n",
    "\n",
    "    tasks = []\n",
    "    # Choose a usable multiprocessing start method (prefer fork)\n",
    "    try:\n",
    "        mp_ctx = multiprocessing.get_context(\"fork\")\n",
    "    except ValueError:\n",
    "        mp_ctx = multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS, mp_context=mp_ctx) as ex:\n",
    "        for (y_raw, m_raw), g in groups:\n",
    "            try:\n",
    "                year, month = get_year_month_strs(y_raw, m_raw)\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to parse Year/Month: ({y_raw}, {m_raw}) -> {e}. Skipping this group.\")\n",
    "                continue\n",
    "            fut = ex.submit(\n",
    "                worker_group,\n",
    "                year, month, int(depth_target),\n",
    "                g[\"Latitude\"].tolist(),\n",
    "                g[\"Longitude\"].tolist(),\n",
    "                g.index.astype(int).tolist(),\n",
    "                LATLON_TOL_DEG, DEPTH_TOL\n",
    "            )\n",
    "            tasks.append(fut)\n",
    "\n",
    "        total = len(tasks)\n",
    "        if total == 0:\n",
    "            print(\"[Info] No tasks to execute.\")\n",
    "        else:\n",
    "            # Progress handling: tqdm if available, else percentage; do not print per-group OK/errors\n",
    "            if HAS_TQDM:\n",
    "                with tqdm(total=total, unit=\"grp\", desc=f\"Depth {int(depth_target)}\", dynamic_ncols=True) as pbar:\n",
    "                    for fut in as_completed(tasks):\n",
    "                        res = fut.result()\n",
    "                        idx = res[\"index\"]\n",
    "                        new_temp = pd.Series(res[\"Temp\"], index=idx)\n",
    "                        new_sal  = pd.Series(res[\"Sal\"],  index=idx)\n",
    "                        if OVERWRITE_TRAIN:\n",
    "                            df_all.loc[idx, \"Temp\"] = new_temp\n",
    "                            df_all.loc[idx, \"Sal\"]  = new_sal\n",
    "                        else:\n",
    "                            m1 = df_all.loc[idx, \"Temp\"].isna()\n",
    "                            m2 = df_all.loc[idx, \"Sal\"].isna()\n",
    "                            df_all.loc[idx[m1], \"Temp\"] = new_temp[m1]\n",
    "                            df_all.loc[idx[m2], \"Sal\"]  = new_sal[m2]\n",
    "                        pbar.update(1)\n",
    "            else:\n",
    "                done = 0\n",
    "                print(f\"Depth {int(depth_target)} progress: 0/{total} (0%)\", end=\"\", flush=True)\n",
    "                for fut in as_completed(tasks):\n",
    "                    res = fut.result()\n",
    "                    idx = res[\"index\"]\n",
    "                    new_temp = pd.Series(res[\"Temp\"], index=idx)\n",
    "                    new_sal  = pd.Series(res[\"Sal\"],  index=idx)\n",
    "                    if OVERWRITE_TRAIN:\n",
    "                        df_all.loc[idx, \"Temp\"] = new_temp\n",
    "                        df_all.loc[idx, \"Sal\"]  = new_sal\n",
    "                    else:\n",
    "                        m1 = df_all.loc[idx, \"Temp\"].isna()\n",
    "                        m2 = df_all.loc[idx, \"Sal\"].isna()\n",
    "                        df_all.loc[idx[m1], \"Temp\"] = new_temp[m1]\n",
    "                        df_all.loc[idx[m2], \"Sal\"]  = new_sal[m2]\n",
    "                    done += 1\n",
    "                    pct = int(done * 100 / total) if total else 100\n",
    "                    print(f\"\\rDepth {int(depth_target)} progress: {done}/{total} ({pct}%)\", end=\"\", flush=True)\n",
    "                print()  # newline\n",
    "\n",
    "    # ——— Optional unified fallback fill for remaining NaNs (Temperature->Temp, Salinity->Sal) ———\n",
    "    if FALLBACK_COPY_FROM_ORIGINAL:\n",
    "        # Convert to numeric; non-numeric -> NaN (prevents incorrect fills)\n",
    "        df_all[\"Temp\"] = pd.to_numeric(df_all[\"Temp\"], errors=\"coerce\")\n",
    "        df_all[\"Sal\"]  = pd.to_numeric(df_all[\"Sal\"],  errors=\"coerce\")\n",
    "\n",
    "        # Temp fallback\n",
    "        if \"Temperature\" in df_all.columns:\n",
    "            src_temp = pd.to_numeric(df_all[\"Temperature\"], errors=\"coerce\")\n",
    "            mask_temp = df_all[\"Temp\"].isna() & src_temp.notna()\n",
    "            n_temp = int(mask_temp.sum())\n",
    "            if n_temp > 0:\n",
    "                df_all.loc[mask_temp, \"Temp\"] = src_temp[mask_temp]\n",
    "            print(f\"[Fallback] Temp filled for {n_temp} rows (from Temperature).\")\n",
    "        else:\n",
    "            print(\"[Info] No 'Temperature' column found; cannot fallback-fill Temp.\")\n",
    "\n",
    "        # Sal fallback\n",
    "        if \"Salinity\" in df_all.columns:\n",
    "            src_sal = pd.to_numeric(df_all[\"Salinity\"], errors=\"coerce\")\n",
    "            mask_sal = df_all[\"Sal\"].isna() & src_sal.notna()\n",
    "            n_sal = int(mask_sal.sum())\n",
    "            if n_sal > 0:\n",
    "                df_all.loc[mask_sal, \"Sal\"] = src_sal[mask_sal]\n",
    "            print(f\"[Fallback] Sal filled for {n_sal} rows (from Salinity).\")\n",
    "        else:\n",
    "            print(\"[Info] No 'Salinity' column found; cannot fallback-fill Sal.\")\n",
    "\n",
    "    # ——— Safe write-out (atomic replace for in-place update) ———\n",
    "    if INPLACE_UPDATE:\n",
    "        tmp_path = out_path + \".__tmp__\"\n",
    "        df_all.to_csv(tmp_path, index=False)\n",
    "        os.replace(tmp_path, out_path)   # atomic replace\n",
    "        print(f\"[Write] In-place update completed: {out_path} (atomic replace)\")\n",
    "    else:\n",
    "        if OVERWRITE_TRAIN or not os.path.exists(out_path):\n",
    "            df_all.to_csv(out_path, index=False)\n",
    "            print(f\"[Write] Overwrote {out_path}, total rows={len(df_all)}.\")\n",
    "        else:\n",
    "            old = pd.read_csv(out_path)\n",
    "\n",
    "            def make_key(df: pd.DataFrame) -> pd.Series:\n",
    "                return (\n",
    "                    df[\"Year\"].astype(str).str.zfill(4)\n",
    "                    + \"-\"\n",
    "                    + df[\"Month\"].astype(str).str.zfill(2)\n",
    "                    + \"-\"\n",
    "                    + df[\"Latitude\"].round(5).astype(str)\n",
    "                    + \"-\"\n",
    "                    + df[\"Longitude\"].round(5).astype(str)\n",
    "                )\n",
    "\n",
    "            old[\"_key_\"] = make_key(old).values\n",
    "            df_all[\"_key_\"] = make_key(df_all).values\n",
    "            old_idxed = old.set_index(\"_key_\", drop=False)\n",
    "            new_idxed = df_all.set_index(\"_key_\", drop=False)\n",
    "\n",
    "            for col in [\"Temp\", \"Sal\"]:\n",
    "                if col not in old_idxed.columns:\n",
    "                    old_idxed[col] = np.nan\n",
    "                src = new_idxed[col]\n",
    "                is_na = old_idxed[col].isna()\n",
    "                idx_inter = old_idxed.index.intersection(src.index)\n",
    "                old_idxed.loc[is_na & old_idxed.index.isin(idx_inter), col] = src[is_na & src.index.isin(idx_inter)]\n",
    "\n",
    "            only_new = new_idxed.index.difference(old_idxed.index)\n",
    "            combined = pd.concat([old_idxed, new_idxed.loc[only_new]], axis=0, ignore_index=False)\n",
    "            combined = combined.drop(columns=[\"_key_\"], errors=\"ignore\")\n",
    "            combined.to_csv(out_path, index=False)\n",
    "            print(f\"[Write] Incremental update completed: {out_path}, current rows={len(combined)}.\")\n",
    "\n",
    "def main():\n",
    "    if not DEPTHS:\n",
    "        print(\"[Error] DEPTHS is empty; no depth levels specified.\")\n",
    "        return\n",
    "    print(f\"Depth levels to process (in order): {DEPTHS}\")\n",
    "    for d in DEPTHS:\n",
    "        process_depth(d)\n",
    "    print(\"\\nAll processing completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Batch-add for multiple depth-level CSV files:\n",
    "- O2_sat (μmol/kg): oxygen solubility at surface equilibrium (0 dbar), column name configurable (COL_O2_SAT)\n",
    "\n",
    "Configurable temperature/salinity column names:\n",
    "- TempName = 'Temperature'\n",
    "- SalName  = 'Salinity'\n",
    "\n",
    "Priority:\n",
    "- Use TEOS-10 (gsw) when available: SP + pt0 -> O2sol_SP_pt (μmol/kg)\n",
    "Fallback:\n",
    "- If gsw is missing or required fields are unavailable, use Weiss (1970) (ml/L)\n",
    "  + EOS-80 density to convert to μmol/kg\n",
    "\n",
    "The script overwrites the original file. If MAKE_BACKUP=True, it writes a *.bak backup in the same directory.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")  # data root path\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "MAKE_BACKUP = False  # whether to create backup files\n",
    "N_PROCESSES = 24     # moderate parallelism to avoid I/O contention\n",
    "\n",
    "# ---------- New: output column name (configurable) ----------\n",
    "COL_O2_SAT = \"O2_sat\"  # oxygen solubility column name\n",
    "\n",
    "# ---------- New: temperature/salinity column names used for computation ----------\n",
    "TempName = \"Temp\"  # temperature column name\n",
    "SalName  = \"Sal\"   # salinity column name\n",
    "\n",
    "# Optional: original dissolved oxygen column name (kept as 'Oxygen' here)\n",
    "OXYGEN_COL = \"Oxygen\"\n",
    "\n",
    "# ---------- Fallback path: Weiss (1970) + EOS-80 density ----------\n",
    "def _rho_eos80_kg_m3(S, T):\n",
    "    \"\"\"EOS-80 density (kg/m^3), approximate at 0 dbar; S=PSU, T=°C (ITS-90).\"\"\"\n",
    "    S = np.asarray(S, float)\n",
    "    T = np.asarray(T, float)\n",
    "    rho_w = (999.842594 + 6.793952e-2*T - 9.095290e-3*T**2\n",
    "             + 1.001685e-4*T**3 - 1.120083e-6*T**4 + 6.536332e-9*T**5)\n",
    "    A = (0.824493 - 4.0899e-3*T + 7.6438e-5*T**2 - 8.2467e-7*T**3 + 5.3875e-9*T**4)\n",
    "    B = (-5.72466e-3 + 1.0227e-4*T - 1.6546e-6*T**2)\n",
    "    C = 4.8314e-4\n",
    "    return rho_w + A*S + B*(S**1.5) + C*(S**2)\n",
    "\n",
    "def _o2sol_weiss_ml_per_L(T, S):\n",
    "    \"\"\"Weiss (1970) O2 solubility (ml/L); T=°C (ITS-90). Internally converts to IPTS-68 and Kelvin.\"\"\"\n",
    "    T = np.asarray(T, float)\n",
    "    S = np.asarray(S, float)\n",
    "    Tk = T*1.00024 + 273.15  # ITS-90 -> IPTS-68, then to Kelvin\n",
    "    A1, A2, A3, A4 = -173.4292, 249.6339, 143.3483, -21.8492\n",
    "    B1, B2, B3 = -0.033096, 0.014259, -0.0017000\n",
    "    lnC = (A1 + A2*(100.0/Tk) + A3*np.log(Tk/100.0) + A4*(Tk/100.0)\n",
    "           + S*(B1 + B2*(Tk/100.0) + B3*(Tk/100.0)**2))\n",
    "    return np.exp(lnC)\n",
    "\n",
    "def o2_sat_umolkg_weiss(T, S):\n",
    "    \"\"\"Convert Weiss O2 solubility to μmol/kg.\"\"\"\n",
    "    mlL = _o2sol_weiss_ml_per_L(T, S)\n",
    "    rho = _rho_eos80_kg_m3(S, T)  # kg/m^3\n",
    "    return mlL * 44.659 * (1000.0 / rho)  # μmol/kg (1 mL(STP) O2 = 44.659 μmol)\n",
    "\n",
    "# ---------- Preferred path: TEOS-10 (gsw) ----------\n",
    "def o2_sat_umolkg_teos10(SP, t, p, lon, lat):\n",
    "    \"\"\"\n",
    "    TEOS-10: compute O2 solubility using potential temperature pt0 (μmol/kg, referenced to 0 dbar).\n",
    "    Requires: Practical Salinity (SP), in-situ T(°C, ITS-90), Pressure(dbar), lon, lat.\n",
    "    \"\"\"\n",
    "    import gsw\n",
    "    SP = np.asarray(SP, float)\n",
    "    t  = np.asarray(t, float)\n",
    "    p  = np.asarray(p, float)\n",
    "    lon = np.asarray(lon, float)\n",
    "    lat = np.asarray(lat, float)\n",
    "    SA  = gsw.SA_from_SP(SP, p, lon, lat)\n",
    "    pt0 = gsw.pt0_from_t(SA, t, p)\n",
    "    return gsw.O2sol_SP_pt(SP, pt0)  # μmol/kg\n",
    "\n",
    "# ---------- Process a single depth ----------\n",
    "def process_single_depth(depth):\n",
    "    \"\"\"\n",
    "    Process one depth level:\n",
    "      Input:  /data/wang/Result_Data/alldoxy/{depth}dbar/depth{depth}_TRAIN.csv\n",
    "      Output: overwrite the same file with a new COL_O2_SAT column; optionally create a .bak file\n",
    "\n",
    "    Returns: (depth, n_rows, n_teos, n_weiss, msg)\n",
    "    \"\"\"\n",
    "    dir_path = DOXY_BASE / f\"{depth}dbar\"\n",
    "    file_path = dir_path / f\"depth{depth}_TRAIN.csv\"\n",
    "    if not file_path.exists():\n",
    "        return (depth, 0, 0, 0, f\"File not found: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        return (depth, 0, 0, 0, f\"Read failed: {e}\")\n",
    "\n",
    "    n0 = len(df)\n",
    "    if n0 == 0:\n",
    "        return (depth, 0, 0, 0, \"Empty file, skipped\")\n",
    "\n",
    "    # Validate required columns for both paths (minimum shared set)\n",
    "    if not {TempName, SalName}.issubset(df.columns):\n",
    "        missing = {TempName, SalName} - set(df.columns)\n",
    "        return (depth, n0, 0, 0, f\"Missing columns: {missing}\")\n",
    "\n",
    "    O2_sat_arr = np.full(n0, np.nan, dtype=float)\n",
    "    used_teos = 0\n",
    "    used_weiss = 0\n",
    "\n",
    "    # --- Prefer TEOS-10 when possible (only rows with all required fields present) ---\n",
    "    try:\n",
    "        import gsw  # noqa: F401\n",
    "        pres_series = df.get(\"Pressure\", pd.Series(np.nan, index=df.index))\n",
    "        lon_series  = df.get(\"Longitude\", pd.Series(np.nan, index=df.index))\n",
    "        lat_series  = df.get(\"Latitude\", pd.Series(np.nan, index=df.index))\n",
    "\n",
    "        mask_teos = (\n",
    "            df[SalName].notna() &\n",
    "            df[TempName].notna() &\n",
    "            pres_series.notna() &\n",
    "            lon_series.notna() &\n",
    "            lat_series.notna()\n",
    "        ).to_numpy()\n",
    "\n",
    "        if mask_teos.any():\n",
    "            O2_sat_arr[mask_teos] = o2_sat_umolkg_teos10(\n",
    "                df.loc[mask_teos, SalName].to_numpy(dtype=float),\n",
    "                df.loc[mask_teos, TempName].to_numpy(dtype=float),\n",
    "                df.loc[mask_teos, \"Pressure\"].to_numpy(dtype=float),\n",
    "                df.loc[mask_teos, \"Longitude\"].to_numpy(dtype=float),\n",
    "                df.loc[mask_teos, \"Latitude\"].to_numpy(dtype=float),\n",
    "            )\n",
    "            used_teos = int(mask_teos.sum())\n",
    "    except Exception:\n",
    "        # If TEOS-10 fails, continue with Weiss fallback\n",
    "        pass\n",
    "\n",
    "    # --- Weiss fallback (rows still NaN but have T/S) ---\n",
    "    mask_weiss = np.isnan(O2_sat_arr) & df[TempName].notna() & df[SalName].notna()\n",
    "    if mask_weiss.any():\n",
    "        O2_sat_arr[mask_weiss] = o2_sat_umolkg_weiss(\n",
    "            df.loc[mask_weiss, TempName].to_numpy(dtype=float),\n",
    "            df.loc[mask_weiss, SalName].to_numpy(dtype=float),\n",
    "        )\n",
    "        used_weiss = int(mask_weiss.sum())\n",
    "\n",
    "    # Write back to DataFrame (only the new O2_sat column)\n",
    "    df[COL_O2_SAT] = O2_sat_arr\n",
    "\n",
    "    # Diagnostics\n",
    "    finite = np.isfinite(O2_sat_arr)\n",
    "    mean_sat = float(np.nanmean(O2_sat_arr)) if finite.any() else np.nan\n",
    "    std_sat  = float(np.nanstd(O2_sat_arr)) if finite.any() else np.nan\n",
    "    msg_stats = f\"{COL_O2_SAT} finite={int(finite.sum())}/{n0}, mean={mean_sat:.2f}, std={std_sat:.2f} μmol/kg\"\n",
    "\n",
    "    # Backup + overwrite\n",
    "    try:\n",
    "        if MAKE_BACKUP:\n",
    "            bak_path = str(file_path) + \".bak\"\n",
    "            shutil.copy2(file_path, bak_path)\n",
    "        df.to_csv(file_path, index=False)\n",
    "    except Exception as e:\n",
    "        return (depth, n0, used_teos, used_weiss, f\"Write failed: {e}\")\n",
    "\n",
    "    # Summary message\n",
    "    route_note = []\n",
    "    if used_teos > 0:\n",
    "        route_note.append(f\"TEOS-10={used_teos}\")\n",
    "    if used_weiss > 0:\n",
    "        route_note.append(f\"Weiss={used_weiss}\")\n",
    "    if not route_note:\n",
    "        route_note.append(\"No valid rows computed\")\n",
    "\n",
    "    return (depth, n0, used_teos, used_weiss, f\"{', '.join(route_note)} | {msg_stats}\")\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    tasks = list(DEPTHS)\n",
    "    if not tasks:\n",
    "        print(\"DEPTHS is not specified; exiting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Start processing depth levels: {tasks}\")\n",
    "\n",
    "    # Parallelize across depths (I/O-bound; moderate parallelism is recommended)\n",
    "    if len(tasks) == 1 or N_PROCESSES == 1:\n",
    "        results = [process_single_depth(d) for d in tasks]\n",
    "    else:\n",
    "        with Pool(processes=N_PROCESSES) as pool:\n",
    "            results = pool.map(process_single_depth, tasks)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n--- Batch results ---\")\n",
    "    for depth, n0, n_teos, n_weiss, msg in results:\n",
    "        print(f\"[{depth} dbar] rows={n0} | {msg}\")\n",
    "\n",
    "    print(\"\\n✅ All depths finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026 MLD-only\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Only extract MLD (mlotst) from CMEMS ARMOR3D monthly files and merge into DOXY TRAIN CSVs.\n",
    "\n",
    "Optimizations:\n",
    "  - Vectorized nearest-grid matching using np.searchsorted (O(N log M), no per-row argmin)\n",
    "  - Vectorized extraction of mlotst via NumPy advanced indexing\n",
    "  - Safer engine fallback for xr.open_dataset\n",
    "  - Keep multiprocessing but use a moderate process count to avoid I/O thrashing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "CMEMS_DIR = Path(\"/data/wang/CMEMS/MLDuv/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LATLON_TOLERANCE = 0.4  # degree\n",
    "# dbar folders / filenames remain as you use\n",
    "NPROC = min(16, (os.cpu_count() or 16))          # I/O-bound; too many processes are often slower\n",
    "IMAP_CHUNKSIZE = 6                                # reduce overhead; tune for your data\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(output_dir: Path) -> logging.Logger:\n",
    "    \"\"\"Multiprocess-friendly logger setup with directory creation.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger = logging.getLogger(f\"merge_mld_{os.getpid()}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False\n",
    "\n",
    "    # Remove existing handlers to avoid duplicate logs\n",
    "    if logger.handlers:\n",
    "        for h in list(logger.handlers):\n",
    "            logger.removeHandler(h)\n",
    "\n",
    "    pid = os.getpid()\n",
    "    log_file = output_dir / f\"merge_mld_{pid}.log\"\n",
    "\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\"))\n",
    "\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Utilities: open dataset with engine fallback ----------------------\n",
    "def open_dataset_safe(nc_path: Path):\n",
    "    \"\"\"\n",
    "    ARMOR3D files may fail with the default engine due to compression/format edge cases.\n",
    "    Try multiple engines as a fallback strategy.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for eng in [\"netcdf4\", \"h5netcdf\", None]:\n",
    "        try:\n",
    "            if eng is None:\n",
    "                return xr.open_dataset(nc_path, decode_times=False)\n",
    "            return xr.open_dataset(nc_path, decode_times=False, engine=eng)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise last_err\n",
    "\n",
    "# ---------------------- Vectorized nearest index on monotonic 1D axis ----------------------\n",
    "def nearest_index_1d(sorted_axis: np.ndarray, values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For a strictly increasing coordinate sorted_axis, return the nearest grid index for each value.\n",
    "    Uses searchsorted and compares neighbors; much faster than per-point argmin.\n",
    "    \"\"\"\n",
    "    a = np.asarray(sorted_axis, dtype=np.float64)\n",
    "    v = np.asarray(values, dtype=np.float64)\n",
    "\n",
    "    idx = np.searchsorted(a, v, side=\"left\")\n",
    "    idx = np.clip(idx, 0, len(a) - 1)\n",
    "\n",
    "    idx0 = np.clip(idx - 1, 0, len(a) - 1)\n",
    "    idx1 = idx\n",
    "\n",
    "    d0 = np.abs(a[idx0] - v)\n",
    "    d1 = np.abs(a[idx1] - v)\n",
    "\n",
    "    out = np.where(d0 <= d1, idx0, idx1).astype(np.int64)\n",
    "    return out\n",
    "\n",
    "def normalize_lon_0_360(lon_deg: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize longitude to [0, 360).\"\"\"\n",
    "    lon = np.asarray(lon_deg, dtype=np.float64)\n",
    "    return np.mod(lon, 360.0)\n",
    "\n",
    "def lon_0_360_to_pm180(lon_0_360: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert longitude from [0, 360) to (-180, 180].\"\"\"\n",
    "    lon = np.asarray(lon_0_360, dtype=np.float64)\n",
    "    return np.where(lon <= 180.0, lon, lon - 360.0)\n",
    "\n",
    "# ---------------------- Core: process one (Year, Month) group (vectorized) ----------------------\n",
    "def process_group(year: int, month: int, group_df: pd.DataFrame, target_depth: int, logger: logging.Logger) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract MLD only:\n",
    "      - Vectorized nearest-grid matching for all points in group_df\n",
    "      - Apply lat/lon tolerance filtering\n",
    "      - Extract mlotst values and write to a new 'MLD' column\n",
    "    \"\"\"\n",
    "    pattern = f\"dataset-armor-3d-rep-monthly_{year}{month:02d}15T1200Z_*.nc\"\n",
    "    nc_files = list(CMEMS_DIR.glob(pattern))\n",
    "    if not nc_files:\n",
    "        # If file does not exist, return NaNs\n",
    "        out = group_df.copy()\n",
    "        out[\"MLD\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    nc_path = nc_files[0]\n",
    "    try:\n",
    "        with open_dataset_safe(nc_path) as ds:\n",
    "            # Coordinate name compatibility\n",
    "            lat_name = \"latitude\" if \"latitude\" in ds.coords else (\"lat\" if \"lat\" in ds.coords else None)\n",
    "            lon_name = \"longitude\" if \"longitude\" in ds.coords else (\"lon\" if \"lon\" in ds.coords else None)\n",
    "            if lat_name is None or lon_name is None:\n",
    "                raise KeyError(f\"Cannot find lat/lon coords in {nc_path.name}. coords={list(ds.coords)}\")\n",
    "\n",
    "            if \"mlotst\" not in ds.variables:\n",
    "                raise KeyError(f\"'mlotst' not found in {nc_path.name}. vars={list(ds.variables)}\")\n",
    "\n",
    "            lat_axis = ds[lat_name].values.astype(np.float64)\n",
    "            lon_axis = ds[lon_name].values.astype(np.float64)\n",
    "\n",
    "            # Ensure coordinates are increasing; if not, sort and reorder data accordingly\n",
    "            lat_inc = np.all(np.diff(lat_axis) > 0)\n",
    "            lon_inc = np.all(np.diff(lon_axis) > 0)\n",
    "            da_mld = ds[\"mlotst\"].isel(time=0) if \"time\" in ds[\"mlotst\"].dims else ds[\"mlotst\"]\n",
    "\n",
    "            if not lat_inc:\n",
    "                lat_order = np.argsort(lat_axis)\n",
    "                lat_axis = lat_axis[lat_order]\n",
    "                da_mld = da_mld.isel({lat_name: lat_order})\n",
    "\n",
    "            if not lon_inc:\n",
    "                lon_order = np.argsort(lon_axis)\n",
    "                lon_axis = lon_axis[lon_order]\n",
    "                da_mld = da_mld.isel({lon_name: lon_order})\n",
    "\n",
    "            # Extract lat/lon arrays from the group (vectorized)\n",
    "            lats = group_df[\"Latitude\"].to_numpy(dtype=np.float64)\n",
    "            lons_raw = group_df[\"Longitude\"].to_numpy(dtype=np.float64)\n",
    "\n",
    "            # Dataset longitude is usually 0..360; convert observations to 0..360 for indexing\n",
    "            lons_nc = normalize_lon_0_360(lons_raw)\n",
    "\n",
    "            # Nearest indices (vectorized)\n",
    "            lat_idx = nearest_index_1d(lat_axis, lats)\n",
    "            lon_idx = nearest_index_1d(lon_axis, lons_nc)\n",
    "\n",
    "            # Compute tolerance using lon in (-180..180] to avoid artificial wrap-around differences\n",
    "            actual_lat = lat_axis[lat_idx]\n",
    "            actual_lon_nc = lon_axis[lon_idx]\n",
    "            actual_lon = lon_0_360_to_pm180(actual_lon_nc)\n",
    "\n",
    "            lat_gap = np.abs(actual_lat - lats)\n",
    "            lon_gap = np.abs(actual_lon - lons_raw)\n",
    "\n",
    "            ok = (lat_gap <= LATLON_TOLERANCE) & (lon_gap <= LATLON_TOLERANCE)\n",
    "\n",
    "            # Vectorized MLD extraction (invalid points stay NaN)\n",
    "            # da_mld dims: (latitude, longitude) or (lat, lon)\n",
    "            mld_arr = da_mld.values  # 2D numpy\n",
    "            mld_out = np.full(len(group_df), np.nan, dtype=np.float64)\n",
    "            if np.any(ok):\n",
    "                mld_vals = mld_arr[lat_idx[ok], lon_idx[ok]].astype(np.float64)\n",
    "                mld_out[ok] = mld_vals\n",
    "\n",
    "            out = group_df.copy()\n",
    "            out[\"MLD\"] = np.round(mld_out, 3)\n",
    "            return out\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed {year}-{month:02d} file={nc_path.name}: {e}\", exc_info=True)\n",
    "        out = group_df.copy()\n",
    "        out[\"MLD\"] = np.nan\n",
    "        return out\n",
    "\n",
    "# ---------------------- Multiprocessing wrapper ----------------------\n",
    "def process_group_wrapper(args):\n",
    "    year, month, group_df, target_depth = args\n",
    "    logger = setup_logger(DOXY_BASE / \"logs\")\n",
    "    return process_group(year, month, group_df, target_depth, logger)\n",
    "\n",
    "# ---------------------- Per-depth processing ----------------------\n",
    "def process_single_depth(target_depth: int):\n",
    "    log_dir = DOXY_BASE / \"logs\"\n",
    "    logger = setup_logger(log_dir)\n",
    "\n",
    "    input_dir = DOXY_BASE / f\"{target_depth}dbar\"\n",
    "    output_dir = input_dir\n",
    "\n",
    "    try:\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        input_csv = input_dir / f\"depth{target_depth}_TRAIN.csv\"\n",
    "        output_csv = output_dir / f\"depth{target_depth}_TRAIN.csv\"\n",
    "\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file missing: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        df = pd.read_csv(input_csv)\n",
    "        if df.empty:\n",
    "            logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        # Basic required column check\n",
    "        need_cols = {\"Year\", \"Month\", \"Latitude\", \"Longitude\"}\n",
    "        miss = need_cols - set(df.columns)\n",
    "        if miss:\n",
    "            raise ValueError(f\"Missing required columns: {miss}\")\n",
    "\n",
    "        # Group tasks by (Year, Month)\n",
    "        groups = df.groupby([\"Year\", \"Month\"], group_keys=False, sort=False)\n",
    "        tasks = [(int(year), int(month), group.copy(), target_depth) for (year, month), group in groups]\n",
    "\n",
    "        logger.info(f\"Depth {target_depth}dbar: groups={len(tasks)}, NPROC={NPROC}\")\n",
    "\n",
    "        processed = []\n",
    "        # For I/O-bound workloads, too many processes can degrade performance\n",
    "        with Pool(processes=NPROC) as pool:\n",
    "            with tqdm(total=len(tasks), desc=f\"Processing {target_depth}dbar (MLD)\", unit=\"group\") as pbar:\n",
    "                for res in pool.imap_unordered(process_group_wrapper, tasks, chunksize=IMAP_CHUNKSIZE):\n",
    "                    processed.append(res)\n",
    "                    pbar.update(1)\n",
    "\n",
    "        final_df = pd.concat(processed, ignore_index=True)\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        logger.info(f\"Done depth={target_depth}dbar -> {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Depth {target_depth}dbar failed: {e}\", exc_info=True)\n",
    "\n",
    "# ---------------------- Main loop ----------------------\n",
    "def main_process():\n",
    "    log_dir = DOXY_BASE / \"logs\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for target_depth in TARGET_DEPTHS:\n",
    "        process_single_depth(int(target_depth))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_process()\n",
    "    print(\"Batch processing completed. Please check the output directories for each depth level.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alldoxy\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "CCMP_DIR = Path(\"/data/wang/NASA/Wind/Monthly/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "OUTPUT_SUFFIX = \"_TRAIN\"\n",
    "LAT_TOLERANCE = 0.4  # latitude tolerance\n",
    "LON_TOLERANCE = 0.2  # longitude tolerance\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "POOL_SIZE = 48  # number of parallel processes\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_file):\n",
    "    \"\"\"Create an independent logger for each process.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s: %(message)s'))\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Core functions ----------------------\n",
    "def build_ccmp_index():\n",
    "    \"\"\"Build an index dict for CCMP wind files keyed by (year, month).\"\"\"\n",
    "    index = {}\n",
    "    for nc_file in CCMP_DIR.glob(\"CCMP_Wind_Analysis_*_monthly_mean_V03.1_L4.nc\"):\n",
    "        try:\n",
    "            parts = nc_file.stem.split(\"_\")\n",
    "            yyyymm = parts[3]\n",
    "            year = int(yyyymm[:4])\n",
    "            month = int(yyyymm[4:6])\n",
    "            index[(year, month)] = nc_file\n",
    "        except Exception as e:\n",
    "            print(f\"Filename parse error {nc_file}: {str(e)}\")\n",
    "    return index\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    \"\"\"Fast nearest-index lookup.\"\"\"\n",
    "    return np.abs(array - value).argmin()\n",
    "\n",
    "def process_group(args):\n",
    "    \"\"\"Process a single (year, month) group.\"\"\"\n",
    "    (year, month), group, nc_file, depth = args\n",
    "\n",
    "    logger = setup_logger(\n",
    "        f\"process_{os.getpid()}\",\n",
    "        DOXY_BASE / \"logs\" / f\"depth{depth}_{year}_{month}.log\"\n",
    "    )\n",
    "\n",
    "    result_df = group.copy()\n",
    "    result_df['U'] = np.nan\n",
    "    result_df['V'] = np.nan\n",
    "    result_df['W'] = np.nan\n",
    "\n",
    "    try:\n",
    "        if nc_file is None or not nc_file.exists():\n",
    "            # logger.warning(f\"Wind data missing: {year}-{month}\")\n",
    "            return result_df\n",
    "\n",
    "        with xr.open_dataset(nc_file) as ds:\n",
    "            lon_array = ds.longitude.values\n",
    "            lat_array = ds.latitude.values\n",
    "            u_data = ds['u'].isel(time=0).values\n",
    "            v_data = ds['v'].isel(time=0).values\n",
    "            w_data = ds['w'].isel(time=0).values\n",
    "\n",
    "            for idx, row in group.iterrows():\n",
    "                try:\n",
    "                    orig_lon = row['Longitude']\n",
    "                    target_lon = orig_lon % 360\n",
    "                    target_lat = row['Latitude']\n",
    "\n",
    "                    lon_idx = find_nearest_idx(lon_array, target_lon)\n",
    "                    lat_idx = find_nearest_idx(lat_array, target_lat)\n",
    "\n",
    "                    actual_lon = lon_array[lon_idx]\n",
    "                    actual_lat = lat_array[lat_idx]\n",
    "\n",
    "                    if (abs(actual_lon - target_lon) > LON_TOLERANCE or\n",
    "                        abs(actual_lat - target_lat) > LAT_TOLERANCE):\n",
    "                        continue\n",
    "\n",
    "                    result_df.at[idx, 'U'] = round(float(u_data[lat_idx, lon_idx]), 3)\n",
    "                    result_df.at[idx, 'V'] = round(float(v_data[lat_idx, lon_idx]), 3)\n",
    "                    result_df.at[idx, 'W'] = round(float(w_data[lat_idx, lon_idx]), 3)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Row {idx} processing error: {str(e)}\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Group processing failed {year}-{month}: {str(e)}\")\n",
    "        return result_df\n",
    "\n",
    "# ---------------------- Main workflow ----------------------\n",
    "def process_single_depth(target_depth, ccmp_index):\n",
    "    \"\"\"Process a single depth level.\"\"\"\n",
    "    logger = setup_logger(\n",
    "        f\"depth{target_depth}\",\n",
    "        DOXY_BASE / \"logs\" / f\"depth{target_depth}.log\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        input_csv = DOXY_BASE / f\"{target_depth}dbar\" / f\"depth{target_depth}_TRAIN.csv\"\n",
    "        output_csv = DOXY_BASE / f\"{target_depth}dbar\" / f\"depth{target_depth}{OUTPUT_SUFFIX}.csv\"\n",
    "\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file not found: {input_csv}\")\n",
    "            return\n",
    "        if os.path.getsize(input_csv) == 0:\n",
    "            logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Prepare multiprocessing task arguments\n",
    "        task_args = []\n",
    "        for (year, month), group in df.groupby(['Year', 'Month']):\n",
    "            nc_file = ccmp_index.get((year, month))\n",
    "            task_args.append(((year, month), group, nc_file, target_depth))\n",
    "\n",
    "        # Parallel processing over groups\n",
    "        with Pool(processes=POOL_SIZE) as pool:\n",
    "            chunks = [chunk for chunk in tqdm(\n",
    "                pool.imap(process_group, task_args),\n",
    "                total=len(task_args),\n",
    "                desc=f\"Depth {target_depth}dbar progress\"\n",
    "            )]\n",
    "\n",
    "        # Merge outputs\n",
    "        final_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "        # Save result\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        logger.info(f\"Saved: {output_csv} ({len(final_df)} rows)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing error: {str(e)}\", exc_info=True)\n",
    "\n",
    "def main():\n",
    "    # Initialize log directory\n",
    "    log_dir = DOXY_BASE / \"logs\"\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    main_logger = setup_logger(\"main\", log_dir / \"main.log\")\n",
    "    main_logger.info(\"Program started\")\n",
    "\n",
    "    # Build CCMP wind index\n",
    "    ccmp_index = build_ccmp_index()\n",
    "    main_logger.info(f\"Indexed {len(ccmp_index)} wind files\")\n",
    "\n",
    "    # Process depth levels sequentially\n",
    "    for depth in DEPTHS:\n",
    "        main_logger.info(f\"Start processing {depth}dbar\")\n",
    "        process_single_depth(depth, ccmp_index)\n",
    "\n",
    "    main_logger.info(\"All processing completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Done! Please check the output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "AVISO_DIR = Path(\"/data/wang/AVISO/madt_h/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LAT_TOLERANCE = 0.4  # latitude tolerance\n",
    "LON_TOLERANCE = 0.2  # longitude tolerance\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_file):\n",
    "    \"\"\"Create an independent logger for each process.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Avoid adding duplicate handlers\n",
    "    if not logger.handlers:\n",
    "        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\")\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Core utilities ----------------------\n",
    "def build_aviso_index():\n",
    "    \"\"\"Build an index dict for AVISO files keyed by (year, month).\"\"\"\n",
    "    index = {}\n",
    "    for nc_file in AVISO_DIR.glob(\"dt_global_allsat_madt_h_*.nc\"):\n",
    "        try:\n",
    "            # Parse filename to extract year/month\n",
    "            filename = nc_file.stem\n",
    "            parts = filename.split(\"_\")\n",
    "            year = int(parts[-2][1:5])\n",
    "            month = int(parts[-1][1:3])\n",
    "            index[(year, month)] = nc_file\n",
    "        except Exception as e:\n",
    "            print(f\"Filename parse error {nc_file}: {str(e)}\")\n",
    "    return index\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    \"\"\"Fast nearest-index lookup.\"\"\"\n",
    "    return np.abs(array - value).argmin()\n",
    "\n",
    "def process_group(args):\n",
    "    \"\"\"Process a single (year, month) group (multiprocessing worker).\"\"\"\n",
    "    (year, month), group, nc_file, log_file = args\n",
    "    logger = setup_logger(f\"process_{os.getpid()}\", log_file)\n",
    "\n",
    "    result_df = group.copy()\n",
    "    result_df[\"SSH\"] = np.nan\n",
    "\n",
    "    try:\n",
    "        if nc_file is None or not nc_file.exists():\n",
    "            # logger.warning(f\"SSH data missing: {year}-{month}\")\n",
    "            return result_df\n",
    "\n",
    "        with xr.open_dataset(nc_file) as ds:\n",
    "            lon_array = ds.longitude.values\n",
    "            lat_array = ds.latitude.values\n",
    "            ssh_data = ds[\"adt\"].isel(time=0).values\n",
    "\n",
    "            for idx, row in group.iterrows():\n",
    "                try:\n",
    "                    orig_lon = row[\"Longitude\"]\n",
    "                    target_lon = orig_lon % 360\n",
    "                    target_lat = row[\"Latitude\"]\n",
    "\n",
    "                    lon_idx = find_nearest_idx(lon_array, target_lon)\n",
    "                    lat_idx = find_nearest_idx(lat_array, target_lat)\n",
    "\n",
    "                    actual_lon = lon_array[lon_idx]\n",
    "                    actual_lat = lat_array[lat_idx]\n",
    "\n",
    "                    if (abs(actual_lon - target_lon) > LON_TOLERANCE or\n",
    "                        abs(actual_lat - target_lat) > LAT_TOLERANCE):\n",
    "                        continue\n",
    "\n",
    "                    result_df.at[idx, \"SSH\"] = float(ssh_data[lat_idx, lon_idx])\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Row {idx} processing error: {str(e)}\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Group processing failed {year}-{month}: {str(e)}\")\n",
    "        return result_df\n",
    "\n",
    "# ---------------------- Main processing ----------------------\n",
    "def process_single_depth(target_depth, aviso_index):\n",
    "    \"\"\"Process one depth level.\"\"\"\n",
    "    log_dir = DOXY_BASE / \"logs\"\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    logger = setup_logger(f\"depth{target_depth}\", log_dir / f\"depth{target_depth}.log\")\n",
    "\n",
    "    try:\n",
    "        input_dir = DOXY_BASE / f\"{target_depth}dbar\"\n",
    "        input_csv = input_dir / f\"depth{target_depth}_TRAIN.csv\"\n",
    "\n",
    "        # Validate input file\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file not found: {input_csv}\")\n",
    "            return\n",
    "        if os.path.getsize(input_csv) == 0:\n",
    "            logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        # Read original data\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Prepare multiprocessing task arguments\n",
    "        task_args = []\n",
    "        for (year, month), group in df.groupby([\"Year\", \"Month\"]):\n",
    "            nc_file = aviso_index.get((year, month))\n",
    "            task_args.append((\n",
    "                (year, month),\n",
    "                group,\n",
    "                nc_file,\n",
    "                log_dir / f\"process_{year}_{month}.log\"\n",
    "            ))\n",
    "\n",
    "        # Parallel processing by groups\n",
    "        results = []\n",
    "        if task_args:\n",
    "            with Pool(processes=48) as pool:\n",
    "                with tqdm(total=len(task_args), desc=f\"Depth {target_depth} dbar\") as pbar:\n",
    "                    for result in pool.imap(process_group, task_args):\n",
    "                        results.append(result)\n",
    "                        pbar.update()\n",
    "\n",
    "            # Merge outputs and set precision\n",
    "            final_df = pd.concat(results, ignore_index=True)\n",
    "            final_df[\"SSH\"] = final_df[\"SSH\"].round(3)  # keep 3 decimals for the new variable\n",
    "\n",
    "            # Overwrite the original file\n",
    "            final_df.to_csv(input_csv, index=False)\n",
    "            logger.info(f\"Updated file: {input_csv} ({len(final_df)} rows)\")\n",
    "        else:\n",
    "            logger.warning(\"No valid tasks to process\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing error: {str(e)}\", exc_info=True)\n",
    "\n",
    "# ---------------------- Driver ----------------------\n",
    "def main():\n",
    "    # Initialize global logger\n",
    "    main_logger = setup_logger(\"main\", DOXY_BASE / \"logs/main.log\")\n",
    "\n",
    "    # Build file index\n",
    "    aviso_index = build_aviso_index()\n",
    "    main_logger.info(f\"Indexed {len(aviso_index)} SSH files\")\n",
    "\n",
    "    # Process in depth order\n",
    "    for depth in DEPTHS:\n",
    "        main_logger.info(f\"Start processing depth: {depth} dbar\")\n",
    "        process_single_depth(depth, aviso_index)\n",
    "\n",
    "    main_logger.info(\"All processing finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Done! Please verify that the original files have been updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "aviso_DIR = Path(\"/data/wang/AVISO/eke/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LAT_TOLERANCE = 0.4  # latitude tolerance (degrees)\n",
    "LON_TOLERANCE = 0.2  # longitude tolerance (degrees)\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_file):\n",
    "    \"\"\"Create a file-only logger.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\"))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Core utilities ----------------------\n",
    "def build_aviso_index():\n",
    "    \"\"\"Build an index dict mapping (year, month) -> AVISO NetCDF file path.\"\"\"\n",
    "    index = {}\n",
    "    for nc_file in aviso_DIR.glob(\"dt_global_allsat_eke_*.nc\"):\n",
    "        try:\n",
    "            filename = nc_file.stem\n",
    "            parts = filename.split(\"_\")\n",
    "            year = int(parts[-2][1:5])\n",
    "            month = int(parts[-1][1:3])\n",
    "            index[(year, month)] = nc_file\n",
    "        except Exception:\n",
    "            continue  # silently skip filename parsing errors\n",
    "    return index\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    \"\"\"Find the nearest index (simple argmin distance).\"\"\"\n",
    "    return np.abs(array - value).argmin()\n",
    "\n",
    "def process_group(args):\n",
    "    \"\"\"Process a single (year, month) group (silent mode).\"\"\"\n",
    "    (year, month), group, nc_file = args\n",
    "    result_df = group.copy()\n",
    "    result_df[\"EKE\"] = np.nan  # initialize as NaN\n",
    "\n",
    "    if nc_file is None or not nc_file.exists():\n",
    "        return result_df\n",
    "\n",
    "    try:\n",
    "        with xr.open_dataset(nc_file) as ds:\n",
    "            lon_array = ds.longitude.values\n",
    "            lat_array = ds.latitude.values\n",
    "            eke_data = ds[\"eke\"].isel(time=0).values  # assume time dimension is correct\n",
    "\n",
    "            for idx, row in group.iterrows():\n",
    "                # Normalize longitude to [0, 360)\n",
    "                target_lon = row[\"Longitude\"] % 360\n",
    "                target_lat = row[\"Latitude\"]\n",
    "\n",
    "                # Find nearest grid point\n",
    "                lon_idx = find_nearest_idx(lon_array, target_lon)\n",
    "                lat_idx = find_nearest_idx(lat_array, target_lat)\n",
    "\n",
    "                # Tolerance check\n",
    "                if (abs(lon_array[lon_idx] - target_lon) <= LON_TOLERANCE and\n",
    "                    abs(lat_array[lat_idx] - target_lat) <= LAT_TOLERANCE):\n",
    "                    result_df.at[idx, \"EKE\"] = eke_data[lat_idx, lon_idx]\n",
    "\n",
    "    except Exception:\n",
    "        pass  # silently ignore all exceptions\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# ---------------------- Main workflow ----------------------\n",
    "def process_single_depth(target_depth, aviso_index):\n",
    "    \"\"\"Process one depth level.\"\"\"\n",
    "    # Configure logging\n",
    "    log_file = DOXY_BASE / \"logs\" / f\"depth{target_depth}.log\"\n",
    "    logger = setup_logger(f\"depth{target_depth}\", log_file)\n",
    "\n",
    "    try:\n",
    "        # Build file paths\n",
    "        depth_dir = DOXY_BASE / f\"{target_depth}dbar\"\n",
    "        input_csv = depth_dir / f\"depth{target_depth}_TRAIN.csv\"  # adjust if naming differs\n",
    "\n",
    "        # Validate input file\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file not found: {input_csv}\")\n",
    "            return\n",
    "        if os.path.getsize(input_csv) == 0:\n",
    "            logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv(input_csv)\n",
    "        if \"EKE\" in df.columns:\n",
    "            df = df.drop(columns=[\"EKE\"])  # remove old EKE column if present\n",
    "\n",
    "        # Prepare parallel tasks\n",
    "        task_args = []\n",
    "        for (year, month), group in df.groupby([\"Year\", \"Month\"]):\n",
    "            nc_path = aviso_index.get((year, month))\n",
    "            task_args.append(((year, month), group, nc_path))\n",
    "\n",
    "        # Parallel processing\n",
    "        with Pool(processes=12) as pool:\n",
    "            results = []\n",
    "            with tqdm(total=len(task_args), desc=f\"Depth {target_depth}dbar\", leave=False) as pbar:\n",
    "                for res in pool.imap(process_group, task_args):\n",
    "                    results.append(res)\n",
    "                    pbar.update()\n",
    "\n",
    "        # Merge results and set precision\n",
    "        final_df = pd.concat(results).sort_index()\n",
    "        final_df[\"EKE\"] = final_df[\"EKE\"].round(3)  # keep 3 decimals\n",
    "\n",
    "        # Overwrite the original file\n",
    "        final_df.to_csv(input_csv, index=False)\n",
    "        logger.info(f\"Wrote {len(final_df)} rows to {input_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing error: {str(e)}\", exc_info=True)\n",
    "\n",
    "def main():\n",
    "    # Build global index\n",
    "    aviso_index = build_aviso_index()\n",
    "\n",
    "    # Create log directory\n",
    "    (DOXY_BASE / \"logs\").mkdir(exist_ok=True)\n",
    "\n",
    "    # Process each depth sequentially\n",
    "    for depth in DEPTHS:\n",
    "        process_single_depth(depth, aviso_index)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Processing completed. Please check the output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "PAR_DIR = Path(\"/data/wang/NASA/PAR/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LAT_TOLERANCE = 0.4\n",
    "LON_TOLERANCE = 0.2\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_file):\n",
    "    \"\"\"Create a logger that writes to both file and console; ensure parent dirs exist.\"\"\"\n",
    "    log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s: %(message)s'))\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter('%(levelname)s: %(levelname)s: %(message)s'))\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Core processing ----------------------\n",
    "def build_par_index():\n",
    "    \"\"\"Build two index dicts for PAR netCDF files keyed by (year, month).\"\"\"\n",
    "    seawifs_index = {}\n",
    "    modis_index = {}\n",
    "\n",
    "    # Build SEAWIFS index (1997–2002)\n",
    "    for nc_file in PAR_DIR.glob(\"SEASTAR_SEAWIFS_GAC.*.L3m.MO.PAR.par.9km.nc\"):\n",
    "        try:\n",
    "            match = re.search(r\"\\.(\\d{6})\\d{2}_\\d{8}\", nc_file.name)\n",
    "            if match:\n",
    "                yyyymm = match.group(1)\n",
    "                year = int(yyyymm[:4])\n",
    "                month = int(yyyymm[4:6])\n",
    "                if 1997 <= year <= 2002:\n",
    "                    seawifs_index[(year, month)] = nc_file\n",
    "        except Exception as e:\n",
    "            print(f\"SEAWIFS index build error: {str(e)}\")\n",
    "\n",
    "    # Build MODIS index (2003–2024)\n",
    "    for nc_file in PAR_DIR.glob(\"AQUA_MODIS.*.L3m.MO.PAR.par.4km.nc\"):\n",
    "        try:\n",
    "            match = re.search(r\"\\.(\\d{6})\\d{2}_\\d{8}\", nc_file.name)\n",
    "            if match:\n",
    "                yyyymm = match.group(1)\n",
    "                year = int(yyyymm[:4])\n",
    "                month = int(yyyymm[4:6])\n",
    "                if 2003 <= year <= 2024:\n",
    "                    modis_index[(year, month)] = nc_file\n",
    "        except Exception as e:\n",
    "            print(f\"MODIS index build error: {str(e)}\")\n",
    "\n",
    "    return seawifs_index, modis_index\n",
    "\n",
    "def find_nearest_idx(array, value):\n",
    "    \"\"\"Return the index of the nearest value in a 1D array.\"\"\"\n",
    "    return np.abs(array - value).argmin()\n",
    "\n",
    "def process_group_par(args):\n",
    "    \"\"\"Multiprocessing worker for one (year, month) group.\"\"\"\n",
    "    (year, month), group, seawifs_idx, modis_idx, log_file = args\n",
    "    logger = setup_logger(f\"process_{os.getpid()}\", log_file)\n",
    "\n",
    "    result_df = group.copy()\n",
    "    result_df['PAR'] = np.nan\n",
    "\n",
    "    try:\n",
    "        # Dynamically choose sensor by year\n",
    "        if year < 2003:\n",
    "            nc_file = seawifs_idx.get((year, month))\n",
    "            sensor = \"SEAWIFS\"\n",
    "        else:\n",
    "            nc_file = modis_idx.get((year, month))\n",
    "            sensor = \"MODIS\"\n",
    "\n",
    "        if nc_file is None or not nc_file.exists():\n",
    "            return result_df\n",
    "\n",
    "        with xr.open_dataset(nc_file) as ds:\n",
    "            lons = ds.lon.values.astype(float)\n",
    "            lats = ds.lat.values.astype(float)\n",
    "            par_data = ds.par.values\n",
    "\n",
    "            # Ensure shape is (time, lat, lon)\n",
    "            if par_data.ndim == 2:\n",
    "                par_data = par_data[np.newaxis, :, :]\n",
    "\n",
    "            for idx, row in group.iterrows():\n",
    "                target_lat = row['Latitude']\n",
    "                target_lon = row['Longitude']\n",
    "\n",
    "                lon_idx = find_nearest_idx(lons, target_lon)\n",
    "                lat_idx = find_nearest_idx(lats, target_lat)\n",
    "\n",
    "                # Apply nearest-neighbor tolerance filter\n",
    "                if (abs(lons[lon_idx] - target_lon) > LON_TOLERANCE or\n",
    "                    abs(lats[lat_idx] - target_lat) > LAT_TOLERANCE):\n",
    "                    continue\n",
    "\n",
    "                par_value = par_data[0, lat_idx, lon_idx]\n",
    "                if not np.isnan(par_value):\n",
    "                    result_df.at[idx, 'PAR'] = round(par_value, 3)  # keep 3 decimals\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Group processing failed {year}-{month}: {str(e)}\")\n",
    "        return result_df\n",
    "\n",
    "# ---------------------- Main workflow ----------------------\n",
    "def process_single_depth(target_depth, seawifs_index, modis_index):\n",
    "    \"\"\"Process one depth layer.\"\"\"\n",
    "    log_dir = DOXY_BASE / \"logs_par\"\n",
    "    logger = setup_logger(f\"depth{target_depth}\", log_dir / f\"depth{target_depth}.log\")\n",
    "\n",
    "    try:\n",
    "        # Input/output paths\n",
    "        input_dir = DOXY_BASE / f\"{target_depth}dbar\"\n",
    "        input_csv = input_dir / f\"depth{target_depth}_TRAIN.csv\"\n",
    "\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file not found: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv(input_csv)\n",
    "        if df.empty:\n",
    "            logger.warning(f\"Empty dataset: {input_csv}\")\n",
    "            return\n",
    "\n",
    "        # Build multiprocessing tasks\n",
    "        task_args = []\n",
    "        for (year, month), group in df.groupby(['Year', 'Month']):\n",
    "            task_args.append((\n",
    "                (year, month),\n",
    "                group,\n",
    "                seawifs_index,\n",
    "                modis_index,\n",
    "                log_dir / f\"par_{year}_{month}.log\"\n",
    "            ))\n",
    "\n",
    "        # Parallel execution\n",
    "        with Pool(processes=12) as pool:\n",
    "            results = []\n",
    "            with tqdm(total=len(task_args), desc=f\"Depth {target_depth}dbar\") as pbar:\n",
    "                for result in pool.imap(process_group_par, task_args):\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # Merge outputs and sort by original index\n",
    "            final_df = pd.concat(results).sort_index()\n",
    "\n",
    "            # Overwrite the original file\n",
    "            final_df.to_csv(input_csv, index=False)\n",
    "            logger.info(f\"File updated: {input_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Depth-layer processing error: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point.\"\"\"\n",
    "    main_log_dir = DOXY_BASE / \"logs_par\"\n",
    "    main_logger = setup_logger(\"main_par\", main_log_dir / \"main.log\")\n",
    "\n",
    "    # Build indices\n",
    "    seawifs_idx, modis_idx = build_par_index()\n",
    "    main_logger.info(f\"Index loaded: SEAWIFS({len(seawifs_idx)}), MODIS({len(modis_idx)})\")\n",
    "\n",
    "    # Process depths sequentially\n",
    "    for depth in DEPTHS:\n",
    "        main_logger.info(f\"Start processing depth: {depth}dbar\")\n",
    "        process_single_depth(depth, seawifs_idx, modis_idx)\n",
    "\n",
    "    main_logger.info(\"All tasks completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"Done! Please check log files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "CMEMS_DIR = Path(\"/data/wang/CMEMS/pco2/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LAT_TOLERANCE = 0.4\n",
    "LON_TOLERANCE = 0.2\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "\n",
    "CO2_VARS = [\"CO2_flux\", \"pH\", \"pCO2\", \"DIC\", \"Alkalinity\"]\n",
    "NC_VARS = [\"fgco2\", \"ph\", \"spco2\", \"tco2\", \"talk\"]\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_dir):\n",
    "    \"\"\"Robust logger initialization.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if logger.handlers:\n",
    "        return logger\n",
    "\n",
    "    try:\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_handler = logging.FileHandler(log_dir / f\"{name}.log\")\n",
    "        file_handler.setFormatter(\n",
    "            logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "        )\n",
    "        logger.addHandler(file_handler)\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied: {log_dir}. Falling back to console logging.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Logger initialization error: {str(e)}\")\n",
    "\n",
    "    if not logger.handlers:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(\n",
    "            logging.Formatter(\"%(levelname)s - %(message)s\")\n",
    "        )\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------- Core processor ----------------------\n",
    "class CO2Processor:\n",
    "    def __init__(self):\n",
    "        self.logger = setup_logger(\"CO2Processor\", DOXY_BASE / \"logs\")\n",
    "        try:\n",
    "            self.file_index = self._build_file_index()\n",
    "            self.logger.info(\"CO2 processor initialized successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Initialization failed: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def _build_file_index(self):\n",
    "        \"\"\"Build a file index dict keyed by (year, month) -> nc file.\"\"\"\n",
    "        index = {}\n",
    "        pattern = re.compile(r\"cmems_obs-mob_glo_bgc-car_my_irr-i_(\\d{6})\\.nc$\")\n",
    "        try:\n",
    "            for nc_file in CMEMS_DIR.glob(\"*.nc\"):\n",
    "                if not nc_file.is_file():\n",
    "                    continue\n",
    "                match = pattern.match(nc_file.name)\n",
    "                if match:\n",
    "                    yyyymm = match.group(1)\n",
    "                    year = int(yyyymm[:4])\n",
    "                    month = int(yyyymm[4:6])\n",
    "                    index[(year, month)] = nc_file\n",
    "                    self.logger.debug(f\"Indexed file: {year}-{month:02d}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to build file index: {str(e)}\")\n",
    "            raise\n",
    "        return index\n",
    "\n",
    "    def _lon_360(self, lon):\n",
    "        \"\"\"Convert longitude to [0, 360).\"\"\"\n",
    "        try:\n",
    "            return lon % 360\n",
    "        except TypeError:\n",
    "            return np.nan\n",
    "\n",
    "    def _find_nearest_idx(self, array, value):\n",
    "        \"\"\"Return the nearest index for value in array, robust to NaNs/errors.\"\"\"\n",
    "        try:\n",
    "            return np.nanargmin(np.abs(array - value))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Coordinate lookup failed: {str(e)}\")\n",
    "            return -1\n",
    "\n",
    "    def process_depth(self, target_depth):\n",
    "        \"\"\"Process a single depth level.\"\"\"\n",
    "        input_dir = DOXY_BASE / f\"{target_depth}dbar\"\n",
    "        input_csv = input_dir / f\"depth{target_depth}_TRAIN.csv\"\n",
    "\n",
    "        if not self._validate_input(input_csv):\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(input_csv)\n",
    "            for col in CO2_VARS:\n",
    "                df[col] = np.nan  # initialize new columns\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to read CSV: {str(e)}\")\n",
    "            return\n",
    "\n",
    "        # Parallel processing by (year, month) groups\n",
    "        groups = list(df.groupby([\"Year\", \"Month\"]))\n",
    "        with Pool(processes=48) as pool:\n",
    "            processed_groups = list(\n",
    "                tqdm(\n",
    "                    pool.imap(self._process_group_wrapper, [(g[0], g[1]) for g in groups]),\n",
    "                    total=len(groups),\n",
    "                    desc=f\"Processing {target_depth} dbar\",\n",
    "                    unit=\"group\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Merge results\n",
    "        final_df = pd.concat([pg for pg in processed_groups if pg is not None])\n",
    "\n",
    "        try:\n",
    "            # Apply rounding only to the newly added variables\n",
    "            final_df[CO2_VARS] = final_df[CO2_VARS].round(3)\n",
    "\n",
    "            # Overwrite the original file (write all columns once)\n",
    "            final_df.to_csv(input_csv, index=False)\n",
    "            self.logger.info(f\"Successfully overwrote file: {input_csv}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to write file: {str(e)}\")\n",
    "\n",
    "    def _validate_input(self, input_csv):\n",
    "        \"\"\"Check that the input CSV exists and is non-empty.\"\"\"\n",
    "        if not input_csv.exists():\n",
    "            self.logger.error(f\"Input file not found: {input_csv}\")\n",
    "            return False\n",
    "        if input_csv.stat().st_size == 0:\n",
    "            self.logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _process_group_wrapper(self, args):\n",
    "        \"\"\"Wrapper for multiprocessing to isolate exceptions.\"\"\"\n",
    "        try:\n",
    "            return self._process_group(*args)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Group processing exception: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _process_group(self, year_month, group_df):\n",
    "        \"\"\"Process one (year, month) group and fill CO2-related variables.\"\"\"\n",
    "        year, month = year_month\n",
    "        nc_file = self.file_index.get((year, month))\n",
    "        if not nc_file or not nc_file.exists():\n",
    "            self.logger.warning(f\"Missing NC file: {year}-{month:02d}\")\n",
    "            return group_df\n",
    "\n",
    "        try:\n",
    "            with xr.open_dataset(nc_file) as ds:\n",
    "                lats = ds.latitude.values.astype(\"float32\")\n",
    "                lons = ds.longitude.values.astype(\"float32\")\n",
    "\n",
    "                target_lons = group_df[\"Longitude\"].apply(self._lon_360).values\n",
    "                target_lats = group_df[\"Latitude\"].values\n",
    "\n",
    "                lat_indices = np.array([self._find_nearest_idx(lats, lat) for lat in target_lats])\n",
    "                lon_indices = np.array([self._find_nearest_idx(lons, lon) for lon in target_lons])\n",
    "\n",
    "                valid_mask = (\n",
    "                    (np.abs(lats[lat_indices] - target_lats) <= LAT_TOLERANCE) &\n",
    "                    (np.abs(lons[lon_indices] - target_lons) <= LON_TOLERANCE)\n",
    "                )\n",
    "                valid_idx = np.where(valid_mask)[0]\n",
    "\n",
    "                results = np.full((len(group_df), len(CO2_VARS)), np.nan, dtype=np.float32)\n",
    "\n",
    "                if len(valid_idx) > 0:\n",
    "                    for var_idx, var in enumerate(NC_VARS):\n",
    "                        try:\n",
    "                            var_data = ds[var][0].values\n",
    "                            valid_values = var_data[lat_indices[valid_idx], lon_indices[valid_idx]]\n",
    "                            results[valid_idx, var_idx] = valid_values\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Failed to extract variable {var}: {str(e)}\")\n",
    "\n",
    "                group_df[CO2_VARS] = results\n",
    "                return group_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Processing failed {year}-{month:02d}: {str(e)}\")\n",
    "            return group_df\n",
    "\n",
    "# ---------------------- Entry point ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = CO2Processor()\n",
    "        # Process depths in order; internally parallelize by (year, month) groups\n",
    "        for depth in tqdm(DEPTHS, desc=\"Depth processing progress\"):\n",
    "            processor.process_depth(depth)\n",
    "        print(\"Batch processing finished!\")\n",
    "    except Exception as e:\n",
    "        logging.getLogger(\"Main\").critical(f\"Batch processing failed: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chla\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# ---------------------- Configuration ----------------------\n",
    "CHLA_DIR = Path(\"/data/wang/NASA/Chla/\")\n",
    "DOXY_BASE = Path(\"/data/wang/Result_Data/alldoxy/\")\n",
    "LAT_TOLERANCE = 0.4\n",
    "LON_TOLERANCE = 0.2\n",
    "DEPTHS = TARGET_DEPTHS\n",
    "N_PROCESSES = 48  # number of worker processes\n",
    "\n",
    "# ---------------------- Logging ----------------------\n",
    "def setup_logger(name, log_dir):\n",
    "    \"\"\"Process-safe logger configuration.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        log_file = log_dir / f\"{name}.log\"\n",
    "\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s: %(message)s\"))\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "# ---------------------- File index builder ----------------------\n",
    "class ChlaIndexer:\n",
    "    def __init__(self):\n",
    "        self.seawifs_index = {}  # 1997-2002\n",
    "        self.modis_index = {}    # 2003-2024\n",
    "        self._build_index()\n",
    "\n",
    "    def _parse_filename(self, filename):\n",
    "        \"\"\"Generic filename parser.\"\"\"\n",
    "        patterns = [\n",
    "            r\"SEASTAR_SEAWIFS_GAC\\.(\\d{6})\\d{2}_\\d+\\.L3m\\.MO\\.CHL\\.chlor_a\\.par\\.9km\\.nc\",\n",
    "            r\"AQUA_MODIS\\.(\\d{6})\\d{2}_\\d+\\.L3m\\.MO\\.CHL\\.chlor_a\\.4km\\.nc\"\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, filename)\n",
    "            if match:\n",
    "                yyyymm = match.group(1)\n",
    "                year = int(yyyymm[:4])\n",
    "                month = int(yyyymm[4:6])\n",
    "                return year, month\n",
    "        return None, None\n",
    "\n",
    "    def _build_index(self):\n",
    "        \"\"\"Build a two-era file index.\"\"\"\n",
    "        for nc_file in CHLA_DIR.glob(\"*.nc\"):\n",
    "            if not nc_file.is_file():\n",
    "                continue\n",
    "\n",
    "            year, month = self._parse_filename(nc_file.name)\n",
    "            if year is None:\n",
    "                continue\n",
    "\n",
    "            # Store by era\n",
    "            if 1997 <= year <= 2002:\n",
    "                self.seawifs_index[(year, month)] = nc_file\n",
    "            elif 2003 <= year <= 2024:\n",
    "                self.modis_index[(year, month)] = nc_file\n",
    "\n",
    "# ---------------------- Core processing class ----------------------\n",
    "class ChlaProcessor:\n",
    "    def __init__(self):\n",
    "        self.logger = setup_logger(\"ChlaProcessor\", DOXY_BASE / \"logs_chla\")\n",
    "        self.indexer = ChlaIndexer()\n",
    "        self.logger.info(\n",
    "            f\"Index summary: SEAWIFS({len(self.indexer.seawifs_index)}) | MODIS({len(self.indexer.modis_index)})\"\n",
    "        )\n",
    "\n",
    "    def process_depth(self, target_depth):\n",
    "        \"\"\"Process a single depth level.\"\"\"\n",
    "        processor_id = f\"{target_depth}dbar_{os.getpid()}\"\n",
    "        logger = setup_logger(processor_id, DOXY_BASE / \"logs_chla\")\n",
    "\n",
    "        try:\n",
    "            input_csv = DOXY_BASE / f\"{target_depth}dbar\" / f\"depth{target_depth}_TRAIN.csv\"\n",
    "            output_csv = input_csv  # overwrite in place\n",
    "\n",
    "            if not self._validate_input(input_csv, logger):\n",
    "                return\n",
    "\n",
    "            df = pd.read_csv(input_csv)\n",
    "            if \"Chla\" not in df.columns:\n",
    "                df[\"Chla\"] = np.nan\n",
    "\n",
    "            # Extract all (Year, Month) groups\n",
    "            year_month_groups = df.groupby([\"Year\", \"Month\"]).groups\n",
    "            task_args = [(target_depth, ym, df.loc[idx]) for ym, idx in year_month_groups.items()]\n",
    "\n",
    "            # Parallel processing by (Year, Month)\n",
    "            with Pool(processes=N_PROCESSES) as pool:\n",
    "                results = list(tqdm(\n",
    "                    pool.imap(self._process_year_month, task_args),\n",
    "                    total=len(task_args),\n",
    "                    desc=f\"Depth {target_depth}dbar\",\n",
    "                    unit=\"group\"\n",
    "                ))\n",
    "\n",
    "            # Merge results and save\n",
    "            final_df = pd.concat([r for r in results if r is not None])\n",
    "            final_df[\"Chla\"] = final_df[\"Chla\"].round(4)  # keep 4 decimals\n",
    "            final_df.to_csv(output_csv, index=False)\n",
    "            logger.info(f\"Overwritten successfully: {output_csv}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Processing failed: {str(e)}\", exc_info=True)\n",
    "\n",
    "    def _process_year_month(self, args):\n",
    "        \"\"\"Process a single (Year, Month) group (parallelized).\"\"\"\n",
    "        target_depth, (year, month), group_df = args\n",
    "        try:\n",
    "            # Select NetCDF file by era\n",
    "            if year <= 2002:\n",
    "                nc_file = self.indexer.seawifs_index.get((year, month))\n",
    "            else:\n",
    "                nc_file = self.indexer.modis_index.get((year, month))\n",
    "\n",
    "            if not nc_file or not nc_file.exists():\n",
    "                return group_df\n",
    "\n",
    "            with xr.open_dataset(nc_file) as ds:\n",
    "                # Validate variable presence\n",
    "                if \"chlor_a\" not in ds.variables:\n",
    "                    return group_df\n",
    "\n",
    "                # Read coordinate arrays\n",
    "                lats = ds.lat.values.astype(float)\n",
    "                lons = ds.lon.values.astype(float)\n",
    "                chla_data = ds[\"chlor_a\"].values\n",
    "\n",
    "                # Ensure data has a time dimension\n",
    "                if chla_data.ndim == 2:\n",
    "                    chla_data = chla_data[np.newaxis, :, :]\n",
    "\n",
    "                # Compute nearest indices (vectorized via list comprehension)\n",
    "                lat_indices = np.array([self._find_nearest(lats, lat) for lat in group_df[\"Latitude\"]])\n",
    "                lon_indices = np.array([self._find_nearest(lons, lon) for lon in group_df[\"Longitude\"]])\n",
    "\n",
    "                # Tolerance filtering\n",
    "                valid_mask = (\n",
    "                    (np.abs(lats[lat_indices] - group_df[\"Latitude\"]) <= LAT_TOLERANCE) &\n",
    "                    (np.abs(lons[lon_indices] - group_df[\"Longitude\"]) <= LON_TOLERANCE)\n",
    "                )\n",
    "                valid_idx = np.where(valid_mask)[0]\n",
    "\n",
    "                # Extract values\n",
    "                chla_values = np.full(len(group_df), np.nan, dtype=np.float32)\n",
    "                if len(valid_idx) > 0:\n",
    "                    try:\n",
    "                        chla_values[valid_idx] = chla_data[0, lat_indices[valid_idx], lon_indices[valid_idx]]\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "                group_df[\"Chla\"] = chla_values\n",
    "                return group_df\n",
    "\n",
    "        except Exception:\n",
    "            return group_df\n",
    "\n",
    "    def _find_nearest(self, array, value):\n",
    "        \"\"\"Find nearest index (NaN-safe).\"\"\"\n",
    "        return np.nanargmin(np.abs(array - value))\n",
    "\n",
    "    def _validate_input(self, input_csv, logger):\n",
    "        \"\"\"Validate input CSV file.\"\"\"\n",
    "        if not input_csv.exists():\n",
    "            logger.warning(f\"Input file not found: {input_csv}\")\n",
    "            return False\n",
    "        if input_csv.stat().st_size == 0:\n",
    "            logger.warning(f\"Empty file: {input_csv}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# ---------------------- Main program ----------------------\n",
    "def main():\n",
    "    # Initialize main logger\n",
    "    main_logger = setup_logger(\"ChlaMain\", DOXY_BASE / \"logs_chla\")\n",
    "\n",
    "    # Process depths sequentially\n",
    "    processor = ChlaProcessor()\n",
    "    for depth in DEPTHS:\n",
    "        main_logger.info(f\"Start processing depth: {depth}dbar\")\n",
    "        processor.process_depth(depth)\n",
    "\n",
    "    print(\"✅ Chlorophyll-a processing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
